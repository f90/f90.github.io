
@phdthesis{adilogluVariationalBayesian2012,
  title = {Variational {{Bayesian}} Inference for Source Separation and Robust Feature Extraction},
  author = {Adiloglu, Kamil and Vincent, Emmanuel},
  year = {2012},
  school = {INRIA},
  type = {{{PhD Thesis}}}
}

@inproceedings{airasComparisonMultiple2007,
  title = {Comparison of Multiple Voice Source Parameters in Different Phonation Types},
  booktitle = {8th {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Airas, Matti and Alku, Paavo},
  year = {2007},
  pages = {1410--1413},
  abstract = {A large sample of vowels produced by male and female speakers were inverse filtered and parameterized using 21 different glottal flow parameters. The performance of the different parameters in expression of the phonation type was then tested using objective statistical methods. The comparison of the results revealed marked differences in the parameters ' performance, and therefore, guidelines for parameter use and comparison were established. Index Terms: voice quality, phonation type, inverse filtering, voice source, parameterization},
  file = {/home/daniel/Zotero/storage/K2DG5AJH/Airas und Alku - 2007 - Comparison of multiple voice source parameters in .pdf}
}

@article{aljanakiStudyingEmotion2016,
  title = {Studying Emotion Induced by Music through a Crowdsourcing Game},
  author = {Aljanaki, Anna and Wiering, Frans and Veltkamp, Remco C},
  year = {2016},
  volume = {52},
  pages = {115--128},
  publisher = {{Elsevier}},
  journal = {Information Processing \& Management},
  number = {1}
}

@inproceedings{alkhouliAlignmentBasedNeural2016,
  title = {Alignment-{{Based Neural Machine Translation}}},
  booktitle = {{{WMT}}},
  author = {Alkhouli, Tamer and Bretschner, Gabriel and Peter, Jan-Thorsten and Hethnawi, Mohammed and Guta, Andreas and Ney, Hermann},
  year = {2016}
}

@inproceedings{alkuAutomaticMethod1992,
  title = {An Automatic Method to Estimate the Time-Based Parameters of the Glottal Pulseform},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Alku, Paavo},
  year = {1992},
  volume = {2},
  pages = {29--32}
}

@article{alkuNormalizedAmplitude2002,
  title = {Normalized Amplitude Quotient for Parametrization of the Glottal Flow},
  author = {Alku, Paavo and B{\"a}ckstr{\"o}m, Tom and Vilkman, Erkki},
  year = {2002},
  volume = {112},
  pages = {701--710},
  doi = {http://dx.doi.org/10.1121/1.1490365},
  journal = {The Journal of the Acoustical Society of America},
  number = {2}
}

@incollection{allanHarmonisingChorales2005,
  title = {Harmonising {{Chorales}} by {{Probabilistic Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 17},
  author = {Allan, Moray and Williams, Christopher},
  editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
  year = {2005},
  pages = {25--32},
  publisher = {{MIT Press}}
}

@article{almahairiAugmentedCycleGAN2018,
  title = {Augmented {{CycleGAN}}: {{Learning Many}}-to-{{Many Mappings}} from {{Unpaired Data}}},
  shorttitle = {Augmented {{CycleGAN}}},
  author = {Almahairi, Amjad and Rajeswar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
  year = {2018},
  volume = {abs/1802.10151},
  abstract = {Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.},
  archivePrefix = {arXiv},
  eprint = {1802.10151},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/NIGD8INC/Almahairi et al. - 2018 - Augmented CycleGAN Learning Many-to-Many Mappings.pdf;/home/daniel/Zotero/storage/CLSNITZQ/1802.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning}
}

@article{ambrogioniKernelMixture2017,
  title = {The {{Kernel Mixture Network}}: {{A Nonparametric Method}} for {{Conditional Density Estimation}} of {{Continuous Random Variables}}},
  author = {Ambrogioni, Luca and G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel AJ and Maris, Eric},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.07111}
}

@article{andrewsSurveyCritique1995,
  title = {Survey and Critique of Techniques for Extracting Rules from Trained Artificial Neural Networks},
  author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
  year = {1995},
  volume = {8},
  pages = {373--389},
  journal = {Knowledge-based systems},
  number = {6}
}

@article{andrychowiczLearningLearn2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio Gomez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and de Freitas, Nando},
  year = {2016},
  volume = {abs/1606.04474},
  file = {/home/daniel/Zotero/storage/H9YIV6UL/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf},
  journal = {CoRR}
}

@inproceedings{arjovskyPrincipledMethods2017,
  title = {Towards Principled Methods for Training Generative Adversarial Networks},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Arjovsky, Martin and Bottou, L{\'e}on},
  year = {2017}
}

@article{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  volume = {abs/1701.07875},
  journal = {CoRR}
}

@book{aronsonClinicalVoice2011,
  title = {Clinical Voice Disorders},
  author = {Aronson, Arnold E and Bless, Diane},
  year = {2011},
  publisher = {{Thieme}}
}

@article{artemismoroniVoxPopuli2000,
  title = {Vox {{Populi}}: {{An Interactive Evolutionary System}} for {{Algorithmic Music Composition}}},
  author = {Artemis Moroni, J{\^o}natas Manzolli, Fernando von Zuben, Ricardo Gudwin},
  year = {2000},
  volume = {10},
  pages = {49--54},
  issn = {09611215, 15314812},
  abstract = {While recent techniques of digital sound synthesis have put numerous new sounds on the musician's desktop, several artificial-intelligence (AI) techniques have also been applied to algorithmic composition. This article introduces Vox Populi, a system based on evolutionary computation techniques for composing music in real time. In Vox Populi, a population of chords codified according to MIDI protocol evolves through the application of genetic algorithms to maximize a fitness criterion based on physical factors relevant to music. Graphical controls allow the user to manipulate fitness and sound attributes.},
  journal = {Leonardo Music Journal}
}

@article{assayagUsingFactor2004,
  title = {Using Factor Oracles for Machine Improvisation},
  author = {Assayag, G{\'e}rard and Dubnov, Shlomo},
  year = {2004},
  volume = {8},
  pages = {604--610},
  journal = {Soft Computing},
  number = {9}
}

@book{audacityteamCrossfadeTypes,
  title = {Crossfade Types},
  author = {{Audacity team}}
}

@inproceedings{avidanSeamCarving2007,
  title = {Seam Carving for Content-Aware Image Resizing},
  booktitle = {{{ACM Transactions}} on Graphics ({{TOG}})},
  author = {Avidan, S. and Shamir, A.},
  year = {2007},
  volume = {26},
  pages = {10}
}

@book{backEvolutionaryAlgorithms1996,
  title = {Evolutionary {{Algorithms}} in {{Theory}} and {{Practice}}: {{Evolution Strategies}}, {{Evolutionary Programming}}, {{Genetic Algorithms}}},
  author = {B{\"a}ck, Thomas},
  year = {1996},
  publisher = {{Oxford University Press}},
  address = {{Oxford, UK}},
  isbn = {0-19-509971-0}
}

@inproceedings{bahdanauEndtoendAttentionbased2016,
  title = {End-to-End Attention-Based Large Vocabulary Speech Recognition},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Bengio, Yoshua and others},
  year = {2016},
  pages = {4945--4949},
  publisher = {{IEEE}}
}

@article{bahdanauNeuralMachine2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.0473}
}

@inproceedings{baiConvolutionalSequence2018,
  title = {Convolutional {{Sequence Modeling Revisited}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}}) {{Workshop}} Track},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018}
}

@inproceedings{balcanPACstyleModel2005,
  title = {A {{PAC}}-Style Model for Learning from Labeled and Unlabeled Data},
  booktitle = {International {{Conference}} on {{Computational Learning Theory}}},
  author = {Balcan, Maria-Florina and Blum, Avrim},
  year = {2005},
  pages = {111--126},
  publisher = {{Springer}}
}

@article{balduzziStronglyTypedRecurrent2016,
  title = {Strongly-{{Typed Recurrent Neural Networks}}},
  author = {Balduzzi, David and Ghifary, Muhammad},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.02218}
}

@inproceedings{barkerFifthCHiME2018,
  title = {The Fifth {{CHiME Speech Separation}} and {{Recognition Challenge}}: {{Dataset}}, Task and Baselines},
  booktitle = {Proceedings {{Interspeech}}},
  author = {Barker, Jon and Watanabe, Shinji and Vincent, Emmanuel and Trmal, Jan},
  year = {2018}
}

@article{bartschAudioThumbnailing2005,
  title = {Audio Thumbnailing of Popular Music Using Chroma-Based Representations},
  author = {Bartsch, Mark A and Wakefield, Gregory H},
  year = {2005},
  volume = {7},
  pages = {96--104},
  journal = {Multimedia, IEEE Transactions on},
  number = {1}
}

@inproceedings{ben-davidDoesUnlabeled2008,
  title = {Does {{Unlabeled Data Provably Help}}? {{Worst}}-Case {{Analysis}} of the {{Sample Complexity}} of {{Semi}}-{{Supervised Learning}}.},
  booktitle = {{{COLT}}},
  author = {{Ben-David}, Shai and Lu, Tyler and P{\'a}l, D{\'a}vid},
  year = {2008},
  pages = {33--44}
}

@article{benetosAutomaticMusic2013,
  title = {Automatic Music Transcription: Challenges and Future Directions},
  author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
  year = {2013},
  volume = {41},
  pages = {407--434},
  journal = {Journal of Intelligent Information Systems},
  number = {3}
}

@article{bengioFeedforwardInitialization2016,
  title = {Feedforward {{Initialization}} for {{Fast Inference}} of {{Deep Generative Networks}} Is Biologically Plausible},
  author = {Bengio, Yoshua and Scellier, Benjamin and Bilaniuk, Olexa and Sacramento, Joao and Senn, Walter},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.01651}
}

@inproceedings{bengioGeneralizedDenoising2013,
  title = {Generalized Denoising Auto-Encoders as Generative Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
  year = {2013},
  pages = {899--907}
}

@inproceedings{bengioGreedyLayerwise2007,
  title = {Greedy Layer-Wise Training of Deep Networks},
  booktitle = {Advances {{In Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  pages = {153--160}
}

@article{bengioLearningLongterm1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  year = {1994},
  month = mar,
  volume = {5},
  pages = {157--166},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  number = {2}
}

@article{bengioRepresentationLearning2013,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pierre},
  year = {2013},
  volume = {35},
  pages = {1798--1828},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  number = {8}
}

@article{berganPerceptionTwo2004,
  title = {The Perception of Two Vocal Qualities in a Synthesized Vocal Utterance: Ring and Pressed Voice},
  author = {Bergan, Christine C. and Titze, Ingo R. and Story, Brad},
  year = {2004},
  volume = {18},
  pages = {305--317},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/j.jvoice.2003.09.004},
  abstract = {Two vocal qualities, ring quality and pressed quality, were analyzed perceptually. Listeners were asked to rate (on a scale from 0 to 10) the ``amount of ring\dbend{}? in one listening and the ``amount of pressedness\dbend{}? in another listening. The stimulus was the synthesized utterance /ya-ya-ya-ya-ya/. In the continuum representation of ring, the skewing quotient and the cross section of the epilaryngeal tube area were systematically varied, independently and by a covariation rule. In the continuum representation of pressed, the flow amplitude and open quotient were similarly varied. Results indicated that the crossover point between ring and no ring occurred with an epilaryngeal area of around 1.0 cm2, and the crossover point between pressed and not pressed quality occurred at an open quotient of about 0.4. Fundamental frequency also had an effect on the perceptions, with a higher fundamental frequency receiving higher ratings of ring and pressed for otherwise the same parameters. Listeners demonstrated highly variable perceptions in both continua with poor intersubject, intrasubject, and intergroup reliability.},
  journal = {Journal of Voice},
  keywords = {Voice},
  number = {3}
}

@article{berthouzozToolsPlacing2012,
  title = {Tools for Placing Cuts and Transitions in Interview Video},
  author = {Berthouzoz, Floraine and Li, Wilmot and Agrawala, Maneesh},
  year = {2012},
  volume = {31},
  pages = {67},
  journal = {ACM Transactions on Graphics (TOG)},
  number = {4}
}

@inproceedings{bilesGenJamGenetic1994,
  title = {{{GenJam}}: {{A}} Genetic Algorithm for Generating Jazz Solos},
  booktitle = {Proceedings of the {{International Computer Music Conference}}},
  author = {Biles, John},
  year = {1994},
  pages = {131--131},
  publisher = {{INTERNATIONAL COMPUTER MUSIC ACCOCIATION}}
}

@article{binkowskiHighFidelity2019,
  title = {High {{Fidelity Speech Synthesis}} with {{Adversarial Networks}}},
  author = {Bi{\'n}kowski, Miko{\l}aj and Donahue, Jeff and Dieleman, Sander and Clark, Aidan and Elsen, Erich and Casagrande, Norman and Cobo, Luis C. and Simonyan, Karen},
  year = {2019},
  month = sep,
  abstract = {Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\textbackslash{}'echet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav.},
  archivePrefix = {arXiv},
  eprint = {1909.11646},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/EDBAIA3L/Bińkowski et al. - 2019 - High Fidelity Speech Synthesis with Adversarial Ne.pdf;/home/daniel/Zotero/storage/YKPZFA6D/1909.html},
  journal = {arXiv:1909.11646 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{birkholzArticulatorySynthesis2007,
  title = {Articulatory Synthesis of Singing.},
  booktitle = {{{INTERSPEECH}}},
  author = {Birkholz, Peter},
  year = {2007},
  pages = {4001--4004}
}

@inproceedings{bittnerDeepSalience2017,
  title = {Deep {{Salience Representations}} for {{F0 Estimation}} in {{Polyphonic Music}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Bittner, Rachel M and McFee, Brian and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
  year = {2017},
  pages = {63--70}
}

@inproceedings{bittnerMedleyDBMultitrack2014,
  title = {{{MedleyDB}}: {{A}} Multitrack Dataset for Annotation-Intensive {{MIR}} Research},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
  year = {2014}
}

@article{bittnerMultitaskLearning2018,
  title = {Multitask {{Learning}} for {{Fundamental Frequency Estimation}} in {{Music}}},
  author = {Bittner, Rachel M. and McFee, Brian and Bello, Juan P.},
  year = {2018},
  month = sep,
  volume = {abs/1809.00381},
  abstract = {Fundamental frequency (f0) estimation from polyphonic music includes the tasks of multiple-f0, melody, vocal, and bass line estimation. Historically these problems have been approached separately, and only recently, using learning-based approaches. We present a multitask deep learning architecture that jointly estimates outputs for various tasks including multiple-f0, melody, vocal and bass line estimation, and is trained using a large, semi-automatically annotated dataset. We show that the multitask model outperforms its single-task counterparts, and explore the effect of various design decisions in our approach, and show that it performs better or at least competitively when compared against strong baseline methods.},
  archivePrefix = {arXiv},
  eprint = {1809.00381},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/DZYGFPJJ/Bittner et al. - 2018 - Multitask Learning for Fundamental Frequency Estim.pdf;/home/daniel/Zotero/storage/37RKRRSU/1809.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{blaauwNeuralParametric2017,
  title = {A {{Neural Parametric Singing Synthesizer}}},
  author = {Blaauw, Merlijn and Bonada, Jordi},
  year = {2017},
  journal = {arXiv preprint arXiv:1704.03809}
}

@inproceedings{blackAutomaticIdentification2014,
  title = {Automatic {{Identification}} of {{Emotional Cues}} in {{Chinese Opera Singing}}},
  booktitle = {13th {{Int}}. {{Conf}}. on {{Music Perception}} and {{Cognition}} ({{ICMPC}})},
  author = {Black, Dawn A. A. and Li, Ma and Tian, Mi},
  year = {2014},
  pages = {250--255}
}

@article{bleiVariationalInference2016,
  title = {Variational Inference: {{A}} Review for Statisticians},
  author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  year = {2016},
  journal = {arXiv preprint arXiv:1601.00670}
}

@inproceedings{bockAccurateTempo2015,
  title = {Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {B{\"o}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
  year = {2015}
}

@inproceedings{bockJointBeat2016,
  title = {Joint Beat and Downbeat Tracking with Recurrent Neural Networks},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {B{\"o}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
  year = {2016}
}

@inproceedings{bogdanovMTGJamendoDataset2019,
  title = {The {{MTG}}-{{Jamendo}} Dataset for Automatic Music Tagging},
  booktitle = {Machine {{Learning}} for {{Music Discovery Workshop}} at the {{International Conference}} on {{Machine Learning}} ({{ICML}} 2019)},
  author = {Bogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier},
  year = {2019},
  month = jun,
  address = {{Long Beach, CA, United States}},
  abstract = {We present the MTG-Jamendo Dataset, a new open dataset for music auto-tagging. It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. The dataset contains over 55,000 full audio tracks with 195 tags from genre, instru- ment, and mood/theme categories. We provide elaborated data splits for researchers and report the performance of a simple baseline approach on five different sets of tags: genre, instrument, mood/theme, top-50, and overall.}
}

@article{borchPhonatoryResonatory2011,
  title = {Some {{Phonatory}} and {{Resonatory Characteristics}} of the {{Rock}}, {{Pop}}, {{Soul}}, and {{Swedish Dance Band Styles}} of {{Singing}}},
  author = {Borch, Daniel Z. and Sundberg, Johan},
  year = {2011},
  volume = {25},
  pages = {532--537},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/j.jvoice.2010.07.014},
  abstract = {Summary This investigation aims at describing voice function of four nonclassical styles of singing, Rock, Pop, Soul, and Swedish Dance Band. A male singer, professionally experienced in performing in these genres, sang representative tunes, both with their original lyrics and on the syllable /pae/. In addition, he sang tones in a triad pattern ranging from the pitch Bb2 to the pitch \{C4\} on the syllable /pae/ in pressed and neutral phonation. An expert panel was successful in classifying the samples, thus suggesting that the samples were representative of the various styles. Subglottal pressure was estimated from oral pressure during the occlusion for the consonant [p]. Flow glottograms were obtained from inverse filtering. The four lowest formant frequencies differed between the styles. The mean of the subglottal pressure and the mean of the normalized amplitude quotient (NAQ), that is, the ratio between the flow pulse amplitude and the product of period and maximum flow declination rate, were plotted against the mean of fundamental frequency. In these graphs, Rock and Swedish Dance Band assumed opposite extreme positions with respect to subglottal pressure and mean phonation frequency, whereas the mean \{NAQ\} values differed less between the styles.},
  journal = {Journal of Voice},
  keywords = {Formant frequencies},
  number = {5}
}

@book{borisBoostLibraries2011,
  title = {The {{Boost C}}++ {{Libraries}}},
  author = {Boris, S.},
  year = {2011},
  publisher = {{XML Press}}
}

@article{borjiProsCons2019,
  title = {Pros and Cons of Gan Evaluation Measures},
  author = {Borji, Ali},
  year = {2019},
  volume = {179},
  pages = {41--65},
  publisher = {{Elsevier}},
  journal = {Computer Vision and Image Understanding}
}

@inproceedings{boulanger-lewandowskiAudioChord2013,
  title = {Audio {{Chord Recognition}} with {{Recurrent Neural Networks}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {{Boulanger-Lewandowski}, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  year = {2013},
  pages = {335--340}
}

@article{boulanger-lewandowskiModelingTemporal2012,
  title = {Modeling Temporal Dependencies in High-Dimensional Sequences: {{Application}} to Polyphonic Music Generation and Transcription},
  author = {{Boulanger-Lewandowski}, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  year = {2012},
  pages = {1159--1166},
  journal = {Proc. of the International Conference on Machine Learning (ICML)}
}

@book{bracewellFourierTransform1986,
  title = {The {{Fourier}} Transform and Its Applications},
  author = {Bracewell, Ronald Newbold and Bracewell, Ronald N},
  year = {1986},
  volume = {31999},
  publisher = {{McGraw-Hill New York}}
}

@article{brakelLearningIndependent2017,
  title = {Learning {{Independent Features}} with {{Adversarial Nets}} for {{Non}}-Linear {{ICA}}},
  author = {Brakel, Philemon and Bengio, Yoshua},
  year = {2017},
  month = oct,
  volume = {abs/1710.05050},
  abstract = {Reliable measures of statistical dependence could be useful tools for learning independent features and performing tasks like source separation using Independent Component Analysis (ICA). Unfortunately, many of such measures, like the mutual information, are hard to estimate and optimize directly. We propose to learn independent features with adversarial objectives which optimize such measures implicitly. These objectives compare samples from the joint distribution and the product of the marginals without the need to compute any probability densities. We also propose two methods for obtaining samples from the product of the marginals using either a simple resampling trick or a separate parametric distribution. Our experiments show that this strategy can easily be applied to different types of model architectures and solve both linear and non-linear ICA problems.},
  archivePrefix = {arXiv},
  eprint = {1710.05050},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/35NE5ANQ/Brakel und Bengio - 2017 - Learning Independent Features with Adversarial Net.pdf;/home/daniel/Zotero/storage/MIQNAL9H/1710.html},
  journal = {CoRR},
  keywords = {Statistics - Machine Learning}
}

@article{bregmanAuditoryStreaming1978,
  title = {Auditory Streaming and the Building of Timbre.},
  author = {Bregman, Albert S and Pinker, Steven},
  year = {1978},
  volume = {32},
  pages = {19},
  journal = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  number = {1}
}

@article{bregmanPrimaryAuditory1971,
  title = {Primary Auditory Stream Segregation and Perception of Order in Rapid Sequences of Tones.},
  author = {Bregman, Albert S and Campbell, Jeffrey},
  year = {1971},
  volume = {89},
  pages = {244},
  journal = {Journal of experimental psychology},
  number = {2}
}

@book{breimanClassificationRegression1984,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, Richard A.},
  year = {1984},
  publisher = {{CRC press}}
}

@article{brockLargeScale2019,
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  year = {2019},
  month = feb,
  volume = {abs/1809.11096},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
  archivePrefix = {arXiv},
  eprint = {1809.11096},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/RJXFXQZX/Brock et al. - 2019 - Large Scale GAN Training for High Fidelity Natural.pdf;/home/daniel/Zotero/storage/DJF7AXFQ/1809.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{burgoyneMetaanalysisTimbre2007,
  title = {A Meta-Analysis of Timbre Perception Using Nonlinear Extensions to {{CLASCAL}}},
  booktitle = {Computer {{Music Modeling}} and {{Retrieval}}. {{Sense}} of {{Sounds}}},
  author = {Burgoyne, John Ashley and McAdams, Stephen},
  year = {2007},
  pages = {181--202},
  publisher = {{Springer}}
}

@article{caclinAcousticCorrelates2005,
  title = {Acoustic Correlates of Timbre Space Dimensions: {{A}} Confirmatory Study Using Synthetic Tonesa)},
  author = {Caclin, Anne and McAdams, Stephen and Smith, Bennett K and Winsberg, Suzanne},
  year = {2005},
  volume = {118},
  pages = {471--482},
  journal = {The Journal of the Acoustical Society of America},
  number = {1}
}

@article{camposSkipRNN2017,
  title = {Skip {{RNN}}: {{Learning}} to Skip State Updates in Recurrent Neural Networks},
  shorttitle = {Skip {{RNN}}},
  author = {Campos, Victor and Jou, Brendan and {Giro-i-Nieto}, Xavier and Torres, Jordi and Chang, Shih-Fu},
  year = {2017},
  month = aug,
  volume = {abs/1708.06834},
  abstract = {Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ .},
  archivePrefix = {arXiv},
  eprint = {1708.06834},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/TFBGNJFD/Campos et al. - 2017 - Skip RNN Learning to Skip State Updates in Recurr.pdf;/home/daniel/Zotero/storage/HDGQSHUQ/1708.html},
  journal = {CoRR},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{canoEvaluationQuality2016,
  title = {Evaluation of Quality of Sound Source Separation Algorithms: {{Human}} Perception vs Quantitative Metrics},
  booktitle = {2016 24th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Cano, E. and FitzGerald, D. and Brandenburg, K.},
  year = {2016},
  month = aug,
  pages = {1758--1762},
  issn = {2076-1465},
  doi = {10.1109/EUSIPCO.2016.7760550},
  abstract = {In this paper we look into the test methods to evaluate the quality of audio separation algorithms. Specifically we try to correlate the results of listening tests with state-of-the-art objective measures. To this end, the quality of the harmonic signals obtained with two harmonic-percussive separation algorithms was evaluated with BSS\textsubscript{E}val, PEASS and via listening tests. A correlation analysis was conducted and results show that for harmonic-percussive separation algorithms, neither BSS\textsubscript{E}val nor PEASS show strong correlation with the ratings obtained via listening tests and suggest that existing perceptual objective measures for quality assessment do not generalize well to different separation algorithms.},
  keywords = {Algorithm design and analysis,audio separation algorithms,audio signal processing,blind source separation,BSS_Eval,Correlation,correlation analysis,correlation methods,Distortion,Harmonic analysis,harmonic signals,harmonic-percussive separation algorithms,hearing,human perception,Interference,listening tests,Measurement,PEASS,quantitative metrics,Signal processing algorithms,sound source separation algorithms}
}

@incollection{caruanaMultitaskLearning1998,
  title = {Multitask {{Learning}}},
  booktitle = {Learning to {{Learn}}},
  author = {Caruana, Rich},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {95--133},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
  isbn = {978-1-4615-5529-2}
}

@article{celeuxClassificationEM1992,
  title = {A {{Classification EM Algorithm}} for {{Clustering}} and {{Two Stochastic Versions}}},
  author = {Celeux, Gilles and Govaert, G{\'e}rard},
  year = {1992},
  volume = {14},
  pages = {315--332},
  issn = {0167-9473},
  journal = {Computational Statistics and Data Analysis},
  number = {3}
}

@article{cemgilVariationalStochastic2007,
  title = {Variational and Stochastic Inference for {{Bayesian}} Source Separation},
  author = {Cemgil, A Taylan and F{\'e}votte, C{\'e}dric and Godsill, Simon J},
  year = {2007},
  volume = {17},
  pages = {891--913},
  journal = {Digital Signal Processing},
  number = {5}
}

@misc{centerforhistoryandnewmediaSchnelleinstieg,
  title = {Schnelleinstieg},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{chanComplexQuaternionic2016,
  title = {Complex and {{Quaternionic Principal Component Pursuit}} and {{Its Application}} to {{Audio Separation}}},
  author = {Chan, Tak-Shing T and Yang, Yi-Hsuan},
  year = {2016},
  volume = {23},
  pages = {287--291},
  journal = {IEEE Signal Processing Letters},
  number = {2}
}

@article{chandnaVocoderBased2019,
  title = {A {{Vocoder Based Method For Singing Voice Extraction}}},
  author = {Chandna, Pritish and Blaauw, Merlijn and Bonada, Jordi and Gomez, Emilia},
  year = {2019},
  month = mar,
  abstract = {This paper presents a novel method for extracting the vocal track from a musical mixture. The musical mixture consists of a singing voice and a backing track which may comprise of various instruments. We use a convolutional network with skip and residual connections as well as dilated convolutions to estimate vocoder parameters, given the spectrogram of an input mixture. The estimated parameters are then used to synthesize the vocal track, without any interference from the backing track. We evaluate our system, through objective metrics pertinent to audio quality and interference from background sources, and via a comparative subjective evaluation. We use open-source source separation systems based on Non-negative Matrix Factorization (NMFs) and Deep Learning methods as benchmarks for our system and discuss future applications for this particular algorithm.},
  archivePrefix = {arXiv},
  eprint = {1903.07554},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/F63ISZ5G/Chandna et al. - 2019 - A Vocoder Based Method For Singing Voice Extractio.pdf;/home/daniel/Zotero/storage/D9494NZN/1903.html},
  journal = {arXiv:1903.07554 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@incollection{changDilatedRecurrent2017,
  ids = {changDilatedRecurrent2017a},
  title = {Dilated {{Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Chang, Shiyu and Zhang, Yang and Han, Wei and Yu, Mo and Guo, Xiaoxiao and Tan, Wei and Cui, Xiaodong and Witbrock, Michael and {Hasegawa-Johnson}, Mark A and Huang, Thomas S},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {77--87},
  publisher = {{Curran Associates, Inc.}}
}

@article{changLyricstoAudioAlignment2017,
  title = {Lyrics-to-{{Audio Alignment}} by {{Unsupervised Discovery}} of {{Repetitive Patterns}} in {{Vowel Acoustics}}},
  author = {Chang, Sungkyun and Lee, Kyogu},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.06078}
}

@inproceedings{chanVocalActivity2015,
  title = {Vocal Activity Informed Singing Voice Separation with the {{iKala}} Dataset},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chan, Tak-Shing and Yeh, Tzu-Chun and Fan, Zhe-Cheng and Chen, Hung-Wei and Su, Li and Yang, Yi-Hsuan and Jang, Roger},
  year = {2015},
  pages = {718--722},
  publisher = {{IEEE}}
}

@inproceedings{chenFunctionalHarmony2018,
  title = {Functional Harmony Recognition of Symbolic Music Data with Multi-Task Recurrent Neural Networks.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Chen, Tsung-Ping and Su, Li and others},
  year = {2018},
  pages = {90--97}
}

@article{chenInfoGANInterpretable2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.03657}
}

@article{chenJNetRandomly2019,
  title = {J-{{Net}}: {{Randomly}} Weighted {{U}}-{{Net}} for Audio Source Separation},
  shorttitle = {J-{{Net}}},
  author = {Chen, Bo-Wen and Hsu, Yen-Min and Lee, Hung-Yi},
  year = {2019},
  month = nov,
  abstract = {Several results in the computer vision literature have shown the potential of randomly weighted neural networks. While they perform fairly well as feature extractors for discriminative tasks, a positive correlation exists between their performance and their fully trained counterparts. According to these discoveries, we pose two questions: what is the value of randomly weighted networks in difficult generative audio tasks such as audio source separation and does such positive correlation still exist when it comes to large random networks and their trained counterparts? In this paper, we demonstrate that the positive correlation still exists. Based on this discovery, we can try out different architecture designs or tricks without training the whole model. Meanwhile, we find a surprising result that in comparison to the non-trained encoder (down-sample path) in Wave-U-Net, fixing the decoder (up-sample path) to random weights results in better performance, almost comparable to the fully trained model.},
  archivePrefix = {arXiv},
  eprint = {1911.12926},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/V3ENYQI3/Chen et al. - 2019 - J-Net Randomly weighted U-Net for audio source se.pdf;/home/daniel/Zotero/storage/ZXQA8GSN/1911.html},
  journal = {arXiv:1911.12926 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{chenSimpleFramework2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = feb,
  volume = {abs/2002.05709},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archivePrefix = {arXiv},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/D6PQ4I7B/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/home/daniel/Zotero/storage/QJWPUJBP/2002.html},
  journal = {CoRR},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{chenTrainingDeep2016,
  title = {Training {{Deep Nets}} with {{Sublinear Memory Cost}}},
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  year = {2016},
  month = apr,
  abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
  archivePrefix = {arXiv},
  eprint = {1604.06174},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/W6YAKRZS/Chen et al. - 2016 - Training Deep Nets with Sublinear Memory Cost.pdf;/home/daniel/Zotero/storage/WMSF5RUN/1604.html},
  journal = {arXiv:1604.06174 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{chettriEnsembleModels2019,
  title = {Ensemble Models for Spoofing Detection in Automatic Speaker Verification},
  booktitle = {Proceedings of {{INTERSPEECH}}},
  author = {Chettri, Bhusan and Stoller, Daniel and Morfi, Veronica and Ram{\'i}rez, Marco A. Mart{\'i}nez and Benetos, Emmanouil and Sturm, Bob L.},
  year = {2019},
  pages = {1018--1022},
  code = {https://github.com/BhusanChettri/ASVspoof2019/}
}

@incollection{chewConceptualExperiential2013,
  title = {Conceptual and {{Experiential Representations}} of {{Tempo}}: {{Effects}} on {{Expressive Performance Comparisons}}},
  booktitle = {Mathematics and {{Computation}} in {{Music}}},
  author = {Chew, Elaine and Callender, Clifton},
  year = {2013},
  pages = {76--87},
  publisher = {{Springer}}
}

@article{chienAlignmentLyrics2016,
  title = {Alignment of {{Lyrics With Accompanied Singing Audio Based}} on {{Acoustic}}-{{Phonetic Vowel Likelihood Modeling}}},
  author = {Chien, Yu-Ren and Wang, Hsin-Min and Jeng, Shyh-Kang},
  year = {2016},
  volume = {24},
  pages = {1998--2008},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  number = {11}
}

@article{childGeneratingLong2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  volume = {abs/1904.10509},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash{}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archivePrefix = {arXiv},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/85XWZE2T/Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf;/home/daniel/Zotero/storage/LZW9KK82/1904.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{choiTransferLearning2018,
  title = {Transfer Learning for Music Classification and Regression Tasks},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Choi, Keunwoo and Fazekas, Gy{\"o}rgy and Sandler, Mark and Cho, Kyunghyun},
  year = {2018},
  volume = {18},
  pages = {141--149}
}

@article{choLearningPhrase2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  volume = {CoRR},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archivePrefix = {arXiv},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/TWZGP8LU/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/home/daniel/Zotero/storage/DN7ZAHV5/1406.html},
  journal = {abs/1406.1078},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{chorowskiUnsupervisedSpeech2019,
  title = {Unsupervised Speech Representation Learning Using {{WaveNet}} Autoencoders},
  author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, A{\"a}ron},
  year = {2019},
  month = dec,
  volume = {27},
  pages = {2041--2053},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2019.2938863},
  abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\textbackslash{} phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
  archivePrefix = {arXiv},
  eprint = {1901.08810},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/3T4EKZ5E/Chorowski et al. - 2019 - Unsupervised speech representation learning using .pdf;/home/daniel/Zotero/storage/VAV7QRX6/1901.html},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  number = {12}
}

@article{chuanGeneratingEvaluating2011,
  title = {Generating and Evaluating Musical Harmonizations That Emulate Style},
  author = {Chuan, Ching-Hua and Chew, Elaine},
  year = {2011},
  volume = {35},
  pages = {64--82},
  journal = {Computer Music Journal},
  number = {4}
}

@article{chungAudioWord2Vec2016,
  title = {Audio {{Word2Vec}}: {{Unsupervised Learning}} of {{Audio Segment Representations}} Using {{Sequence}}-to-Sequence {{Autoencoder}}},
  shorttitle = {Audio {{Word2Vec}}},
  author = {Chung, Yu-An and Wu, Chao-Chung and Shen, Chia-Hao and Lee, Hung-Yi and Lee, Lin-Shan},
  year = {2016},
  month = mar,
  abstract = {The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning.},
  archivePrefix = {arXiv},
  eprint = {1603.00982},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/RBPTCHAG/Chung et al. - 2016 - Audio Word2Vec Unsupervised Learning of Audio Seg.pdf;/home/daniel/Zotero/storage/WLKVPPUR/1603.html},
  journal = {arXiv:1603.00982 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@article{chungEmpiricalEvaluation2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  volume = {CoRR},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archivePrefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/MP6FIGTW/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf;/home/daniel/Zotero/storage/4SMXXMU7/1412.html},
  journal = {abs/1412.3555},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{chungHierarchicalMultiscale2016,
  title = {Hierarchical {{Multiscale Recurrent Neural Networks}}},
  author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  year = {2016},
  volume = {abs/1609.01704},
  journal = {CoRR}
}

@inproceedings{chungRecurrentLatent2015,
  title = {A Recurrent Latent Variable Model for Sequential Data},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  year = {2015},
  pages = {2980--2988}
}

@article{chungSpeech2VecSequencetoSequence2018,
  title = {{{Speech2Vec}}: {{A Sequence}}-to-{{Sequence Framework}} for {{Learning Word Embeddings}} from {{Speech}}},
  shorttitle = {{{Speech2Vec}}},
  author = {Chung, Yu-An and Glass, James},
  year = {2018},
  month = mar,
  abstract = {In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.},
  archivePrefix = {arXiv},
  eprint = {1803.08976},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/8IMRJJMJ/Chung und Glass - 2018 - Speech2Vec A Sequence-to-Sequence Framework for L.pdf;/home/daniel/Zotero/storage/ZVUS5VR3/1803.html},
  journal = {arXiv:1803.08976 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{ciarleglioModularAbstract2008,
  title = {Modular Abstract Self-Learning Tabu Search ({{MASTS}}): {{Metaheuristic}} Search Theory and Practice},
  author = {Ciarleglio, Michael Ian},
  year = {2008}
}

@article{clarkeCategoricalRhythm1987,
  title = {Categorical Rhythm Perception: An Ecological Perspective},
  author = {Clarke, Eric F},
  year = {1987},
  volume = {55},
  pages = {19--33},
  journal = {Action and perception in rhythm and music}
}

@article{clevelandLocallyWeighted1988,
  title = {Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting},
  author = {Cleveland, William S. and Devlin, Susan J.},
  year = {1988},
  volume = {83},
  pages = {596--610},
  journal = {Journal of the American statistical association},
  number = {403}
}

@inproceedings{cordtsCityscapesDataset2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  booktitle = {Proc. of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  year = {2016}
}

@article{correaSurveySymbolic2016,
  title = {A Survey on Symbolic Data-Based Music Genre Classification},
  author = {Corr{\^e}a, D{\'e}bora C and Rodrigues, Francisco Ap},
  year = {2016},
  volume = {60},
  pages = {190--210},
  publisher = {{Elsevier}},
  journal = {Expert Systems with Applications}
}

@inproceedings{cramerLookListen2019,
  title = {Look, Listen, and Learn More: {{Design}} Choices for Deep Audio Embeddings},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cramer, Jason and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
  year = {2019},
  pages = {3852--3856},
  organization = {{IEEE}}
}

@article{creswellGenerativeAdversarial2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = jan,
  volume = {35},
  pages = {53--65},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  archivePrefix = {arXiv},
  eprint = {1710.07035},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/F6ZCBEQ2/Creswell et al. - 2018 - Generative Adversarial Networks An Overview.pdf;/home/daniel/Zotero/storage/29V5YELN/1710.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  number = {1}
}

@article{cuddyRecognitionTransposed1976,
  title = {Recognition of Transposed Melodic Sequences},
  author = {Cuddy, Lola L and Cohen, Annabel J},
  year = {1976},
  volume = {28},
  pages = {255--270},
  journal = {The Quarterly Journal of Experimental Psychology},
  number = {2}
}

@article{cybenkoApproximationSuperpositions1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, George},
  year = {1989},
  volume = {2},
  pages = {303--314},
  journal = {Mathematics of control, signals and systems},
  number = {4}
}

@inproceedings{daiANALYSISINTONATION2015,
  title = {{{ANALYSIS OF INTONATION TRAJECTORIES IN SOLO SINGING}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Dai, Jiajie and Mauch, Matthias and Dixon, Simon},
  year = {2015},
  volume = {421}
}

@article{daiCalibratingEnergybased2017,
  title = {Calibrating Energy-Based Generative Adversarial Networks},
  author = {Dai, Zihang and Almahairi, Amjad and Bachman, Philip and Hovy, Eduard and Courville, Aaron},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.01691}
}

@article{daiGoodSemisupervised2017,
  title = {Good {{Semi}}-Supervised {{Learning}} That {{Requires}} a {{Bad GAN}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Fan and Cohen, William W and Salakhutdinov, Ruslan},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.09783}
}

@article{danihelkaComparisonMaximum2017,
  title = {Comparison of {{Maximum Likelihood}} and {{GAN}}-Based Training of {{Real NVPs}}},
  author = {Danihelka, Ivo and Lakshminarayanan, Balaji and Uria, Benigno and {Daan Wierstra} and Dayan, Peter},
  year = {2017},
  volume = {abs/1705.05263},
  file = {/home/daniel/Zotero/storage/R8LYNVYI/Danihelka et al. - 2017 - Comparison of Maximum Likelihood and GAN-based tra.pdf},
  journal = {CoRR}
}

@inproceedings{daviesContextDependentBeat2007,
  title = {Context-{{Dependent Beat Tracking}} of {{Musical Audio}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Davies, M. E. P. and Plumbley, M. D.},
  year = {2007},
  volume = {15},
  pages = {1009--1020}
}

@incollection{davisSoundReinforcement1989,
  title = {The Sound Reinforcement Handbook},
  author = {Davis, Gary and Jones, Ralph},
  editor = {Corporation, Yamaha},
  year = {1989},
  pages = {201--203},
  publisher = {{Hal Leonard Corporation}}
}

@inproceedings{defferrardFMADataset2017,
  title = {{{FMA}}: {{A Dataset}} for {{Music Analysis}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Defferrard, Micha{\"e}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
  year = {2017},
  pages = {316--323},
  file = {/home/daniel/Zotero/storage/Q7CPEM8E/Benzi et al. - 2016 - FMA A Dataset For Music Analysis.pdf}
}

@article{defossezDemucsDeep2019,
  title = {Demucs: {{Deep Extractor}} for {{Music Sources}} with Extra Unlabeled Data Remixed},
  shorttitle = {Demucs},
  author = {D{\'e}fossez, Alexandre and Usunier, Nicolas and Bottou, L{\'e}on and Bach, Francis},
  year = {2019},
  month = sep,
  abstract = {We study the problem of source separation for music using deep learning with four known sources: drums, bass, vocals and other accompaniments. State-of-the-art approaches predict soft masks over mixture spectrograms while methods working on the waveform are lagging behind as measured on the standard MusDB benchmark. Our contribution is two fold. (i) We introduce a simple convolutional and recurrent model that outperforms the state-of-the-art model on waveforms, that is, Wave-U-Net, by 1.6 points of SDR (signal to distortion ratio). (ii) We propose a new scheme to leverage unlabeled music. We train a first model to extract parts with at least one source silent in unlabeled tracks, for instance without bass. We remix this extract with a bass line taken from the supervised dataset to form a new weakly supervised training example. Combining our architecture and scheme, we show that waveform methods can play in the same ballpark as spectrogram ones.},
  archivePrefix = {arXiv},
  eprint = {1909.01174},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/DJ8MKP3V/Défossez et al. - 2019 - Demucs Deep Extractor for Music Sources with extr.pdf;/home/daniel/Zotero/storage/GJS9Z54H/1909.html},
  journal = {arXiv:1909.01174 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{dekromCepstrumbasedTechnique1993,
  title = {A Cepstrum-Based Technique for Determining a Harmonics-to-Noise Ratio in Speech Signals},
  author = {{de Krom}, Guus},
  year = {1993},
  volume = {36},
  pages = {254--266},
  journal = {Journal of Speech, Language, and Hearing Research},
  number = {2}
}

@inproceedings{delannoyWeightedSVMs2010,
  title = {Weighted {{SVMs}} and Feature Relevance Assessment in Supervised Heart Beat Classification},
  booktitle = {International {{Joint Conference}} on {{Biomedical Engineering Systems}} and {{Technologies}}},
  author = {De Lannoy, Gael and Fran{\c c}ois, Damien and Delbeke, Jean and Verleysen, Michel},
  year = {2010},
  pages = {212--223},
  publisher = {{Springer}}
}

@article{delongComparingAreas1988,
  title = {Comparing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach},
  author = {DeLong, Elizabeth R and DeLong, David M and {Clarke-Pearson}, Daniel L},
  year = {1988},
  pages = {837--845},
  journal = {Biometrics}
}

@inproceedings{demetriouVocalsMusic2018,
  title = {Vocals in {{Music Matter}}: {{The Relevance}} of {{Vocals}} in the {{Minds}} of {{Listeners}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Demetriou, Andrew and Jansson, Andreas and Kumar, Aparna and M. Bittner, Rachel},
  year = {2018},
  month = sep,
  volume = {19},
  pages = {514--520}
}

@inproceedings{dengBinaryCoding2010,
  title = {Binary Coding of Speech Spectrograms Using a Deep Auto-Encoder.},
  booktitle = {Interspeech},
  author = {Deng, Li and Seltzer, Michael L and Yu, Dong and Acero, Alex and Mohamed, Abdel-Rahman and Hinton, Geoffrey E},
  year = {2010},
  pages = {1692--1695},
  publisher = {{Citeseer}}
}

@inproceedings{dengImageNetLargescale2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {{{CVPR09}}},
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and {Fei-Fei}, L.},
  year = {2009},
  bibsource = {http://www.image-net.org/papers/imagenet\textsubscript{c}vpr09.bib}
}

@inproceedings{dengNewTypes2013,
  title = {New Types of Deep Neural Network Learning for Speech Recognition and Related Applications: {{An}} Overview},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
  year = {2013},
  pages = {8599--8603},
  publisher = {{IEEE}}
}

@inproceedings{dentonDeepGenerative2015,
  title = {Deep {{Generative Image Models}} Using A￼ {{Laplacian Pyramid}} of {{Adversarial Networks}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Denton, Emily L and Chintala, Soumith and Fergus, Rob and others},
  year = {2015},
  pages = {1486--1494}
}

@inproceedings{derrienQuasiorthogonalInvertible2015,
  title = {A Quasi-Orthogonal, Invertible, and Perceptually Relevant Time-Frequency Transform for Audio Coding},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), 2015 23rd {{European}}},
  author = {Derrien, Olivier and Necciarf, Thibaud and Balazs, Peter},
  year = {2015},
  pages = {799--803},
  publisher = {{IEEE}}
}

@article{desainDoesExpressive1994,
  title = {Does Expressive Timing in Music Performance Scale Proportionally with Tempo?},
  author = {Desain, Peter and Honing, Henkjan},
  year = {1994},
  volume = {56},
  pages = {285--292},
  journal = {Psychological Research},
  number = {4}
}

@article{desaiSpectralMapping2010,
  title = {Spectral Mapping Using Artificial Neural Networks for Voice Conversion},
  author = {Desai, Srinivas and Black, Alan W and Yegnanarayana, B and Prahallad, Kishore},
  year = {2010},
  volume = {18},
  pages = {954--964},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  number = {5}
}

@article{devlinBERTPretraining2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  volume = {abs/1810.04805},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/VBXVJ9B5/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/daniel/Zotero/storage/CKM7V7CB/1810.html},
  journal = {CoRR},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{dielemanAudiobasedMusic2011,
  title = {Audio-Based Music Classification with a Pretrained Convolutional Network},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Dieleman, Sander and Brakel, Phil{\'e}mon and Schrauwen, Benjamin},
  year = {2011},
  pages = {669--674},
  publisher = {{University of Miami}}
}

@inproceedings{dielemanChallengeRealistic2018,
  title = {The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Dieleman, Sander and {van den Oord}, A{\"a}ron and Simonyan, Karen},
  year = {2018},
  pages = {8000--8010}
}

@book{dielemanLasagneFirst2015,
  title = {Lasagne: {{First}} Release.},
  author = {Dieleman, S. and Schl{\"u}ter, J. and Raffel, C. and Olson, E. and S{\o}nderby, S. K. and Nouri, D. and Maturana, D. and Thoma, M. and Battenberg, E. and Kelly, J. and De Fauw, J. and Heilman, M. and {de Almeida}, D. M. and McFee, B. and Weideman, H. and Tak{\'a}cs, G. and {de Rivaz}, P. and Crall, J. and Sanders, G. and Rasul, K. and Liu, C. and French, G. and Degrave, J.},
  year = {2015},
  month = aug,
  doi = {10.5281/zenodo.27878}
}

@article{dijkstraNoteTwo1959,
  title = {A Note on Two Problems in Connexion with Graphs.},
  author = {Dijkstra, E. W.},
  year = {1959},
  volume = {1},
  pages = {269--271},
  journal = {Numerische Mathematik}
}

@article{dinhDensityEstimation2016,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.08803}
}

@article{dixonAirWorm2005,
  title = {The" {{Air Worm}}": {{An Interface}} for {{Real}}-{{Time Manipulation}} of {{Expressive Music Performance}}},
  author = {Dixon, Simon and Goebl, Werner and Widmer, Gerhard},
  year = {2005},
  pages = {614--617},
  journal = {Proc. ICMC'05}
}

@article{doerschTutorialVariational2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.05908}
}

@article{doireInterleavedMultitask2019,
  title = {Interleaved {{Multitask Learning}} for {{Audio Source Separation}} with {{Independent Databases}}},
  author = {Doire, Clement S. J. and Okubadejo, Olumide},
  year = {2019},
  month = aug,
  abstract = {Deep Neural Network-based source separation methods usually train independent models to optimize for the separation of individual sources. Although this can lead to good performance for well-defined targets, it can also be computationally expensive. The multitask alternative of a single network jointly optimizing for all targets simultaneously usually requires the availability of all target sources for each input. This requirement hampers the ability to create large training databases. In this paper, we present a model that decomposes the learnable parameters into a shared parametric model (encoder) and independent components (decoders) specific to each source. We propose an interleaved training procedure that optimizes the sub-task decoders independently and thus does not require each sample to possess a ground truth for all of its composing sources. Experimental results on MUSDB18 with the proposed method show comparable performance to independently trained models, with less trainable parameters, more efficient inference, and an encoder transferable to future target objectives. The results also show that using the proposed interleaved training procedure leads to better Source-to-Interference energy ratios when compared to the simultaneous optimization of all training objectives, even when all composing sources are available.},
  archivePrefix = {arXiv},
  eprint = {1908.05182},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/SI5JY3PN/Doire und Okubadejo - 2019 - Interleaved Multitask Learning for Audio Source Se.pdf;/home/daniel/Zotero/storage/HY9IBNGU/1908.html},
  journal = {arXiv:1908.05182 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{doireOnlineSinging2019,
  title = {Online {{Singing Voice Separation Using}} a {{Recurrent One}}-Dimensional {{U}}-{{NET Trained}} with {{Deep Feature Losses}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Doire, Clement S. J.},
  year = {2019},
  month = may,
  pages = {3752--3756},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683251},
  isbn = {978-1-4799-8131-1}
}

@article{dollingerAktuelleMethoden,
  title = {Aktuelle {{Methoden}} Zur {{Modellierung}} Des {{Stimmgebungsprozesses}}},
  author = {D{\"o}llinger, Ing M and Kniesburges, S and Kaltenbacher, M and Echternach, M},
  pages = {1--8},
  journal = {HNO}
}

@inproceedings{donahueAdversarialAudio2019,
  title = {Adversarial {{Audio Synthesis}}},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
  year = {2019}
}

@article{donahueDeCAFDeep2013,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  year = {2013},
  volume = {abs/1310.1531},
  file = {/home/daniel/Zotero/storage/R7X76BHM/Donahue et al. - 2013 - DeCAF A Deep Convolutional Activation Feature for.pdf},
  journal = {CoRR}
}

@inproceedings{dongConvolutionalGenerative2018,
  title = {Convolutional {{Generative Adversarial Networks}} with {{Binary Neurons}} for {{Polyphonic Music Generation}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Dong, Hao-Wen and Yang, Yi-Hsuan},
  year = {2018},
  pages = {190--196}
}

@inproceedings{dongMuseGANMultitrack2018,
  title = {{{MuseGAN}}: {{Multi}}-Track {{Sequential Generative Adversarial Networks}} for {{Symbolic Music Generation}} and {{Accompaniment}}},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Dong, Hao-Wen and Hsiao, Wen-Yi and Yang, Li-Chia and Yang, Yi-Hsuan},
  year = {2018},
  pages = {34--41}
}

@article{dongPrincipledMissing2013,
  title = {Principled Missing Data Methods for Researchers},
  author = {Dong, Yiran and Peng, Chao-Ying Joanne},
  year = {2013},
  volume = {2},
  pages = {1},
  journal = {SpringerPlus},
  number = {1}
}

@article{dowlingPerceptionInterleaved1973,
  title = {The Perception of Interleaved Melodies},
  author = {Dowling, W Jay},
  year = {1973},
  volume = {5},
  pages = {322--337},
  journal = {Cognitive psychology},
  number = {3}
}

@article{dowlingRhythmicGroups1973,
  title = {Rhythmic Groups and Subjective Chunks in Memory for Melodies},
  author = {Dowling, W Jay},
  year = {1973},
  volume = {14},
  pages = {37--40},
  journal = {Perception \& Psychophysics},
  number = {1}
}

@article{dowlingScaleContour1978,
  title = {Scale and Contour: {{Two}} Components of a Theory of Memory for Melodies.},
  author = {Dowling, W Jay},
  year = {1978},
  volume = {85},
  pages = {341},
  journal = {Psychological review},
  number = {4}
}

@article{dowlingTonalStrength1991,
  title = {Tonal Strength and Melody Recognition after Long and Short Delays},
  author = {Dowling, W Jay},
  year = {1991},
  volume = {50},
  pages = {305--313},
  journal = {Perception \& Psychophysics},
  number = {4}
}

@inproceedings{driedgerTSMToolbox2014,
  title = {{{TSM Toolbox}}: {{MATLAB Implementations}} of {{Time}}-{{Scale Modification Algorithms}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Digital Audio Effects}} ({{DAFx}})},
  author = {Driedger, J. and M{\"u}ller, M.},
  year = {2014},
  pages = {249--256},
  address = {{Erlangen, Germany}}
}

@inproceedings{drugmanJointRobust2011,
  title = {Joint {{Robust Voicing Detection}} and {{Pitch Estimation Based}} on {{Residual Harmonics}}.},
  booktitle = {12th {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Drugman, Thomas and Alwan, Abeer},
  year = {2011},
  pages = {1973--1976}
}

@inproceedings{duanNUSSung2013,
  title = {The {{NUS}} Sung and Spoken Lyrics Corpus: {{A}} Quantitative Comparison of Singing and Speech},
  booktitle = {Signal and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}}), 2013 {{Asia}}-{{Pacific}}},
  author = {Duan, Zhiyan and Fang, Haotian and Li, Bo and Sim, Khe Chai and Wang, Ye},
  year = {2013},
  pages = {1--9},
  publisher = {{IEEE}}
}

@article{dumoulinAdversariallyLearned2016,
  title = {Adversarially {{Learned Inference}}},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Lamb, Alex and Arjovsky, Martin and Mastropietro, Olivier and Courville, Aaron},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.00704}
}

@article{dumoulinGuideConvolution2016,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2016},
  journal = {arXiv preprint arXiv:1603.07285}
}

@inproceedings{durandEnhancingDownbeat2014,
  title = {Enhancing Downbeat Detection When Facing Different Music Styles},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Durand, Simon and David, Barak and Richard, Guilhem},
  year = {2014},
  pages = {3132--3136},
  publisher = {{IEEE}}
}

@book{durbinBiologicalSequence1998,
  title = {Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids},
  author = {Durbin, Richard and Eddy, Sean R and Krogh, Anders and Mitchison, Graeme},
  year = {1998},
  publisher = {{Cambridge university press}}
}

@article{durrieuSourceFilter2013,
  title = {Source/{{Filter Factorial Hidden Markov Model}}, {{With Application}} to {{Pitch}} and {{Formant Tracking}}},
  author = {Durrieu, Jean-Louis and Thiran, Jean-Philippe},
  year = {2013},
  volume = {21},
  pages = {2541--2553},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {12}
}

@article{durugkarGenerativeMultiAdversarial2016,
  title = {Generative {{Multi}}-{{Adversarial Networks}}},
  author = {Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar},
  year = {2016},
  month = nov,
  abstract = {Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \textbackslash{}emph\{Generative Multi-Adversarial Network\} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.},
  archivePrefix = {arXiv},
  eprint = {1611.01673},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/EJ8L3VXM/Durugkar et al. - 2016 - Generative Multi-Adversarial Networks.pdf;/home/daniel/Zotero/storage/GWXYLR7E/1611.html},
  journal = {arXiv:1611.01673 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@inproceedings{dzhambazovModelingPhoneme2015,
  title = {Modeling of Phoneme Durations for Alignment between Polyphonic Audio and Lyrics},
  booktitle = {Proceedings of the {{Sound}} and {{Music Computing Conference}} ({{SMC}})},
  author = {Dzhambazov, Georgi and Serra, Xavier},
  year = {2015}
}

@inproceedings{einbondFinetunedControl2014,
  title = {Fine-Tuned Control of Concatenative Synthesis with Catart Using the Bach Library for {{Max}}},
  booktitle = {International {{Computer Music Conference}} ({{ICMC}})},
  author = {Einbond, Aaron and Trapani, Christopher and Agostini, Andrea and Ghisi, Daniele and Schwarz, Diemo},
  year = {2014},
  pages = {1037--1042},
  file = {/home/daniel/Zotero/storage/QTTKRB7X/Einbond, Trapani, Agostini, Ghisi, Schwarz - Fine-tuned control of concatenative synthesis with cataRT using the bach library for Max.pdf}
}

@inproceedings{elhihiHierarchicalRecurrent1995,
  title = {Hierarchical {{Recurrent Neural Networks}} for {{Long}}-Term {{Dependencies}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {El Hihi, Salah and Bengio, Yoshua},
  year = {1995},
  pages = {493--499},
  publisher = {{MIT Press}},
  address = {{Denver, Colorado}},
  acmid = {2998898},
  numpages = {7},
  series = {{{NIPS}}'95}
}

@inproceedings{elkanFoundationsCostsensitive2001,
  title = {The Foundations of Cost-Sensitive Learning},
  booktitle = {International Joint Conference on Artificial Intelligence},
  author = {Elkan, Charles},
  year = {2001},
  volume = {17},
  pages = {973--978},
  publisher = {{Lawrence Erlbaum Associates LTD}}
}

@article{emiyaSubjectiveObjective2011,
  title = {Subjective and {{Objective Quality Assessment}} of {{Audio Source Separation}}},
  author = {Emiya, V. and Vincent, E. and Harlander, N. and Hohmann, V.},
  year = {2011},
  month = sep,
  volume = {19},
  pages = {2046--2057},
  issn = {1558-7916},
  doi = {10.1109/TASL.2011.2109381},
  abstract = {We aim to assess the perceived quality of estimated source signals in the context of audio source separation. These signals may involve one or more kinds of distortions, including distortion of the target source, interference from the other sources or musical noise artifacts. We propose a subjective test protocol to assess the perceived quality with respect to each kind of distortion and collect the scores of 20 subjects over 80 sounds. We then propose a family of objective measures aiming to predict these subjective scores based on the decomposition of the estimation error into several distortion components and on the use of the PEMO-Q perceptual salience measure to provide multiple features that are then combined. These measures increase correlation with subjective scores up to 0.5 compared to nonlinear mapping of individual state-of-the-art source separation measures. Finally, we released the data and code presented in this paper in a freely available toolkit called PEASS.},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {acoustic distortion,Audio,audio signal processing,audio source separation,Distortion measurement,estimation error,estimation theory,interference (signal),musical acoustics,musical noise artifact,Noise,Nonlinear distortion,nonlinear mapping,objective measure,objective quality assessment,PEASS,PEMO-Q perceptual salience measure,Protocols,quality assessment,signal sources,signals distortion,source separation,Source separation,source signal estimation,Speech,subjective test protocol},
  number = {7}
}

@inproceedings{erdoganPhasesensitiveRecognitionboosted2015,
  title = {Phase-Sensitive and Recognition-Boosted Speech Separation Using Deep Recurrent Neural Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Erdogan, Hakan and Hershey, John R and Watanabe, Shinji and Le Roux, Jonathan},
  year = {2015},
  pages = {708--712},
  publisher = {{IEEE}}
}

@article{erhanUnderstandingRepresentations2010,
  title = {Understanding Representations Learned in Deep Architectures},
  author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua},
  year = {2010},
  volume = {1355},
  journal = {Department d'Informatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep}
}

@book{everestMasterHandbook2009,
  title = {Master {{Handbook}} of {{Acoustics}}},
  author = {Everest, F. A. and Pohlmann, K.},
  year = {2009},
  publisher = {{McGraw-Hill Education}}
}

@article{ewertScoreInformedSource2014,
  title = {Score-{{Informed Source Separation}} for {{Musical Audio Recordings}}: {{An Overview}}},
  author = {Ewert, Sebastian and Pardo, Bryan and M{\"u}ller, Meinard and Plumbley, Mark D.},
  year = {2014},
  volume = {31},
  pages = {116--124},
  issn = {1053-5888},
  doi = {10.1109/MSP.2013.2296076},
  file = {/home/daniel/Zotero/storage/XNHDKZHQ/Ewert et al. - 2014 - Score-Informed Source Separation for Musical Audio.pdf},
  journal = {IEEE Signal Processing Magazine},
  number = {3}
}

@article{ewertStructuredDropout2016,
  title = {Structured {{Dropout}} for {{Weak Label}} and {{Multi}}-{{Instance Learning}} and {{Its Application}} to {{Score}}-{{Informed Source Separation}}},
  author = {Ewert, Sebastian and Sandler, Mark B},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.04557}
}

@inproceedings{ezzatMorphingSpectral2005,
  title = {Morphing Spectral Envelopes Using Audio Flow.},
  booktitle = {12th {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Ezzat, Tony and Meyers, Ethan and Glass, James R and Poggio, Tomaso},
  year = {2005},
  pages = {2545--2548}
}

@article{fabiusVariationalRecurrent2014,
  title = {Variational Recurrent Auto-Encoders},
  author = {Fabius, Otto and {van Amersfoort}, Joost R},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6581}
}

@article{fanSVSGANSinging2017,
  title = {{{SVSGAN}}: {{Singing Voice Separation}} via {{Generative Adversarial Network}}},
  author = {Fan, Zhe-Cheng and Lai, Yen-Lin and Jang, Jyh-Shing Roger},
  year = {2017},
  volume = {abs/1710.11428},
  file = {/home/daniel/Zotero/storage/BV9J2FHZ/Fan et al. - 2017 - SVSGAN Singing Voice Separation via Generative Ad.pdf},
  journal = {CoRR}
}

@article{farboodHyperscoreGraphical2004,
  title = {Hyperscore: A Graphical Sketchpad for Novice Composers},
  author = {Farbood, Morwaread M and Pasztor, Egon and Jennings, Kevin},
  year = {2004},
  volume = {24},
  pages = {50--54},
  journal = {Computer Graphics and Applications, IEEE},
  number = {1}
}

@book{fastlPsychoacousticsFacts2007,
  title = {Psychoacoustics: {{Facts}} and Models},
  author = {Fastl, H. and Zwicker, E.},
  year = {2007},
  volume = {22},
  publisher = {{Springer Science \& Business Media}}
}

@article{fernandezAIMethods2013,
  title = {{{AI}} Methods in Algorithmic Composition: {{A}} Comprehensive Survey},
  author = {Fern{\'a}ndez, Jose D and Vico, Francisco},
  year = {2013},
  pages = {513--582},
  journal = {Journal of Artificial Intelligence Research}
}

@inproceedings{fernandezSequenceLabelling2007,
  title = {Sequence {{Labelling}} in {{Structured Domains}} with {{Hierarchical Recurrent Neural Networks}}.},
  booktitle = {{{IJCAI}}},
  author = {Fern{\'a}ndez, Santiago and Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2007},
  pages = {774--779}
}

@techreport{fevotteBASSEVAL2005,
  title = {{{BASS}}\_{{EVAL Toolbox User Guide}}: {{IRISA Technical Report}} 1706},
  author = {Fevotte, C. and Gribonval, R. and Vincent, E.},
  year = {2005}
}

@incollection{fevotteBayesianAudio2007,
  title = {Bayesian Audio Source Separation},
  booktitle = {Blind {{Speech Separation}}},
  author = {F{\'e}votte, C{\'e}dric},
  year = {2007},
  pages = {305--335},
  publisher = {{Springer}}
}

@inproceedings{finnModelAgnosticMetaLearning2017,
  title = {Model-{{Agnostic Meta}}-{{Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1126--1135},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{fletcherRelationLoudness1937,
  title = {Relation between Loudness and Masking},
  author = {Fletcher, H. and Munson, W. A.},
  year = {1937},
  volume = {9},
  pages = {78--78},
  journal = {The Journal of the Acoustical Society of America},
  number = {1}
}

@inproceedings{footeAutomaticAudio2000,
  title = {Automatic Audio Segmentation Using a Measure of Audio Novelty},
  booktitle = {Multimedia and {{Expo}}, 2000. {{ICME}} 2000. 2000 {{IEEE International Conference}} On},
  author = {Foote, Jonathan},
  year = {2000},
  volume = {1},
  pages = {452--455},
  publisher = {{IEEE}}
}

@inproceedings{footeMediaSegmentation2003,
  title = {Media Segmentation Using Self-Similarity Decomposition},
  booktitle = {Electronic {{Imaging}} 2003},
  author = {Foote, J. T. and Cooper, M. L.},
  year = {2003},
  pages = {167--175},
  publisher = {{International Society for Optics and Photonics}}
}

@article{fraccaroSequentialNeural2016,
  title = {Sequential {{Neural Models}} with {{Stochastic Layers}}},
  author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.07571}
}

@inproceedings{franziniConnectionistViterbi1990,
  title = {Connectionist {{Viterbi}} Training: A New Hybrid Method for Continuous Speech Recognition},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Franzini, Michael and Lee, K-F and Waibel, Alex},
  year = {1990},
  pages = {425--428}
}

@article{fredmanFibonacciHeaps1987,
  title = {Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms},
  author = {Fredman, M. L. and Tarjan, R. E.},
  year = {1987},
  volume = {34},
  pages = {596--615},
  journal = {Journal of the ACM (JACM)},
  number = {3}
}

@article{froeschelsHygieneVoice1943,
  title = {Hygiene of the Voice},
  author = {Froeschels, Emil},
  year = {1943},
  volume = {38},
  pages = {122--130},
  journal = {Archives of Otolaryngology},
  number = {2}
}

@inproceedings{fujiharaAutomaticSynchronization2006,
  title = {Automatic {{Synchronization}} between {{Lyrics}} and {{Music CD Recordings Based}} on {{Viterbi Alignment}} of {{Segregated Vocal Signals}}},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Multimedia}} ({{ISM}})},
  author = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun and Komatani, Kazunori and Ogata, Tetsuya and Okuno, Hiroshi G.},
  year = {2006},
  pages = {257--264}
}

@inproceedings{fujiharaLyricstoAudioAlignment2012,
  title = {Lyrics-to-{{Audio Alignment}} and Its {{Application}}},
  booktitle = {Multimodal {{Music Processing}}},
  author = {Fujihara, Hiromasa and Goto, Masataka},
  year = {2012}
}

@article{fujiharaLyricSynchronizerAutomatic2011,
  title = {{{LyricSynchronizer}}: {{Automatic}} Synchronization System between Musical Audio Signals and Lyrics},
  author = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun and Okuno, Hiroshi G},
  year = {2011},
  volume = {5},
  pages = {1252--1261},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  number = {6}
}

@article{fujiharaModelingSinging2010,
  title = {A Modeling of Singing Voice Robust to Accompaniment Sounds and Its Application to Singer Identification and Vocal-Timbre-Similarity-Based Music Information Retrieval},
  author = {Fujihara, Hiromasa and Goto, Masataka and Kitahara, Tetsuro and Okuno, Hiroshi G},
  year = {2010},
  volume = {18},
  pages = {638--648},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  number = {3}
}

@article{fukushimaNeocognitronSelforganizing1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  volume = {36},
  pages = {193--202},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00344251},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4}
}

@article{fuMetricGANGenerative2019,
  title = {{{MetricGAN}}: {{Generative Adversarial Networks}} Based {{Black}}-Box {{Metric Scores Optimization}} for {{Speech Enhancement}}},
  shorttitle = {{{MetricGAN}}},
  author = {Fu, Szu-Wei and Liao, Chien-Feng and Tsao, Yu and Lin, Shou-De},
  year = {2019},
  month = may,
  abstract = {Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by Lp or conventional adversarial losses.},
  archivePrefix = {arXiv},
  eprint = {1905.04874},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/C45G7H82/Fu et al. - 2019 - MetricGAN Generative Adversarial Networks based B.pdf;/home/daniel/Zotero/storage/DBZDZTPC/1905.html},
  journal = {arXiv:1905.04874 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{galDropoutBayesian2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  volume = {48},
  pages = {1050--1059}
}

@article{galModernDeep,
  title = {On {{Modern Deep Learning}} and {{Variational Inference}}},
  author = {Gal, Yarin and Ghahramani, Zoubin}
}

@article{galRapidPrototyping,
  title = {Rapid {{Prototyping}} of {{Probabilistic Models}}: {{Emerging Challenges}} in {{Variational Inference}}},
  author = {Gal, Yarin}
}

@inproceedings{ganinUnsupervisedDomain2015,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  pages = {1180--1189}
}

@article{ganTriangleGenerative2017,
  title = {Triangle {{Generative Adversarial Networks}}},
  author = {Gan, Zhe and Chen, Liqun and Wang, Weiyao and Pu, Yunchen and Zhang, Yizhe and Liu, Hao and Li, Chunyuan and Carin, Lawrence},
  year = {2017},
  month = sep,
  volume = {abs/1709.06548},
  abstract = {A Triangle Generative Adversarial Network (\$\textbackslash{}Delta\$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. \$\textbackslash{}Delta\$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.},
  archivePrefix = {arXiv},
  eprint = {1709.06548},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/T5TD8R7A/Gan et al. - 2017 - Triangle Generative Adversarial Networks.pdf;/home/daniel/Zotero/storage/WAN4M8PL/1709.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{gemmekeAudioSet2017,
  title = {Audio {{Set}}: {{An}} Ontology and Human-Labeled Dataset for Audio Events},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  year = {2017},
  address = {{New Orleans, LA}}
}

@book{genesisMATLABLoudness2009,
  title = {{{MATLAB Loudness Toolbox}}},
  author = {{Genesis}},
  year = {2009}
}

@inproceedings{gershmanAmortizedInference2014,
  title = {Amortized Inference in Probabilistic Reasoning},
  booktitle = {Proceedings of the 36th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Gershman, Samuel J and Goodman, Noah D},
  year = {2014}
}

@inproceedings{girshickRichFeature2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {Proceedings of the 2014 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  pages = {580--587},
  publisher = {{IEEE Computer Society}},
  address = {{USA}},
  doi = {10.1109/CVPR.2014.81},
  isbn = {978-1-4799-5118-5},
  numpages = {8},
  series = {{{CVPR}} '14}
}

@article{glasbergModelLoudness2002,
  title = {A Model of Loudness Applicable to Time-Varying Sounds},
  author = {Glasberg, B. R. and Moore, Moore, B. C. J.},
  year = {2002},
  volume = {50},
  pages = {331--342},
  journal = {Journal of the Audio Engineering Society},
  number = {5}
}

@incollection{gomezCompressedNetwork2012,
  title = {Compressed Network Complexity Search},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}-{{PPSN XII}}},
  author = {Gomez, Faustino and Koutn{\'i}k, Jan and Schmidhuber, J{\"u}rgen},
  year = {2012},
  pages = {316--326},
  publisher = {{Springer}}
}

@inproceedings{gongImpactAliasing2018,
  title = {Impact of {{Aliasing}} on {{Deep CNN}}-{{Based End}}-to-{{End Acoustic Models}}},
  booktitle = {Interspeech 2018},
  author = {Gong, Yuan and Poellabauer, Christian},
  year = {2018},
  month = sep,
  pages = {2698--2702},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1371},
  language = {en}
}

@unpublished{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016}
}

@inproceedings{goodfellowGenerativeAdversarial2014,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  pages = {2672--2680}
}

@inproceedings{goodfellowMultipredictionDeep2013,
  title = {Multi-Prediction Deep {{Boltzmann}} Machines},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  year = {2013},
  pages = {548--556}
}

@article{graisMultiBandMultiResolution2019,
  title = {Multi-{{Band Multi}}-{{Resolution Fully Convolutional Neural Networks}} for {{Singing Voice Separation}}},
  author = {Grais, Emad M. and Zhao, Fei and Plumbley, Mark D.},
  year = {2019},
  month = oct,
  abstract = {Deep neural networks with convolutional layers usually process the entire spectrogram of an audio signal with the same time-frequency resolutions, number of filters, and dimensionality reduction scale. According to the constant-Q transform, good features can be extracted from audio signals if the low frequency bands are processed with high frequency resolution filters and the high frequency bands with high time resolution filters. In the spectrogram of a mixture of singing voices and music signals, there is usually more information about the voice in the low frequency bands than the high frequency bands. These raise the need for processing each part of the spectrogram differently. In this paper, we propose a multi-band multi-resolution fully convolutional neural network (MBR-FCN) for singing voice separation. The MBR-FCN processes the frequency bands that have more information about the target signals with more filters and smaller dimentionality reduction scale than the bands with less information. Furthermore, the MBR-FCN processes the low frequency bands with high frequency resolution filters and the high frequency bands with high time resolution filters. Our experimental results show that the proposed MBR-FCN with very few parameters achieves better singing voice separation performance than other deep neural networks.},
  archivePrefix = {arXiv},
  eprint = {1910.09266},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/K345JPLE/Grais et al. - 2019 - Multi-Band Multi-Resolution Fully Convolutional Ne.pdf;/home/daniel/Zotero/storage/TBLMQ3FN/1910.html},
  journal = {arXiv:1910.09266 [cs, eess, stat]},
  keywords = {68T01; 68T10; 68T45; 62H25,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing,H.5.5,I.2,I.2.6,I.4,I.4.3,I.5,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{graisRawMultiChannel2018,
  title = {Raw {{Multi}}-{{Channel Audio Source Separation}} Using {{Multi}}-{{Resolution Convolutional Auto}}-{{Encoders}}},
  author = {Grais, Emad M and Ward, Dominic and Plumbley, Mark D},
  year = {2018},
  journal = {arXiv preprint arXiv:1803.00702}
}

@article{graisReferencelessPerformance2018,
  title = {Referenceless {{Performance Evaluation}} of {{Audio Source Separation}} Using {{Deep Neural Networks}}},
  author = {Grais, Emad M. and Wierstorf, Hagen and Ward, Dominic and Mason, Russell and Plumbley, Mark D.},
  year = {2018},
  month = nov,
  abstract = {Current performance evaluation for audio source separation depends on comparing the processed or separated signals with reference signals. Therefore, common performance evaluation toolkits are not applicable to real-world situations where the ground truth audio is unavailable. In this paper, we propose a performance evaluation technique that does not require reference signals in order to assess separation quality. The proposed technique uses a deep neural network (DNN) to map the processed audio into its quality score. Our experiment results show that the DNN is capable of predicting the sources-to-artifacts ratio from the blind source separation evaluation toolkit without the need for reference signals.},
  archivePrefix = {arXiv},
  eprint = {1811.00454},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/P3T5GPSG/Grais et al. - 2018 - Referenceless Performance Evaluation of Audio Sour.pdf;/home/daniel/Zotero/storage/246LEGUZ/1811.html},
  journal = {arXiv:1811.00454 [cs, eess]},
  keywords = {68T01; 68T10; 68T45; 62H25,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,H.5.5,I.2,I.2.6,I.4,I.4.3,I.5},
  primaryClass = {cs, eess}
}

@article{graveImprovingNeural2016,
  title = {Improving {{Neural Language Models}} with a {{Continuous Cache}}},
  author = {Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  year = {2016},
  journal = {arXiv preprint arXiv:1612.04426}
}

@inproceedings{gravesConnectionistTemporal2006,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376}
}

@inproceedings{gravesEndToEndSpeech2014,
  title = {Towards {{End}}-{{To}}-{{End Speech Recognition}} with {{Recurrent Neural Networks}}.},
  booktitle = {{{ICML}}},
  author = {Graves, Alex and Jaitly, Navdeep},
  year = {2014},
  volume = {14},
  pages = {1764--1772},
  publisher = {{IEEE}}
}

@article{gravesNeuralTuring2014,
  title = {Neural Turing Machines},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  journal = {arXiv preprint arXiv:1410.5401}
}

@article{gravesSequenceTransduction2012,
  title = {Sequence Transduction with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  journal = {arXiv preprint arXiv:1211.3711}
}

@inproceedings{gravesSpeechRecognition2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Graves, Alan and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  year = {2013},
  pages = {6645--6649}
}

@article{greffTaggerDeep2016,
  title = {Tagger: {{Deep Unsupervised Perceptual Grouping}}},
  author = {Greff, Klaus and Rasmus, Antti and Berglund, Mathias and Hao, Tele Hotloo and Schmidhuber, J{\"u}rgen and Valpola, Harri},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.06724}
}

@article{gregorDRAWRecurrent2015,
  title = {{{DRAW}}: {{A}} Recurrent Neural Network for Image Generation},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
  year = {2015},
  journal = {arXiv preprint arXiv:1502.04623}
}

@article{griffinSignalEstimation1984,
  title = {Signal Estimation from Modified Short-Time {{Fourier}} Transform},
  author = {Griffin, D. and Lim, Jae},
  year = {1984},
  volume = {32},
  pages = {236--243},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1984.1164317},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  keywords = {Degradation,Discrete Fourier transforms,Estimation theory,Fourier transforms,Hardware,Iterative algorithms,Monitoring,Sampling methods,Signal processing,Speech enhancement},
  number = {2}
}

@inproceedings{grillMusicBoundary2015,
  title = {Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Grill, Thomas and Schl{\"u}ter, Jan},
  year = {2015},
  pages = {531--537}
}

@article{grilloEvidenceDistinguishing2008,
  title = {Evidence for {{Distinguishing Pressed}}, {{Normal}}, {{Resonant}}, and {{Breathy Voice Qualities}} by {{Laryngeal Resistance}} and {{Vocal Efficiency}} in {{Vocally Trained Subjects}}},
  author = {Grillo, Elizabeth U. and {and}, Katherine Verdolini},
  year = {2008},
  volume = {22},
  pages = {546--552},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/j.jvoice.2006.12.008},
  abstract = {Summary The purpose of this study was to determine if pressed, normal, resonant, and breathy voice qualities can be distinguished from one another by laryngeal resistance (LR; cm H2O/l/s) and/or vocal efficiency (VE; dB/cm \{H2O\} \texttimes{} l/s) in vocally, trained subjects. The experimental design was a within-subjects repeated measures design. Independent variables were pressed, normal, resonant, and breathy voice qualities. Dependent variables were \{LR\} and VE. Participants were 13 women of age 18\textendash{}45 years with established vocal expertise. After a brief training phase, subjects were asked to produce each of the voice qualities on the pitch \{A3\} (220 Hz) at a constant, individually identified comfortable dB level ({$\pm$}1 dB), during a repeated consonant-vowel utterance of /pi pi pi pi pi/. Results indicated that \{LR\} but not \{VE\} reliably distinguished pressed, normal, and breathy voice. Neither of the measures, however, distinguished normal from resonant voice, which were distinguished perceptually. The results suggest that \{LR\} may provide a useful tool for studying the coordinative dynamics of pressed, normal, and breathy voice qualities.},
  journal = {Journal of Voice},
  keywords = {Laryngeal resistance},
  number = {5}
}

@article{groscheExtractingPredominant2011,
  title = {Extracting {{Predominant Local Pulse Information}} from {{Music Recordings}}},
  author = {Grosche, P. and M{\"u}ller, M.},
  year = {2011},
  volume = {19},
  pages = {1688--1701},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {6}
}

@article{gulrajaniImprovedTraining2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Mart{\'i}n and {Vincent Dumoulin} and Courville, Aaron C.},
  year = {2017},
  volume = {abs/1704.00028},
  file = {/home/daniel/Zotero/storage/6RZURIE6/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf},
  journal = {CoRR}
}

@article{guptaAcousticModeling2019,
  title = {Acoustic {{Modeling}} for {{Automatic Lyrics}}-to-{{Audio Alignment}}},
  author = {Gupta, Chitralekha and Y\i{}lmaz, Emre and Li, Haizhou},
  year = {2019},
  month = jun,
  abstract = {Automatic lyrics to polyphonic audio alignment is a challenging task not only because the vocals are corrupted by background music, but also there is a lack of annotated polyphonic corpus for effective acoustic modeling. In this work, we propose (1) using additional speech and music-informed features and (2) adapting the acoustic models trained on a large amount of solo singing vocals towards polyphonic music using a small amount of in-domain data. Incorporating additional information such as voicing and auditory features together with conventional acoustic features aims to bring robustness against the increased spectro-temporal variations in singing vocals. By adapting the acoustic model using a small amount of polyphonic audio data, we reduce the domain mismatch between training and testing data. We perform several alignment experiments and present an in-depth alignment error analysis on acoustic features, and model adaptation techniques. The results demonstrate that the proposed strategy provides a significant error reduction of word boundary alignment over comparable existing systems, especially on more challenging polyphonic data with long-duration musical interludes.},
  archivePrefix = {arXiv},
  eprint = {1906.10369},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/TSLQEF6S/Gupta et al. - 2019 - Acoustic Modeling for Automatic Lyrics-to-Audio Al.pdf;/home/daniel/Zotero/storage/3DCSI37X/1906.html},
  journal = {arXiv:1906.10369 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{guptaAutomaticLyrics2019,
  title = {Automatic {{Lyrics Transcription}} in {{Polyphonic Music}}: {{Does Background Music Help}}?},
  shorttitle = {Automatic {{Lyrics Transcription}} in {{Polyphonic Music}}},
  author = {Gupta, Chitralekha and Y\i{}lmaz, Emre and Li, Haizhou},
  year = {2019},
  month = sep,
  abstract = {Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.},
  archivePrefix = {arXiv},
  eprint = {1909.10200},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/RDZV7AVH/Gupta et al. - 2019 - Automatic Lyrics Transcription in Polyphonic Music.pdf;/home/daniel/Zotero/storage/A22HZ49G/1909.html},
  journal = {arXiv:1909.10200 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{guptaPerceptualRelevance2015,
  title = {On the Perceptual Relevance of Objective Source Separation Measures for Singing Voice Separation},
  booktitle = {2015 {{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}} ({{WASPAA}})},
  author = {Gupta, U. and Moore, E. and Lerch, A.},
  year = {2015},
  month = oct,
  pages = {1--5},
  doi = {10.1109/WASPAA.2015.7336923},
  abstract = {Singing Voice Separation (SVS) is a task which uses audio source separation methods to isolate the vocal component from the background accompaniment for a song mix. This paper discusses the methods of evaluating SVS algorithms, and determines how the current state of the art measures correlate to human perception. A modified ITU-R BS.1543 MUSHRA test is used to get the human perceptual ratings for the outputs of various SVS algorithms, which are correlated with widely used objective measures for source separation quality. The results show that while the objective measures provide a moderate correlation with perceived intelligibility and isolation, they may not adequately assess the overall perceptual quality.},
  keywords = {1543 MUSHRA test,audio signal processing,audio source separation method,Correlation,Distortion,Distortion measurement,information retrieval,Instruments,ITU-R BS,MUSHRA,music,Music Information Retrieval,objective source separation,perceptual relevance,Radio frequency,Signal processing algorithms,singing voice separation,Singing Voice Separation,source separation,Source separation,Source Separation,SVS algorithm}
}

@inproceedings{guptaSemisupervisedLyrics2018,
  title = {Semi-Supervised Lyrics and Solo-Singing Alignment},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Gupta, Chitralekha and Tong, Rong and Li, Haizhou and Wang, Ye},
  year = {2018},
  pages = {600--607},
  file = {/home/daniel/Zotero/storage/QVRBXUVV/Gupta, Tong, Li, Wang - Semi-supervised lyrics and solo-singing alignment.pdf}
}

@inproceedings{hadsellDimensionalityReduction2006,
  title = {Dimensionality Reduction by Learning an Invariant Mapping},
  booktitle = {Computer Vision and Pattern Recognition, 2006 {{IEEE}} Computer Society Conference On},
  author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  year = {2006},
  volume = {2},
  pages = {1735--1742},
  publisher = {{IEEE}}
}

@inproceedings{hamelTemporalPooling2011,
  title = {Temporal {{Pooling}} and {{Multiscale Learning}} for {{Automatic Annotation}} and {{Ranking}} of {{Music Audio}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Hamel, Philippe and Lemieux, Simon and Bengio, Yoshua and Eck, Douglas},
  year = {2011},
  pages = {729--734}
}

@inproceedings{hanLearningBoth2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  year = {2015},
  pages = {1135--1143}
}

@article{hannonMetricalCategories2005,
  title = {Metrical Categories in Infancy and Adulthood},
  author = {Hannon, Erin E and Trehub, Sandra E},
  year = {2005},
  volume = {16},
  pages = {48--55},
  journal = {Psychological Science},
  number = {1}
}

@article{hannonTuningMusical2005,
  title = {Tuning in to Musical Rhythms: {{Infants}} Learn More Readily than Adults},
  author = {Hannon, Erin E and Trehub, Sandra E},
  year = {2005},
  volume = {102},
  pages = {12639--12643},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {35}
}

@inproceedings{hansenRecognitionPhonemes2012,
  title = {Recognition of Phonemes in A-Cappella Recordings Using Temporal Patterns and Mel Frequency Cepstral Coefficients},
  booktitle = {9th {{Sound}} and {{Music Computing Conference}} ({{SMC}})},
  author = {Hansen, Jens Kofod and Fraunhofer, IDMT},
  year = {2012},
  pages = {494--499}
}

@inproceedings{hartFormalBasis1968,
  title = {A {{Formal Basis}} for the {{Heuristic Determination}} of {{Minimum Cost Paths}}},
  booktitle = {{{IEEE Transactions}} on {{Systems Science}} and {{Cybernetics}}},
  author = {Hart, P. E. and Nilsson, N. J. and Raphael, B.},
  year = {1968},
  volume = {4},
  pages = {100--107},
  keywords = {Automatic control,Automatic programming,Chemical technology,Costs,Functional programming,Gradient methods,Instruction sets,Mathematical programming,Minimax techniques,Minimization methods}
}

@article{hauslerTemporalAutoencoding2012,
  title = {Temporal {{Autoencoding Restricted Boltzmann Machine}}},
  author = {H{\"a}usler, Chris and Susemihl, Alex},
  year = {2012},
  journal = {arXiv preprint arXiv:1210.8353}
}

@inproceedings{hawthorneEnablingFactorized2019,
  title = {Enabling {{Factorized Piano Music Modeling}} and {{Generation}} with the {{MAESTRO Dataset}}},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
  year = {2019}
}

@article{heDeepResidual2015,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  volume = {abs/1512.03385},
  journal = {CoRR}
}

@inproceedings{heittolaSoundEvent2011,
  title = {Sound Event Detection in Multisource Environments Using Source Separation},
  booktitle = {Machine {{Listening}} in {{Multisource Environments}}},
  author = {Heittola, Toni and Mesaros, Annamaria and Virtanen, Tuomas and Eronen, Antti},
  year = {2011},
  file = {/home/daniel/Zotero/storage/U9WHMQ6W/Heittola, Mesaros, Virtanen, Eronen - Sound event detection in multisource environments using source separation.pdf}
}

@article{heRethinkingImageNet2018,
  title = {Rethinking {{ImageNet Pre}}-Training},
  author = {He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  year = {2018},
  month = nov,
  abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.},
  archivePrefix = {arXiv},
  eprint = {1811.08883},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/5S6LKJIF/He et al. - 2018 - Rethinking ImageNet Pre-training.pdf;/home/daniel/Zotero/storage/8HQBYN2W/1811.html},
  journal = {arXiv:1811.08883 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{hersheyDeepClustering2016,
  title = {Deep Clustering: {{Discriminative}} Embeddings for Segmentation and Separation},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hershey, John R and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
  year = {2016},
  pages = {31--35},
  publisher = {{IEEE}}
}

@article{hersheyDeepUnfolding2014,
  title = {Deep Unfolding: {{Model}}-Based Inspiration of Novel Deep Architectures},
  author = {Hershey, John R and Roux, Jonathan Le and Weninger, Felix},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.2574}
}

@inproceedings{heuselGANsTrained2017,
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  booktitle = {Advances {{In Neural Information Processing Systems}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and {Bernhard Nessler} and Hochreiter, Sepp},
  year = {2017},
  pages = {6629--6640},
  file = {/home/daniel/Zotero/storage/2RW2GGLT/Heusel, Ramsauer, Unterthiner, Nessler - GANs trained by a two time-scale update rule converge to a local nash equilibrium.pdf}
}

@article{higginsEarlyVisual2016,
  title = {Early {{Visual Concept Learning}} with {{Unsupervised Deep Learning}}},
  author = {Higgins, Irina and Matthey, Loic and Glorot, Xavier and Pal, Arka and Uria, Benigno and Blundell, Charles and Mohamed, Shakir and Lerchner, Alexander},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.05579}
}

@inproceedings{higuchiAdversarialTraining2017,
  title = {Adversarial Training for Data-Driven Speech Enhancement without Parallel Corpus},
  booktitle = {Automatic {{Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}}), 2017 {{IEEE}}},
  author = {Higuchi, Takuya and Kinoshita, Keisuke and Delcroix, Marc and Nakatani, Tomohiro},
  year = {2017},
  pages = {40--47},
  publisher = {{IEEE}}
}

@article{hillenbrandAcousticCorrelates1994,
  title = {Acoustic Correlates of Breathy Vocal Quality},
  author = {Hillenbrand, James and Cleveland, Ronald A. and Erickson, Robert L.},
  year = {1994},
  volume = {37},
  pages = {769--778},
  journal = {Journal of Speech, Language, and Hearing Research},
  number = {4}
}

@article{hintonImprovingNeural2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  month = jul,
  volume = {abs/1207.0580},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archivePrefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/J9G2KMTC/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf;/home/daniel/Zotero/storage/I8NKUNZZ/1207.html},
  journal = {CoRR},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{hintonReducingDimensionality2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  year = {2006},
  volume = {313},
  pages = {504--507},
  journal = {Science},
  number = {5786}
}

@article{hjelmBoundarySeekingGenerative2017,
  title = {Boundary-{{Seeking Generative Adversarial Networks}}},
  author = {Hjelm, R Devon and Jacob, Athul Paul and Che, Tong and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.08431}
}

@article{hochreiterVanishingGradient1998,
  title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
  author = {Hochreiter, Sepp},
  year = {1998},
  volume = {6},
  pages = {107--116},
  publisher = {{World Scientific}},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number = {02}
}

@inproceedings{hockmanAutomatedRhythmic2008,
  title = {Automated Rhythmic Transformation of Musical Audio},
  booktitle = {Proceedings of 11th {{International Conference}} on {{Digital Audio Effects}} ({{DAFx}})},
  author = {Hockman, Jason A and Bello, Juan P and Davies, Matthew EP and Plumbley, Mark D},
  year = {2008},
  pages = {177--180},
  publisher = {{Citeseer}}
}

@article{hollienOldVoices1987,
  title = {``{{Old}} Voices\dbend{}?: {{What}} Do We Really Know about Them?},
  author = {Hollien, Harry},
  year = {1987},
  volume = {1},
  pages = {2--17},
  journal = {Journal of voice},
  number = {1}
}

@article{honingSwingOnce2008,
  title = {Swing Once More: {{Relating}} Timing and Tempo in Expert Jazz Drumming},
  author = {Honing, Henkjan and De Haas, W Bas},
  year = {2008}
}

@inproceedings{hoshenUnsupervisedSinglechannel2019,
  title = {Towards {{Unsupervised Single}}-Channel {{Blind Source Separation Using Adversarial Pair Unmix}}-and-Remix},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hoshen, Yedid},
  year = {2019},
  month = may,
  pages = {3272--3276},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682375},
  file = {/home/daniel/Zotero/storage/BHKA466K/Hoshen - 2019 - Towards Unsupervised Single-channel Blind Source S.pdf},
  isbn = {978-1-4799-8131-1}
}

@article{howardFinetunedLanguage2018,
  title = {Fine-Tuned {{Language Models}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  volume = {abs/1801.06146},
  file = {/home/daniel/Zotero/storage/D842ZK5C/Howard und Ruder - 2018 - Fine-tuned Language Models for Text Classification.pdf},
  journal = {CoRR}
}

@article{huangConnectionistTemporal2016,
  title = {Connectionist {{Temporal Modeling}} for {{Weakly Supervised Action Labeling}}},
  author = {Huang, De-An and {Fei-Fei}, Li and Niebles, Juan Carlos},
  year = {2016},
  volume = {abs/1607.08584},
  file = {/home/daniel/Zotero/storage/GSDUAT4U/Huang et al. - 2016 - Connectionist Temporal Modeling for Weakly Supervi.pdf},
  journal = {CoRR}
}

@inproceedings{huangSingingvoiceSeparation2012,
  title = {Singing-Voice Separation from Monaural Recordings Using Robust Principal Component Analysis},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Huang, Po-Sen and Chen, Scott Deeann and Smaragdis, Paris and {Hasegawa-Johnson}, Mark},
  year = {2012},
  pages = {57--60},
  publisher = {{IEEE}}
}

@inproceedings{huangSingingVoiceSeparation2014,
  title = {Singing-{{Voice Separation}} from {{Monaural Recordings}} Using {{Deep Recurrent Neural Networks}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Huang, Po-Sen and Kim, Minje and {Hasegawa-Johnson}, Mark and Smaragdis, Paris},
  year = {2014},
  pages = {477--482}
}

@inproceedings{humphreyMiningLabeled2017,
  title = {Mining Labeled Data from Web-Scale Collections for Vocal Activity Detection in Music},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Humphrey, Eric and Montecchio, Nicola and Bittner, Rachel and Jansson, Andreas and Jehan, Tristan},
  year = {2017},
  file = {/home/daniel/Zotero/storage/PBSLKIIL/Humphrey, Montecchio, Bittner, Jansson, Jehan - Mining labeled data from web-scale collections for vocal activity detection in music.pdf}
}

@inproceedings{humphreyNonlinearSemantic2011,
  title = {Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries},
  booktitle = {International {{Conference}} on {{Machine Learning}} and {{Applications}} and {{Workshops}} ({{ICMLA}})},
  author = {Humphrey, Eric J and Glennon, Aron P and Bello, Juan Pablo},
  year = {2011},
  volume = {2},
  pages = {142--147},
  publisher = {{IEEE}}
}

@article{huszarHowNot2015,
  title = {How (Not) to {{Train}} Your {{Generative Model}}: {{Scheduled Sampling}}, {{Likelihood}}, {{Adversary}}?},
  author = {Husz{\'a}r, Ferenc},
  year = {2015},
  volume = {abs/1511.05101},
  journal = {CoRR}
}

@inproceedings{hwangCharacterlevelIncremental2016,
  title = {Character-Level Incremental Speech Recognition with Recurrent Neural Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hwang, Kyuyeon and Sung, Wonyong},
  year = {2016},
  pages = {5335--5339},
  publisher = {{IEEE}}
}

@inproceedings{hwangSequenceSequence2016,
  title = {Sequence to {{Sequence Training}} of {{CTC}}-{{RNNs}} with {{Partial Windowing}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Hwang, Kyuyeon and Sung, Wonyong},
  year = {2016},
  pages = {2178--2187}
}

@article{hyvarinenFastRobust1999,
  title = {Fast and Robust Fixed-Point Algorithms for Independent Component Analysis},
  author = {Hyvarinen, Aapo},
  year = {1999},
  volume = {10},
  pages = {626--634},
  journal = {IEEE transactions on Neural Networks},
  number = {3}
}

@inproceedings{ikemiyaSingingVoice2015,
  title = {Singing Voice Analysis and Editing Based on Mutually Dependent {{F0}} Estimation and Source Separation},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ikemiya, Yukara and Yoshii, Kazuyoshi and Itoyama, Katsutoshi},
  year = {2015},
  pages = {574--578},
  publisher = {{IEEE}}
}

@article{ikemiyaSingingVoice2016,
  title = {Singing {{Voice Separation}} and {{Vocal F0 Estimation}} Based on {{Mutual Combination}} of {{Robust Principal Component Analysis}} and {{Subharmonic Summation}}},
  author = {Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
  year = {2016},
  journal = {arXiv preprint arXiv:1604.00192}
}

@misc{imirselMusicInformation2020,
  title = {Music {{Information Retrieval Exchange}} ({{MIREX}})},
  author = {IMIRSEL},
  year = {2020},
  month = mar,
  howpublished = {https://www.music-ir.org/mirex/wiki/MIREX\_HOME}
}

@inproceedings{imQuantitativelyEvaluating2018,
  title = {Quantitatively {{Evaluating GANs With Divergences Proposed}} for {{Training}}},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Im, Daniel Jiwoong and Ma, Alllan He and Taylor, Graham W. and Branson, Kristin},
  year = {2018}
}

@inproceedings{ioannidisCaracterisationClassification2014,
  title = {Caract{\'e}risation et Classification Automatique Des Modes Phonatoires En Voix Chant{\'e}e},
  booktitle = {{{XXX{\`e}mes Journ{\'e}es}} d'{\'e}tudes Sur La Parole},
  author = {Ioannidis, L{\'e}onidas and Rouas, Jean-Luc and {Desainte-Catherine}, Myriam},
  year = {2014}
}

@article{isolaImagetoImageTranslation2016,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2016},
  month = nov,
  volume = {abs/1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archivePrefix = {arXiv},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/A4YTSNNH/Isola et al. - 2016 - Image-to-Image Translation with Conditional Advers.pdf;/home/daniel/Zotero/storage/FMVK6I96/1611.html},
  journal = {CoRR},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{ituRecommendationITUR2014,
  title = {Recommendation {{ITU}}-{{R BS}}.1534-2: {{Method}} for the Subjective Assessment of Intermediate Quality Level of Audio Systems},
  author = {{ITU}},
  year = {2014}
}

@article{iversonAuditoryStream1995,
  title = {Auditory Stream Segregation by Musical Timbre: Effects of Static and Dynamic Acoustic Attributes.},
  author = {Iverson, Paul},
  year = {1995},
  volume = {21},
  pages = {751},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  number = {4}
}

@article{jaakkola10Tutorial2001,
  title = {10 {{Tutorial}} on {{Variational Approximation Methods}}},
  author = {Jaakkola, Tommi S},
  year = {2001},
  pages = {129},
  journal = {Advanced mean field methods: theory and practice}
}

@article{jaitlyNeuralTransducer2015,
  title = {A Neural Transducer},
  author = {Jaitly, Navdeep and Sussillo, David and Le, Quoc V and Vinyals, Oriol and Sutskever, Ilya and Bengio, Samy},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.04868}
}

@article{jakubikSimilarityBasedSummarization2018,
  title = {Similarity-{{Based Summarization}} of {{Music Files}} for {{Support Vector Machines}}},
  author = {Jakubik, Jan and Kwa{\'s}nicka, Halina},
  editor = {J{\k{e}}drzejowicz, Piotr},
  year = {2018},
  month = aug,
  volume = {2018},
  pages = {1935938},
  issn = {1076-2787},
  doi = {10.1155/2018/1935938},
  abstract = {Automatic retrieval of music information is an active area of research in which problems such as automatically assigning genres or descriptors of emotional content to music emerge. Recent advancements in the area rely on the use of deep learning, which allows researchers to operate on a low-level description of the music. Deep neural network architectures can learn to build feature representations that summarize music files from data itself, rather than expert knowledge. In this paper, a novel approach to applying feature learning in combination with support vector machines to musical data is presented. A spectrogram of the music file, which is too complex to be processed by SVM, is first reduced to a compact representation by a recurrent neural network. An adjustment to loss function of the network is proposed so that the network learns to build a representation space that replicates a certain notion of similarity between annotations, rather than to explicitly make predictions. We evaluate the approach on five datasets, focusing on emotion recognition and complementing it with genre classification. In experiments, the proposed loss function adjustment is shown to improve results in classification and regression tasks, but only when the learned similarity notion corresponds to a kernel function employed within the SVM. These results suggest that adjusting deep learning methods to build data representations that target a specific classifier or regressor can open up new perspectives for the use of standard machine learning methods in music domain.},
  journal = {Complexity}
}

@book{janermestresSingingdrivenInterfaces2008,
  title = {Singing-Driven Interfaces for Sound Synthesizers},
  author = {Janer Mestres, Jordi and others},
  year = {2008},
  publisher = {{Universitat Pompeu Fabra}}
}

@article{jangCategoricalReparameterization2016,
  title = {Categorical {{Reparameterization}} with {{Gumbel}}-{{Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.01144}
}

@inproceedings{janssonSingingVoice2017,
  title = {Singing {{Voice Separation}} with {{Deep U}}-{{Net Convolutional Networks}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Jansson, Andreas and Humphrey, Eric J. and Montecchio, Nicola and Bittner, Rachel and Kumar, Aparna and Weyde, Tillman},
  year = {2017},
  pages = {323--332}
}

@inproceedings{jinFFTNetRealTime2018,
  title = {{{FFTNet}}: A {{Real}}-{{Time Speaker}}-{{Dependent Neural Vocoder}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jin, Zeyu and Finkelstein, Adam and Mysore, Gautham J. and Lu, Jingwan},
  year = {2018},
  month = apr
}

@inproceedings{johnsonComposingGraphical2016,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances {{In Neural Information Processing Systems}}},
  author = {Johnson, Matthew and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
  year = {2016},
  pages = {2946--2954}
}

@article{jozefowiczExploringLimits2016,
  title = {Exploring the Limits of Language Modeling},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.02410}
}

@article{juWhenVariational2016,
  title = {When {{Variational Auto}}-Encoders Meet {{Generative Adversarial Networks}}},
  author = {Ju, Jianbo Chen Billy Fang Cheng},
  year = {2016}
}

@article{kabalTSPSpeech2002,
  title = {{{TSP}} Speech Database},
  author = {Kabal, Peter},
  year = {2002},
  volume = {1},
  pages = {09--02},
  journal = {McGill University, Database Version},
  number = {0}
}

@article{kabirNewWrapper2010,
  title = {A New Wrapper Feature Selection Approach Using Neural Network},
  author = {Kabir, Md Monirul and Islam, Md Monirul and Murase, Kazuyuki},
  year = {2010},
  volume = {73},
  pages = {3273--3283},
  journal = {Neurocomputing},
  number = {16}
}

@phdthesis{kainHighResolution2001,
  title = {High Resolution Voice Transformation},
  author = {Kain, Alexander Blouke},
  year = {2001},
  school = {Oregon Health \& Science University},
  type = {{{PhD Thesis}}}
}

@inproceedings{kalchbrennerEfficientNeural2018,
  title = {Efficient {{Neural Audio Synthesis}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and {van den Oord}, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {2410--2419},
  publisher = {{PMLR}},
  address = {{Stockholmsm{\"a}ssan, Stockholm Sweden}},
  abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24 kHz 16-bit audio 4 times faster than real time on a GPU. Secondly, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds past sparsity levels of more than 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile phone CPU in real time. Finally, we describe a new dependency scheme for sampling that lets us trade a constant number of non-local, distant dependencies for the ability to generate samples in batches. The Batch WaveRNN produces 8 samples per step without loss of quality and offers orthogonal ways of further increasing sampling efficiency.}
}

@article{kalchbrennerNeuralMachine2016,
  title = {Neural {{Machine Translation}} in {{Linear Time}}},
  author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and van den Oord, Aaron and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = oct,
  volume = {abs/1610.10099},
  abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
  archivePrefix = {arXiv},
  eprint = {1610.10099},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/QK7CTIGY/Kalchbrenner et al. - 2016 - Neural Machine Translation in Linear Time.pdf;/home/daniel/Zotero/storage/R5BGFRYD/1610.html},
  journal = {CoRR},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{kaneIdentifyingRegions2011,
  title = {Identifying {{Regions}} of {{Non}}-{{Modal Phonation Using Features}} of the {{Wavelet Transform}}},
  booktitle = {12th {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Kane, John and Gobl, Christer},
  year = {2011},
  pages = {177--180}
}

@article{kaneWaveletMaxima2013,
  title = {Wavelet Maxima Dispersion for Breathy to Tense Voice Discrimination},
  author = {Kane, John and Gobl, Christer},
  year = {2013},
  volume = {21},
  pages = {1170--1179},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {6}
}

@article{karaletsosAdversarialMessage2016,
  title = {Adversarial {{Message Passing For Graphical Models}}},
  author = {Karaletsos, Theofanis},
  year = {2016},
  month = dec,
  volume = {abs/1612.05048},
  abstract = {Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables. However, inference in implicit models or complex posterior distributions is hard. A popular tool for learning implicit models are generative adversarial networks (GANs) which learn parameters of generators by fooling discriminators. Typically, GANs are considered to be models themselves and are not understood in the context of inference. Current techniques rely on inefficient global discrimination of joint distributions to perform learning, or only consider discriminating a single output variable. We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs. We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations. This allows us to compose models and yields a unified inference and learning framework for adversarial learning. Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs, including components such as intractable likelihoods, non-differentiable models, simulators and generally cumbersome models. A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families, without access to explicit distributions. As a side-result, we discuss the link to likelihood maximization. These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications.},
  archivePrefix = {arXiv},
  eprint = {1612.05048},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/GDXSN9RI/Karaletsos - 2016 - Adversarial Message Passing For Graphical Models.pdf;/home/daniel/Zotero/storage/VYKDDHTE/1612.html},
  journal = {CoRR},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning}
}

@inproceedings{karrasProgressiveGrowing2018,
  title = {Progressive Growing of {{GANs}} for Improved Quality, Stability, and Variation},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018}
}

@article{karrasStyleBasedGenerator2018,
  title = {A {{Style}}-{{Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2018},
  month = dec,
  volume = {abs/1812.04948},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archivePrefix = {arXiv},
  eprint = {1812.04948},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/DAVXFHVD/Karras et al. - 2018 - A Style-Based Generator Architecture for Generativ.pdf;/home/daniel/Zotero/storage/TYTKHPHA/1812.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{kavalerovUniversalSound2019,
  title = {Universal {{Sound Separation}}},
  author = {Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Brian and Wilson, Kevin and Roux, Jonathan Le and Hershey, John R.},
  year = {2019},
  month = may,
  abstract = {Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown whether performance on speech tasks carries over to non-speech tasks. To study this question, we develop a universal dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore using either a short-time Fourier transform (STFT) or a learnable basis, as used in ConvTasNet, and for both of these bases, we examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.},
  archivePrefix = {arXiv},
  eprint = {1905.03330},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/G6D872UE/Kavalerov et al. - 2019 - Universal Sound Separation.pdf;/home/daniel/Zotero/storage/ABRKT4AW/1905.html},
  journal = {arXiv:1905.03330 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{kawaiLyricRecognition2017,
  title = {Lyric Recognition in Monophonic Singing Using Pitch-Dependent {{DNN}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kawai, Dairoku and Yamamoto, Kazumasa and Nakagawa, Seiichi},
  year = {2017},
  pages = {326--330},
  doi = {10.1109/ICASSP.2017.7952171},
  file = {/home/daniel/Zotero/storage/GGMIAWZF/Kawai, Yamamoto, Nakagawa - Lyric recognition in monophonic singing using pitch-dependent DNN.pdf}
}

@inproceedings{kelzMultitaskLearning2019,
  title = {Multitask Learning for Polyphonic Piano Transcription, a Case Study},
  booktitle = {International {{Workshop}} on {{Multilayer Music Representation}} and {{Processing}} ({{MMRP}})},
  author = {Kelz, Rainer and B{\"o}ck, Sebastian and Widnaer, Cierhard},
  year = {2019},
  pages = {85--91},
  organization = {{IEEE}}
}

@inproceedings{kendallMultiTaskLearning2018,
  title = {Multi-{{Task Learning Using Uncertainty}} to {{Weigh Losses}} for {{Scene Geometry}} and {{Semantics}}},
  booktitle = {Proc. of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  year = {2018}
}

@article{kimAreNearby2019,
  title = {Are {{Nearby Neighbors Relatives}}?: {{Diagnosing Deep Music Embedding Spaces}}},
  shorttitle = {Are {{Nearby Neighbors Relatives}}?},
  author = {Kim, Jaehun and Urbano, Juli{\'a}n and Liem, Cynthia C. S. and Hanjalic, Alan},
  year = {2019},
  month = apr,
  abstract = {Deep neural networks have frequently been used to directly learn representations useful for a given task from raw input data. In terms of overall performance metrics, machine learning solutions employing deep representations frequently have been reported to greatly outperform those using hand-crafted feature representations. At the same time, they may pick up on aspects that are predominant in the data, yet not actually meaningful or interpretable. In this paper, we therefore propose a systematic way to diagnose the trustworthiness of deep music representations, considering musical semantics. The underlying assumption is that in case a deep representation is to be trusted, distance consistency between known related points should be maintained both in the input audio space and corresponding latent deep space. We generate known related points through semantically meaningful transformations, both considering imperceptible and graver transformations. Then, we examine within- and between-space distance consistencies, both considering audio space and latent embedded space, the latter either being a result of a conventional feature extractor or a deep encoder. We illustrate how our method, as a complement to task-specific performance, provides interpretable insight into what a network may have captured from training data signals.},
  archivePrefix = {arXiv},
  eprint = {1904.07154},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/F4N7INYZ/Kim et al. - 2019 - Are Nearby Neighbors Relatives Diagnosing Deep M.pdf;/home/daniel/Zotero/storage/H4XSS6Y5/1904.html},
  journal = {arXiv:1904.07154 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{kimBandwidthExtension2019,
  title = {Bandwidth {{Extension}} on {{Raw Audio}} via {{Generative Adversarial Networks}}},
  author = {Kim, Sung and Sathe, Visvesh},
  year = {2019},
  month = mar,
  abstract = {Neural network-based methods have recently demonstrated state-of-the-art results on image synthesis and super-resolution tasks, in particular by using variants of generative adversarial networks (GANs) with supervised feature losses. Nevertheless, previous feature loss formulations rely on the availability of large auxiliary classifier networks, and labeled datasets that enable such classifiers to be trained. Furthermore, there has been comparatively little work to explore the applicability of GAN-based methods to domains other than images and video. In this work we explore a GAN-based method for audio processing, and develop a convolutional neural network architecture to perform audio super-resolution. In addition to several new architectural building blocks for audio processing, a key component of our approach is the use of an autoencoder-based loss that enables training in the GAN framework, with feature losses derived from unlabeled data. We explore the impact of our architectural choices, and demonstrate significant improvements over previous works in terms of both objective and perceptual quality.},
  archivePrefix = {arXiv},
  eprint = {1903.09027},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/42QWSDJ3/Kim und Sathe - 2019 - Bandwidth Extension on Raw Audio via Generative Ad.pdf;/home/daniel/Zotero/storage/WQHQ2PBQ/1903.html},
  journal = {arXiv:1903.09027 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{kimDeepDirected2016,
  title = {Deep Directed Generative Models with Energy-Based Probability Estimation},
  author = {Kim, Taesup and Bengio, Yoshua},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.03439}
}

@inproceedings{kimExcitationCodebook2001,
  title = {Excitation Codebook Design for Coding of the Singing Voice},
  booktitle = {Applications of {{Signal Processing}} to {{Audio}} and {{Acoustics}}, 2001 {{IEEE Workshop}} on The},
  author = {Kim, Youngmoo E},
  year = {2001},
  pages = {155--158},
  publisher = {{IEEE}}
}

@article{kimJointCTCAttention2016,
  title = {Joint {{CTC}}-{{Attention}} Based {{End}}-to-{{End Speech Recognition}} Using {{Multi}}-Task {{Learning}}},
  author = {Kim, Suyoun and Hori, Takaaki and Watanabe, Shinji},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.06773}
}

@inproceedings{kimLearningDiscover2017,
  title = {Learning to {{Discover Cross}}-{{Domain Relations}} with {{Generative Adversarial Networks}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1857--1865},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{kimOneDeep2019,
  ids = {kimOneDeep2018},
  title = {One Deep Music Representation to Rule Them All? {{A}} Comparative Analysis of Different Representation Learning Strategies},
  author = {Kim, Jaehun and Urbano, Juli{\'a}n and Liem, Cynthia CS and Hanjalic, Alan},
  year = {2019},
  pages = {1--27},
  publisher = {{Springer}},
  file = {/home/daniel/Zotero/storage/JZIBX5NP/Kim et al. - 2018 - One Deep Music Representation to Rule Them All  .pdf},
  journal = {Neural Computing and Applications}
}

@article{kingmaAutoEncodingVariational2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, D. P and Welling, M.},
  year = {2013},
  journal = {ArXiv e-prints},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@article{kingmaGlowGenerative2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  author = {Kingma, D. P. and Dhariwal, P.},
  year = {2018},
  journal = {ArXiv e-prints},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kingmaImprovedVariational2016,
  title = {Improved {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016}
}

@inproceedings{kingmaSemisupervisedLearning2014,
  title = {Semi-Supervised Learning with Deep Generative Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  year = {2014},
  pages = {3581--3589}
}

@article{kingmaVariationalDropout2015,
  title = {Variational Dropout and the Local Reparameterization Trick},
  author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
  year = {2015},
  journal = {arXiv preprint arXiv:1506.02557}
}

@inproceedings{kirosSkipthoughtVectors2015,
  title = {Skip-Thought Vectors},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year = {2015},
  pages = {3276--3284}
}

@article{klapuriAnalysisMeter2006,
  title = {Analysis of the Meter of Acoustic Musical Signals},
  author = {Klapuri, Anssi P and Eronen, Antti J and Astola, Jaakko T},
  year = {2006},
  volume = {14},
  pages = {342--355},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  number = {1}
}

@inproceedings{kneesTwoData2015,
  title = {Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections},
  booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference ({{ISMIR}}'15)},
  author = {Knees, Peter and Faraldo, {\'A}ngel and Herrera, Perfecto and Vogl, Richard and B{\"o}ck, Sebastian and H{\"o}rschl{\"a}ger, Florian and Le Goff, Mickael},
  year = {2015},
  month = oct,
  address = {{M{\'a}laga, Spain}},
  date-modified = {2015-09-21 11:18:53 +0000}
}

@inproceedings{knuthInformedSource2005,
  title = {Informed Source Separation: {{A Bayesian}} Tutorial},
  booktitle = {Signal {{Processing Conference}}, 2005 13th {{European}}},
  author = {Knuth, Kevin H},
  year = {2005},
  pages = {1--8},
  publisher = {{IEEE}}
}

@article{kobAnalysingUnderstanding2011,
  title = {Analysing and Understanding the Singing Voice: {{Recent}} Progress and Open Questions},
  author = {Kob, Malte and Henrich, Nathalie and Herzel, Hanspeter and Howard, David and Tokuda, Isao and Wolfe, Joe},
  year = {2011},
  volume = {6},
  pages = {362--374},
  journal = {Current Bioinformatics},
  number = {3}
}

@article{kongCrosstaskLearning2019,
  title = {Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization: {{DCASE}} 2019 Baseline Systems},
  shorttitle = {Cross-Task Learning for Audio Tagging, Sound Event Detection and Spatial Localization},
  author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Xu, Yong and Wang, Wenwu and Plumbley, Mark D.},
  year = {2019},
  month = apr,
  abstract = {The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge focuses on audio tagging, sound event detection and spatial localisation. DCASE 2019 consists of five tasks: 1) acoustic scene classification, 2) audio tagging with noisy labels and minimal supervision, 3) sound event localisation and detection, 4) sound event detection in domestic environments, and 5) urban sound tagging. In this paper, we propose generic cross-task baseline systems based on convolutional neural networks (CNNs). The motivation is to investigate the performance of a variety of models across several tasks without exploiting the specific characteristics of the tasks. We look at CNNs with 5, 9, and 13 layers, and find that the optimal architecture is task-dependent. For the systems we considered, we found that the 9-layer CNN with average pooling is a good model for a majority of the DCASE 2019 tasks.},
  archivePrefix = {arXiv},
  eprint = {1904.03476},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/864NWUWU/Kong et al. - 2019 - Cross-task learning for audio tagging, sound event.pdf;/home/daniel/Zotero/storage/8QMYTMZC/1904.html},
  journal = {arXiv:1904.03476 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{kongJointSeparationclassification2017,
  title = {A Joint Separation-Classification Model for Sound Event Detection of Weakly Labelled Data},
  author = {Kong, Qiuqiang and Xu, Yong and Wang, Wenwu and Plumbley, Mark D.},
  year = {2017},
  volume = {abs/1711.03037},
  file = {/home/daniel/Zotero/storage/UV5APIHN/Kong et al. - 2017 - A joint separation-classification model for sound .pdf},
  journal = {CoRR}
}

@article{kongPANNsLargeScale2020,
  title = {{{PANNs}}: {{Large}}-{{Scale Pretrained Audio Neural Networks}} for {{Audio Pattern Recognition}}},
  shorttitle = {{{PANNs}}},
  author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
  year = {2020},
  month = jan,
  abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification and sound event detection. Recently neural networks have been applied to solve audio pattern recognition problems. However, previous systems focus on small datasets, which limits the performance of audio pattern recognition systems. Recently in computer vision and natural language processing, systems pretrained on large datasets have generalized well to several tasks. However, there is limited research on pretraining neural networks on large datasets for audio pattern recognition. In this paper, we propose large-scale pretrained audio neural networks (PANNs) trained on AudioSet. We propose to use Wavegram, a feature learned from waveform, and the mel spectrogram as input. We investigate the performance and complexity of a variety of convolutional neural networks. Our proposed AudioSet tagging system achieves a state-of-the-art mean average precision (mAP) of 0.439, outperforming the best previous system of 0.392. We transferred a PANN to six audio pattern recognition tasks and achieve state-of-the-art performance in many tasks. Source code and pretrained models have been released.},
  archivePrefix = {arXiv},
  eprint = {1912.10211},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/W7N8R3SC/Kong et al. - 2020 - PANNs Large-Scale Pretrained Audio Neural Network.pdf;/home/daniel/Zotero/storage/E47VR86E/1912.html},
  journal = {arXiv:1912.10211 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{koutnikClockworkRNN2014,
  title = {A {{Clockwork RNN}}},
  author = {Koutn{\'i}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2014},
  month = feb,
  volume = {abs/1402.3511},
  abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
  archivePrefix = {arXiv},
  eprint = {1402.3511},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/JBBM2I68/Koutník et al. - 2014 - A Clockwork RNN.pdf;/home/daniel/Zotero/storage/C9R4DFF6/1402.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{kozinskiAdversarialRegularisation2017,
  title = {An {{Adversarial Regularisation}} for {{Semi}}-{{Supervised Training}} of {{Structured Output Neural Networks}}},
  author = {Kozi{\'n}ski, Mateusz and Simon, Lo{\"i}c and Jurie, Fr{\'e}d{\'e}ric},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.02382}
}

@inproceedings{krizhevskyImagenetClassification2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105}
}

@article{kruegerZoneoutRegularizing2016,
  title = {Zoneout: {{Regularizing RNNs}} by {{Randomly Preserving Hidden Activations}}},
  shorttitle = {Zoneout},
  author = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  year = {2016},
  month = jun,
  abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.},
  archivePrefix = {arXiv},
  eprint = {1606.01305},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/JZPUH63E/Krueger et al. - 2016 - Zoneout Regularizing RNNs by Randomly Preserving .pdf;/home/daniel/Zotero/storage/V5YKLHZ7/1606.html},
  journal = {arXiv:1606.01305 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@book{krumhanslCognitiveFoundations2001,
  title = {Cognitive Foundations of Musical Pitch},
  author = {Krumhansl, Carol L},
  year = {2001},
  publisher = {{Oxford University Press}}
}

@article{krumhanslEffectsMusical1995,
  title = {Effects of Musical Context on Similarity and Expectancy},
  author = {Krumhansl, Carol Lynn},
  year = {1995},
  volume = {3},
  pages = {211--250},
  journal = {Systematische musikwissenschaft},
  number = {2}
}

@article{krumhanslTracingDynamic1982,
  title = {Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys.},
  author = {Krumhansl, Carol L and Kessler, Edward J},
  year = {1982},
  volume = {89},
  pages = {334},
  journal = {Psychological review},
  number = {4}
}

@phdthesis{kruspeApplicationAutomatic2018,
  title = {Application of Automatic Speech Recognition Technologies to Singing},
  author = {Kruspe, Anna M},
  year = {2018},
  school = {Technische Universit{\"a}t Ilmenau},
  type = {{{PhD Thesis}}}
}

@inproceedings{kruspeBootstrappingSystem2016,
  title = {Bootstrapping a System for Phoneme Recognition and Keyword Spotting in Unaccompanied Singing},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Kruspe, Anna M},
  year = {2016},
  pages = {358--364}
}

@inproceedings{kruspeRetrievalTextual2016,
  title = {Retrieval of Textual Song Lyrics from Sung Inputs},
  booktitle = {Proceedings of {{INTERSPEECH}}},
  author = {Kruspe, Anna M},
  year = {2016},
  pages = {2140--2144}
}

@book{kuhnFormenlehreMusik1987,
  title = {Formenlehre Der {{Musik}}},
  author = {K{\"u}hn, Clemens},
  year = {1987},
  publisher = {{Deutscher Taschenbuch}},
  address = {{Kassel}}
}

@article{kumJointDetection2019,
  title = {Joint {{Detection}} and {{Classification}} of {{Singing Voice Melody Using Convolutional Recurrent Neural Networks}}},
  author = {Kum, Sangeun and Nam, Juhan},
  year = {2019},
  volume = {9},
  issn = {2076-3417},
  doi = {10.3390/app9071324},
  abstract = {Singing melody extraction essentially involves two tasks: one is detecting the activity of a singing voice in polyphonic music, and the other is estimating the pitch of a singing voice in the detected voiced segments. In this paper, we present a joint detection and classification (JDC) network that conducts the singing voice detection and the pitch estimation simultaneously. The JDC network is composed of the main network that predicts the pitch contours of the singing melody and an auxiliary network that facilitates the detection of the singing voice. The main network is built with a convolutional recurrent neural network with residual connections and predicts pitch labels that cover the vocal range with a high resolution, as well as non-voice status. The auxiliary network is trained to detect the singing voice using multi-level features shared from the main network. The two optimization processes are tied with a joint melody loss function. We evaluate the proposed model on multiple melody extraction and vocal detection datasets, including cross-dataset evaluation. The experiments demonstrate how the auxiliary network and the joint melody loss function improve the melody extraction performance. Furthermore, the results show that our method outperforms state-of-the-art algorithms on the datasets.},
  article-number = {1324},
  journal = {Applied Sciences},
  number = {7}
}

@article{kusnerGANSSequences2016,
  title = {{{GANS}} for {{Sequences}} of {{Discrete Elements}} with the {{Gumbel}}-Softmax {{Distribution}}},
  author = {Kusner, Matt J and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.04051}
}

@article{lakeHumanlevelConcept2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  year = {2015},
  volume = {350},
  pages = {1332--1338},
  journal = {Science},
  number = {6266}
}

@book{lamereInfiniteJukebox2012,
  title = {The {{Infinite Jukebox}}},
  author = {Lamere, Paul},
  year = {2012}
}

@article{larsenAutoencodingPixels2015,
  title = {Autoencoding beyond Pixels Using a Learned Similarity Metric},
  author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2015},
  journal = {arXiv preprint arXiv:1512.09300}
}

@inproceedings{lartillotEstimatingTempo2013,
  title = {Estimating Tempo and Metrical Features by Tracking the Whole Metrical Hierarchy},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Music}} \& {{Emotion}} ({{ICME3}}), {{Jyv{\"a}skyl{\"a}}}, {{Finland}}, 11th-15th {{June}} 2013. {{Geoff Luck}} \& {{Olivier Brabant}} ({{Eds}}.). {{ISBN}} 978-951-39-5250-1},
  author = {Lartillot, Olivier and Cereghetti, Donato and Eliard, Kim and Trost, Wiebke J and Rappaz, Marc-Andr{\'e} and Grandjean, Didier},
  year = {2013},
  publisher = {{University of Jyv{\"a}skyl{\"a}, Department of Music}}
}

@incollection{lartillotMatlabToolbox2008,
  title = {A {{Matlab Toolbox}} for {{Music Information Retrieval}}},
  booktitle = {Data {{Analysis}}, {{Machine Learning}} and {{Applications}}},
  author = {Lartillot, O. and Toiviainen, P. and Eerola, T.},
  editor = {Preisach, C. and Burkhardt, H. and {Schmidt-Thieme}, L. and Decker, Reinhold},
  year = {2008},
  pages = {261--268},
  publisher = {{Springer Berlin Heidelberg}},
  language = {English},
  series = {Studies in {{Classification}}, {{Data Analysis}}, and {{Knowledge Organization}}}
}

@inproceedings{lartillotSimpleHighyield2013,
  title = {A Simple, High-Yield Method for Assessing Structural Novelity},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Music}} \& {{Emotion}} ({{ICME3}}), {{Jyv{\"a}skyl{\"a}}}, {{Finland}}, 11th-15th {{June}} 2013. {{Geoff Luck}} \& {{Olivier Brabant}} ({{Eds}}.). {{ISBN}} 978-951-39-5250-1},
  author = {Lartillot, O. and Cereghetti, D. and Eliard, K. and Grandjean, D.},
  year = {2013},
  publisher = {{University of Jyv{\"a}skyl{\"a}, Department of Music}}
}

@article{leech-wilkinsonCortotBerceuse2015,
  title = {Cortot's {{Berceuse}}},
  author = {{Leech-Wilkinson}, Daniel},
  year = {2015},
  volume = {34},
  pages = {335--363},
  journal = {Music Analysis},
  number = {3}
}

@inproceedings{leeRevisitingSinging2018,
  title = {Revisiting {{Singing Voice Detection}}: {{A}} Quantitative Review and the Future Outlook},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Lee, Kyungyun and Choi, Keunwoo and Nam, Juhan},
  year = {2018},
  pages = {506--513},
  abstract = {Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there still is a room to improve to build a more robust singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the current datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector.}
}

@article{leeSamplelevelDeep2017,
  title = {Sample-Level {{Deep Convolutional Neural Networks}} for {{Music Auto}}-Tagging {{Using Raw Waveforms}}},
  author = {Lee, Jongpil and Park, Jiyoung and Kim, Keunhyoung Luke and Nam, Juhan},
  year = {2017},
  month = may,
  volume = {abs/1703.01789},
  abstract = {Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. Our experiments show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million Song Dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency along layer, such as mel-frequency spectrogram that is widely used in music classification systems.},
  archivePrefix = {arXiv},
  eprint = {1703.01789},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/UQTQTLF5/Lee et al. - 2017 - Sample-level Deep Convolutional Neural Networks fo.pdf;/home/daniel/Zotero/storage/E3M9EMWN/1703.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound}
}

@inproceedings{leeSegmentationBasedLyricsAudio2008,
  title = {Segmentation-{{Based Lyrics}}-{{Audio Alignment}} Using {{Dynamic Programming}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Lee, Kyogu and Cremer, Markus},
  year = {2008},
  pages = {395--400}
}

@inproceedings{leeWordLevel2017,
  title = {Word Level Lyrics-Audio Synchronization Using Separated Vocals},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lee, Sang Won and Scott, Jeffrey},
  year = {2017},
  pages = {646--650}
}

@inproceedings{lehnerLightWeightRealTimeCapable2013,
  title = {Towards {{Light}}-{{Weight}}, {{Real}}-{{Time}}-{{Capable Singing Voice Detection}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Lehner, Bernhard and Sonnleitner, Reinhard and Widmer, Gerhard},
  year = {2013},
  pages = {53--58}
}

@inproceedings{lehnerLowlatencyRealtimecapable2015,
  title = {A Low-Latency, Real-Time-Capable Singing Voice Detection Method with {{LSTM}} Recurrent Neural Networks},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), 2015 23rd {{European}}},
  author = {Lehner, Bernhard and Widmer, Gerhard and Bock, Sebastian},
  year = {2015},
  pages = {21--25},
  publisher = {{IEEE}},
  file = {/home/daniel/Zotero/storage/UGW6SFNP/zotero-better-bibtex-5.1.60.xpi}
}

@article{leInferenceCompilation2016,
  title = {Inference {{Compilation}} and {{Universal Probabilistic Programming}}},
  author = {Le, Tuan Anh and Baydin, Atilim Gunes and Wood, Frank},
  year = {2016},
  journal = {arXiv preprint arXiv:1610.09900}
}

@inproceedings{lekeMissingData2016,
  title = {Missing {{Data Estimation}} in {{High}}-{{Dimensional Datasets}}: {{A Swarm Intelligence}}-{{Deep Neural Network Approach}}},
  booktitle = {International {{Conference}} in {{Swarm Intelligence}}},
  author = {Leke, Collins and Marwala, Tshilidzi},
  year = {2016},
  pages = {259--270},
  publisher = {{Springer}}
}

@article{lerdahlGenerativeTheory1987,
  title = {A Generative Theory of Tonal Music},
  author = {Lerdahl, Fred and Jackendoff, Ray},
  year = {1987}
}

@article{lerdahlTonalPitch1988,
  title = {Tonal Pitch Space},
  author = {Lerdahl, Fred},
  year = {1988},
  volume = {5},
  pages = {315--349},
  journal = {Music Perception: An Interdisciplinary Journal},
  number = {3}
}

@inproceedings{lerouxExplicitConsistency2008,
  title = {Explicit Consistency Constraints for {{STFT}} Spectrograms and Their Application to Phase Reconstruction},
  booktitle = {{{SAPA}}@ {{INTERSPEECH}}},
  author = {Le Roux, Jonathan and Ono, Nobutaka and Sagayama, Shigeki},
  year = {2008},
  pages = {23--28},
  file = {/home/daniel/Zotero/storage/LBA294LN/Le Roux, Ono, Sagayama - Explicit consistency constraints for STFT spectrograms and their application to phase reconstruction.pdf}
}

@inproceedings{liGraphicalGenerative2018,
  title = {Graphical {{Generative Adversarial Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} 2018, {{NeurIPS}} 2018, 3-8 {{December}} 2018, {{Montr{\'e}al}}, {{Canada}}.},
  author = {Li, Chongxuan and Welling, Max and Zhu, Jun and Zhang, Bo},
  year = {2018},
  pages = {6072--6083}
}

@article{lingDeepLearning2015,
  title = {Deep Learning for Acoustic Modeling in Parametric Speech Generation: {{A}} Systematic Review of Existing Techniques and Future Trends},
  author = {Ling, Zhen-Hua and Kang, Shi-Yin and Zen, Heiga and Senior, Andrew and Schuster, Mike and Qian, Xiao-Jun and Meng, Helen M and Deng, Li},
  year = {2015},
  volume = {32},
  pages = {35--52},
  publisher = {{IEEE}},
  journal = {IEEE Signal Processing Magazine},
  number = {3}
}

@article{linnainmaaTaylorExpansion1976,
  title = {Taylor Expansion of the Accumulated Rounding Error},
  author = {Linnainmaa, Seppo},
  year = {1976},
  month = jun,
  volume = {16},
  pages = {146--160},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01931367},
  journal = {BIT},
  language = {en},
  number = {2}
}

@article{linNetworkNetwork2013,
  title = {Network in Network},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.4400}
}

@article{liTFAttentionNetEnd2019,
  title = {{{TF}}-{{Attention}}-{{Net}}: {{An End To End Neural Network For Singing Voice Separation}}},
  shorttitle = {{{TF}}-{{Attention}}-{{Net}}},
  author = {Li, Tingle and Chen, Jiawei and Hou, Haowen and Li, Ming},
  year = {2019},
  month = sep,
  abstract = {In terms of source separation task, most of deep neural networks have two main types: one is modeling in the spectrogram, and the other is in the waveform. Most of them use CNNs, LSTMs, but due to the high sampling rate of audio, whether it is LSTMs with a long-distance dependent or CNNs with sliding windows is difficult to extract long-term input context. In this case, we propose an end-to-end network: Time Frequency Attention Net(TF-Attention-Net), to study the ability of the attention mechanism in the source separation task. Later, we will introduce the Slice Attention, which can extract the acoustic features of time and frequency scales under different channels while the time complexity of which is less than Multi-head Attention. Also, attention mechanism can be efficiently parallelized while the LSTMs can not because of their time dependence. Meanwhile, the receptive field of attention mechanism is larger than the CNNs, which means we can use shallower layers to extract deeper features. Experiments for singing voice separation indicate that our model yields a better performance compared with the SotA model: spectrogram-based U-Net and waveform-based Wave-U-Net, given the same data.},
  archivePrefix = {arXiv},
  eprint = {1909.05746},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/X5PZ3KQ2/Li et al. - 2019 - TF-Attention-Net An End To End Neural Network For.pdf;/home/daniel/Zotero/storage/DVXSN2WW/1909.html},
  journal = {arXiv:1909.05746 [cs, eess]},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{liuSteinVariational2016,
  title = {Stein {{Variational Gradient Descent}}: {{A General Purpose Bayesian Inference Algorithm}}},
  author = {Liu, Qiang and Wang, Dilin},
  year = {2016},
  journal = {arXiv preprint arXiv:1608.04471}
}

@inproceedings{liutkus2016Signal2017,
  title = {The 2016 Signal Separation Evaluation Campaign},
  booktitle = {Proceedings of the {{International Conference}} on {{Latent Variable Analysis}} and {{Signal Separation}} ({{LVA}}/{{ICA}})},
  author = {Liutkus, Antoine and St{\"o}ter, Fabian-Robert and Rafii, Zafar and Kitamura, Daichi and Rivet, Bertrand and Ito, Nobutaka and Ono, Nobutaka and Fontecave, Julie},
  year = {2017},
  pages = {323--332}
}

@inproceedings{liutkusScalableAudio2015,
  title = {Scalable Audio Separation with Light Kernel Additive Modelling},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liutkus, Antoine and Fitzgerald, Derry and Rafii, Zafar},
  year = {2015},
  pages = {76--80},
  publisher = {{IEEE}}
}

@article{longuet-higginsRhythmicInterpretation1984,
  title = {The Rhythmic Interpretation of Monophonic Music},
  author = {{Longuet-Higgins}, H Christopher and Lee, Christopher S},
  year = {1984},
  volume = {1},
  pages = {424--441},
  journal = {Music Perception: An Interdisciplinary Journal},
  number = {4}
}

@article{lucicHighFidelityImage2019,
  title = {High-{{Fidelity Image Generation With Fewer Labels}}},
  author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
  year = {2019},
  month = mar,
  abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform state-of-the-art (SOTA) on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the art conditional model BigGAN on ImageNet using only 10\% of the labels and outperform it using 20\% of the labels.},
  archivePrefix = {arXiv},
  eprint = {1903.02271},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/J22HVEXQ/Lucic et al. - 2019 - High-Fidelity Image Generation With Fewer Labels.pdf;/home/daniel/Zotero/storage/I9N23B8A/1903.html},
  journal = {arXiv:1903.02271 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{luFundamentalLimitations2009,
  title = {Fundamental Limitations of Semi-Supervised Learning},
  author = {Lu, Tyler Tian},
  year = {2009}
}

@inproceedings{luggerRelevanceVoice2007,
  title = {The {{Relevance}} of {{Voice Quality Features}} in {{Speaker Independent Emotion Recognition}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lugger, Marko and Yang, Bin},
  year = {2007},
  month = apr,
  volume = {4},
  pages = {IV-17-IV-20},
  doi = {10.1109/ICASSP.2007.367152},
  abstract = {This paper investigates the classification of different emotional states using presodic and voice quality information. We want to exploit the usage of different phonation types within the production of emotions. Therefore, as features we use prosodic features, voice quality parameters, and different combinations of both types. We study how prosodic and voice quality features overlap or complement each other in the application of emotion recognition. The classification is speaker independent and uses a reduced subset of 8 features and a Bayesian classifier.},
  keywords = {Bayes methods,Bayesian classifier,Bayesian methods,emotion recognition,Emotion recognition,Feature extraction,Mel frequency cepstral coefficient,Pattern classification,phonation types,Production,prosodic features,Psychology,Signal processing,Spatial databases,speaker independent,speaker independent emotion recognition,speaker recognition,Speech analysis,speech processing,voice quality features}
}

@inproceedings{luoDeepClustering2017,
  title = {Deep Clustering and Conventional Networks for Music Separation: {{Stronger}} Together},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Luo, Y. and Chen, Z. and Hershey, J. R. and Roux, J. Le and Mesgarani, N.},
  year = {2017},
  pages = {61--65},
  doi = {10.1109/ICASSP.2017.7952118},
  file = {/home/daniel/Zotero/storage/T8PANM7E/Luo et al. - 2017 - Deep clustering and conventional networks for musi.pdf},
  keywords = {approximation theory,audio separation,audio signal processing,conventional networks,deep clustering,Deep clustering,Deep learning,estimation theory,Instruments,Linear programming,music,music separation,Music separation,neural nets,pattern clustering,signal approximation,Singing voice separation,source separation,Source separation,source signal estimation,speaker independent speech separation,speaker recognition,Spectrogram,Speech,Time-frequency analysis,Training}
}

@article{luoDualpathRNN2019,
  title = {Dual-Path {{RNN}}: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation},
  shorttitle = {Dual-Path {{RNN}}},
  author = {Luo, Yi and Chen, Zhuo and Yoshioka, Takuya},
  year = {2019},
  month = oct,
  abstract = {Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra- and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.},
  archivePrefix = {arXiv},
  eprint = {1910.06379},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/WCDXP538/Luo et al. - 2019 - Dual-path RNN efficient long sequence modeling fo.pdf;/home/daniel/Zotero/storage/7LNDL72Z/1910.html},
  journal = {arXiv:1910.06379 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{luoTasNetTimedomain2017,
  title = {{{TasNet}}: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation},
  author = {Luo, Yi and Mesgarani, Nima},
  year = {2017},
  volume = {abs/1711.00541},
  file = {/home/daniel/Zotero/storage/DCVW4W8H/Luo und Mesgarani - 2017 - TasNet time-domain audio separation network for r.pdf},
  journal = {CoRR}
}

@article{maaloeAuxiliaryDeep2016,
  title = {Auxiliary {{Deep Generative Models}}},
  author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.05473}
}

@article{maatenVisualizingData2008,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  volume = {9},
  pages = {2579--2605},
  journal = {Journal of Machine Learning Research},
  number = {Nov}
}

@article{mackayBayesianNeural1995,
  title = {Bayesian Neural Networks and Density Networks},
  author = {MacKay, David JC},
  year = {1995},
  volume = {354},
  pages = {73--80},
  publisher = {{Elsevier}},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  number = {1}
}

@inproceedings{maclaurinGradientbasedHyperparameter2015,
  title = {Gradient-Based Hyperparameter Optimization through Reversible Learning},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
  year = {2015}
}

@article{maFPUTSFully2018,
  title = {{{FPUTS}}: {{Fully Parallel UFANS}}-Based {{End}}-to-{{End Text}}-to-{{Speech System}}},
  shorttitle = {{{FPUTS}}},
  author = {Ma, Dabiao and Su, Zhiba and Wang, Wenxuan and Lu, Yuhao},
  year = {2018},
  month = dec,
  abstract = {A Text-to-speech (TTS) system that can generate high quality audios with small time latency and fewer errors is required for industrial applications and services. In this paper, we propose a new non-autoregressive, fully parallel end-to-end TTS system. It utilizes the new attention structure and the recently proposed convolutional structure, UFANS. Different to RNN, UFANS can capture long term information in a fully parallel manner. Compared with the most popular end-to-end text-to-speech systems, our system can generate equal or better quality audios with fewer errors and reach at least 10 times speed up of inference.},
  archivePrefix = {arXiv},
  eprint = {1812.05710},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/4ZSGJET4/Ma et al. - 2018 - FPUTS Fully Parallel UFANS-based End-to-End Text-.pdf;/home/daniel/Zotero/storage/23HRA6NZ/1812.html},
  journal = {arXiv:1812.05710 [cs, eess, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{makhzaniAdversarialAutoencoders2015,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.05644}
}

@article{makhzaniPixelGANAutoencoders2017,
  title = {{{PixelGAN Autoencoders}}},
  author = {Makhzani, Alireza and Frey, Brendan},
  year = {2017},
  journal = {arXiv preprint arXiv:1706.00531}
}

@article{manilowCuttingMusic2019,
  title = {Cutting {{Music Source Separation Some Slakh}}: {{A Dataset}} to {{Study}} the {{Impact}} of {{Training Data Quality}} and {{Quantity}}},
  shorttitle = {Cutting {{Music Source Separation Some Slakh}}},
  author = {Manilow, Ethan and Wichern, Gordon and Seetharaman, Prem and Roux, Jonathan Le},
  year = {2019},
  month = sep,
  abstract = {Music source separation performance has greatly improved in recent years with the advent of approaches based on deep learning. Such methods typically require large amounts of labelled training data, which in the case of music consist of mixtures and corresponding instrument stems. However, stems are unavailable for most commercial music, and only limited datasets have so far been released to the public. It can thus be difficult to draw conclusions when comparing various source separation methods, as the difference in performance may stem as much from better data augmentation techniques or training tricks to alleviate the limited availability of training data, as from intrinsically better model architectures and objective functions. In this paper, we present the synthesized Lakh dataset (Slakh) as a new tool for music source separation research. Slakh consists of high-quality renderings of instrumental mixtures and corresponding stems generated from the Lakh MIDI dataset (LMD) using professional-grade sample-based virtual instruments. A first version, Slakh2100, focuses on 2100 songs, resulting in 145 hours of mixtures. While not fully comparable because it is purely instrumental, this dataset contains an order of magnitude more data than MUSDB18, the \{\textbackslash{}it de facto\} standard dataset in the field. We show that Slakh can be used to effectively augment existing datasets for musical instrument separation, while opening the door to a wide array of data-intensive music signal analysis tasks.},
  archivePrefix = {arXiv},
  eprint = {1909.08494},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/IGBXFNUC/Manilow et al. - 2019 - Cutting Music Source Separation Some Slakh A Data.pdf;/home/daniel/Zotero/storage/UCB5DTRT/1909.html},
  journal = {arXiv:1909.08494 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{manilowSimultaneousSeparation2019,
  title = {Simultaneous {{Separation}} and {{Transcription}} of {{Mixtures}} with {{Multiple Polyphonic}} and {{Percussive Instruments}}},
  author = {Manilow, Ethan and Seetharaman, Prem and Pardo, Bryan},
  year = {2019},
  month = oct,
  abstract = {We present a single deep learning architecture that can both separate an audio recording of a musical mixture into constituent single-instrument recordings and transcribe these instruments into a human-readable format at the same time, learning a shared musical representation for both tasks. This novel architecture, which we call Cerberus, builds on the Chimera network for source separation by adding a third "head" for transcription. By training each head with different losses, we are able to jointly learn how to separate and transcribe up to 5 instruments in our experiments with a single network. We show that the two tasks are highly complementary with one another and when learned jointly, lead to Cerberus networks that are better at both separation and transcription and generalize better to unseen mixtures.},
  archivePrefix = {arXiv},
  eprint = {1910.12621},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/8ZUADPZT/Manilow et al. - 2019 - Simultaneous Separation and Transcription of Mixtu.pdf;/home/daniel/Zotero/storage/NEW2HKG7/1910.html},
  journal = {arXiv:1910.12621 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{maoLeastSquares2017,
  title = {Least {{Squares Generative Adversarial Networks}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y.K. and Wang, Zhen and Paul Smolley, Stephen},
  year = {2017},
  month = oct
}

@article{marblestoneIntegrationDeep2016,
  title = {Towards an Integration of Deep Learning and Neuroscience},
  author = {Marblestone, Adam and Wayne, Greg and Kording, Konrad},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.03813}
}

@inproceedings{marchandGTZANRhythmExtending2015,
  title = {{{GTZAN}}-{{Rhythm}}: Extending the {{GTZAN}} Test-Set with Beat, Downbeat and Swing Annotations},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Marchand, Ugo and Fresnel, Quentin and Peeters, Geoffroy},
  year = {2015}
}

@article{marcusBuildingLarge1993,
  title = {Building a {{Large Annotated Corpus}} of {{English}}: {{The Penn Treebank}}},
  author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  year = {1993},
  month = jun,
  volume = {19},
  pages = {313--330},
  publisher = {{MIT Press}},
  location = {Cambridge, MA, USA},
  issn = {0891-2017},
  acmid = {972475},
  issue_date = {June 1993},
  journal = {Comput. Linguist.},
  number = {2},
  numpages = {18}
}

@article{marozeauEffectFundamental2007,
  title = {The Effect of Fundamental Frequency on the Brightness Dimension of Timbre},
  author = {Marozeau, Jeremy and {de Cheveign{\'e}}, Alain},
  year = {2007},
  volume = {121},
  pages = {383--387},
  journal = {The Journal of the Acoustical Society of America},
  number = {1}
}

@book{mathworksCrossCorrelationFunction,
  title = {Cross-{{Correlation}} Function "Xcorr" in {{MATLAB}}},
  author = {{Mathworks}}
}

@book{mathworksMATLABParallel,
  title = {{{MATLAB Parallel Computing Toolbox}}},
  author = {{Mathworks}}
}

@book{mathworksMATLABProgramming,
  title = {{{MATLAB}} Programming Language},
  author = {{Mathworks}}
}

@book{mathworksMATLABSignal,
  title = {{{MATLAB Signal Processing Toolbox}}},
  author = {{Mathworks}}
}

@book{matthiash.GraphDemo,
  title = {Graph {{Demo}} - a {{Matlab GUI}} to Explore Similarity Graphs and Their Use in Machine Learning},
  author = {Matthias H., von Luxburg, U.}
}

@inproceedings{mauchLyricstoaudioAlignment2010,
  title = {Lyrics-to-Audio Alignment and Phrase-Level Segmentation Using Incomplete Internet-Style Chord Annotations},
  booktitle = {Proceedings of the {{Sound Music Computing Conference}} ({{SMC}})},
  author = {Mauch, Matthias and Fujihara, Hiromasa and Goto, Masataka},
  year = {2010},
  pages = {9--16}
}

@inproceedings{mauchPYINFundamental2014,
  title = {{{pYIN}}: {{A}} Fundamental Frequency Estimator Using Probabilistic Threshold Distributions},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Mauch, Matthias and Dixon, Simon},
  year = {2014},
  pages = {659--663},
  publisher = {{IEEE}}
}

@inproceedings{mauchTimbreMelody2011,
  title = {Timbre and {{Melody Features}} for the {{Recognition}} of {{Vocal Activity}} and {{Instrumental Solos}} in {{Polyphonic Music}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Mauch, Matthias and Fujihara, Hiromasa and Yoshii, Kazuyoshi and Goto, Masataka},
  year = {2011},
  pages = {233--238}
}

@inproceedings{mayorPerformanceAnalysis2009,
  title = {Performance Analysis and Scoring of the Singing Voice},
  booktitle = {Proc. 35th {{AES Intl}}. {{Conf}}., {{London}}, {{UK}}},
  author = {Mayor, Oscar and Bonada, Jordi and Loscos, Alex},
  year = {2009},
  pages = {1--7}
}

@inproceedings{mayorSingingTutor2006,
  title = {The Singing Tutor: {{Expression}} Categorization and Segmentation of the Singing Voice},
  booktitle = {Proceedings of the {{AES}} 121st {{Convention}}},
  author = {Mayor, Oscar and Bonada, Jordi and Loscos, Alex},
  year = {2006}
}

@article{mcadamsPerceptualScaling1995,
  title = {Perceptual Scaling of Synthesized Musical Timbres: {{Common}} Dimensions, Specificities, and Latent Subject Classes},
  author = {McAdams, Stephen and Winsberg, Suzanne and Donnadieu, Sophie and De Soete, Geert and Krimphoff, Jochen},
  year = {1995},
  volume = {58},
  pages = {177--192},
  journal = {Psychological research},
  number = {3}
}

@inproceedings{mccallumUnsupervisedLearning2019,
  title = {Unsupervised {{Learning}} of {{Deep Features}} for {{Music Segmentation}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {McCallum, Matthew C.},
  year = {2019},
  month = may,
  pages = {346--350},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683407},
  isbn = {978-1-4799-8131-1}
}

@inproceedings{mcvicarLearningSeparate2016,
  title = {Learning to Separate Vocals from Polyphonic Mixtures via Ensemble Methods and Structured Output Prediction},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {McVicar, M and {Santos-Rodr}, R and De Bie, T and others},
  year = {2016},
  pages = {450--454},
  publisher = {{IEEE}}
}

@inproceedings{mcvicarLeveragingRepetition2014,
  title = {Leveraging Repetition for Improved Automatic Lyric Transcription in Popular Music},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {McVicar, Matt and Ellis, Daniel PW and Goto, Masataka},
  year = {2014},
  pages = {3117--3121}
}

@article{mehriSampleRNNUnconditional2016,
  title = {{{SampleRNN}}: {{An Unconditional End}}-to-{{End Neural Audio Generation Model}}},
  author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  volume = {abs/1612.07837},
  journal = {CoRR}
}

@inproceedings{mesarosAutomaticAlignment2008,
  title = {Automatic Alignment of Music Audio and Lyrics},
  booktitle = {Proceedings of the {{International Conference}} on {{Digital Audio Effects}} ({{DAFx}})},
  author = {Mesaros, Annamaria and Virtanen, Tuomas},
  year = {2008}
}

@article{mesarosAutomaticRecognition2010,
  title = {Automatic Recognition of Lyrics in Singing},
  author = {Mesaros, Annamaria and Virtanen, Tuomas},
  year = {2010},
  volume = {2010},
  pages = {1},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  number = {1}
}

@inproceedings{mesarosSingingVoice2013,
  title = {Singing Voice Identification and Lyrics Transcription for Music Information Retrieval Invited Paper},
  booktitle = {Speech {{Technology}} and {{Human}}-{{Computer Dialogue}} ({{SpeD}}), 2013 7th {{Conference}} On},
  author = {Mesaros, Annamaria},
  year = {2013},
  pages = {1--10},
  publisher = {{IEEE}}
}

@inproceedings{mesarosTUTDatabase2016,
  title = {{{TUT}} Database for Acoustic Scene Classification and Sound Event Detection},
  booktitle = {2016 24th European Signal Processing Conference ({{EUSIPCO}})},
  author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  year = {2016},
  pages = {1128--1132},
  organization = {{IEEE}}
}

@article{meschederAdversarialVariational2017,
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.04722}
}

@inproceedings{meschederWhichTraining2018,
  title = {Which Training Methods for {{GANs}} Do Actually Converge?},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {3481--3490},
  publisher = {{PMLR}},
  address = {{Stockholmsm{\"a}ssan, Stockholm Sweden}},
  abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
  pdf = {http://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{meseguer-brocalConditionedUNetIntroducing2019,
  title = {Conditioned-{{U}}-{{Net}}: {{Introducing}} a {{Control Mechanism}} in the {{U}}-{{Net}} for {{Multiple Source Separations}}},
  shorttitle = {Conditioned-{{U}}-{{Net}}},
  author = {{Meseguer-Brocal}, Gabriel and Peeters, Geoffroy},
  year = {2019},
  month = jul,
  abstract = {Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost.},
  archivePrefix = {arXiv},
  eprint = {1907.01277},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/9TBA5MYQ/Meseguer-Brocal und Peeters - 2019 - Conditioned-U-Net Introducing a Control Mechanism.pdf;/home/daniel/Zotero/storage/64N6XKXQ/1907.html},
  journal = {arXiv:1907.01277 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{metzLearningUnsupervised2019,
  title = {Learning Unsupervised Learning Rules},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and {Sohl-Dickstein}, Jascha},
  year = {2019}
}

@article{miconiNeuralNetworks2016,
  title = {Neural Networks with Differentiable Structure},
  author = {Miconi, Thomas},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.06216}
}

@article{mikolovSubwordLanguage2012,
  title = {Subword Language Modeling with Neural Networks},
  author = {Mikolov, Tom{\'a}{\v s} and Sutskever, Ilya and Deoras, Anoop and Le, Hai-Son and Kombrink, Stefan and Cernocky, J},
  year = {2012},
  journal = {preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf)}
}

@article{millgaardFlowGlottogram2015,
  title = {Flow {{Glottogram Characteristics}} and {{Perceived Degree}} of {{Phonatory Pressedness}}},
  author = {{Millg\textbackslash{}a ard}, Moa and Fors, Tobias and Sundberg, Johan},
  year = {2015},
  journal = {Journal of Voice. Article in press. DOI: http://dx.doi.org/10.1016/j.jvoice.2015.03.014}
}

@inproceedings{mimilakisExaminingPerceptual2018,
  title = {Examining the {{Perceptual Effect}} of {{Alternative Objective Functions}} for {{Deep Learning Based Music Source Separation}}},
  booktitle = {2018 52nd {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  author = {Mimilakis, S. I. and Cano, E. and FitzGerald, D. and Drossos, K. and Schuller, G.},
  year = {2018},
  month = oct,
  pages = {679--683},
  doi = {10.1109/ACSSC.2018.8645257},
  keywords = {Computer architecture,deep learning,Deep learning,Linear programming,Measurement,music source separation,Neural networks,noise-to-mask ratio,perceptual evaluation,Source separation,Task analysis}
}

@inproceedings{mironGeneratingData2017,
  title = {Generating Data to Train Convolutional Neural Networks for Classical Music Source Separation},
  booktitle = {Proceedings of the 14th {{Sound}} and {{Music Computing Conference}}},
  author = {Miron, Marius and Janer Mestres, Jordi and G{\'o}mez Guti{\'e}rrez, Emilia},
  year = {2017},
  publisher = {{Aalto University}}
}

@inproceedings{mishraGANbasedGeneration2019,
  title = {{{GAN}}-Based Generation and Automatic Selection of Explanations for Neural Networks},
  booktitle = {Safe {{Machine Learning Workshop}} at the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Mishra, Saumitra and Stoller, Daniel and Benetos, Emmanouil and Sturm, Bob L. and Dixon, Simon},
  year = {2019}
}

@article{miyatoDistributionalSmoothing2015,
  title = {Distributional Smoothing with Virtual Adversarial Training},
  author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Nakae, Ken and Ishii, Shin},
  year = {2015},
  volume = {1050},
  pages = {25},
  journal = {stat}
}

@article{miyatoSpectralNormalization2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  year = {2018},
  month = feb,
  volume = {abs/1802.05957},
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
  archivePrefix = {arXiv},
  eprint = {1802.05957},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/HS6KS6LR/Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf;/home/daniel/Zotero/storage/FM3AMLM9/1802.html},
  journal = {CoRR},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{mnihVariationalInference2016,
  title = {Variational Inference for {{Monte Carlo}} Objectives},
  author = {Mnih, Andriy and Rezende, Danilo J},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.06725}
}

@inproceedings{mogrenCRNNGANContinuous2016,
  title = {C-{{RNN}}-{{GAN}}: {{A}} Continuous Recurrent Neural Network with Adversarial Training},
  booktitle = {Constructive {{Machine Learning Workshop}} ({{CML}}) at {{NIPS}}},
  author = {Mogren, Olof},
  year = {2016}
}

@inproceedings{mohamedLearningImplicit2017,
  title = {Learning in {{Implicit Generative Models}}},
  booktitle = {Proc. of {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2017}
}

@inproceedings{mohamedNeuralNetwork2005,
  title = {Neural Network Based Techniques for Estimating Missing Data in Databases},
  booktitle = {16th {{Annual Symposium}} of the {{Patten Recognition Association}} of {{South Africa}}, {{Langebaan}}},
  author = {Mohamed, Shakir and Marwala, Tshilidzi},
  year = {2005},
  pages = {27--32}
}

@article{mohamedVariationalInference,
  title = {Variational {{Inference}} for {{Machine Learning}}},
  author = {Mohamed, Shakir}
}

@article{mooreModelPrediction1997,
  title = {A Model for the Prediction of Thresholds, Loudness, and Partial Loudness},
  author = {Moore, B. C. J. and Glasberg, B. R. and Baer, T.},
  year = {1997},
  volume = {45},
  pages = {224--240},
  journal = {Journal of the Audio Engineering Society},
  number = {4}
}

@phdthesis{morfiAutomaticDetection2019,
  title = {Automatic Detection and Classification of Bird Sounds in Low-Resource Wildlife Audio Datasets},
  author = {Morfi, Gnostothea-Veroniki},
  year = {2019},
  school = {Quen Mary University of London}
}

@article{morUniversalMusic2018,
  title = {A {{Universal Music Translation Network}}},
  author = {Mor, Noam and Wolf, Lior and Polyak, Adam and Taigman, Yaniv},
  year = {2018},
  month = may,
  abstract = {We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.},
  archivePrefix = {arXiv},
  eprint = {1805.07848},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/6IQR67JN/Mor et al. - 2018 - A Universal Music Translation Network.pdf;/home/daniel/Zotero/storage/ZIIIXDGI/1805.html},
  journal = {arXiv:1805.07848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{muellerSiameseRecurrent2016,
  title = {Siamese {{Recurrent Architectures}} for {{Learning Sentence Similarity}}},
  booktitle = {Thirtieth {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Mueller, Jonas and Thyagarajan, Aditya},
  year = {2016}
}

@article{mullensiefenFantasticFeature2009,
  title = {Fantastic: {{Feature ANalysis Technology Accessing Statistics}} ({{In}} a {{Corpus}}): {{Technical}} Report V1},
  author = {M{\"u}llensiefen, Daniel},
  year = {2009}
}

@article{murrayMultipleImputation2018,
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  shorttitle = {Multiple {{Imputation}}},
  author = {Murray, Jared S.},
  year = {2018},
  month = jan,
  volume = {abs/1801.04058},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  archivePrefix = {arXiv},
  eprint = {1801.04058},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/TT4662ID/Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf;/home/daniel/Zotero/storage/TI8M52NB/1801.html},
  journal = {CoRR},
  keywords = {Statistics - Methodology}
}

@article{nachmaniUnsupervisedSinging2019,
  title = {Unsupervised {{Singing Voice Conversion}}},
  author = {Nachmani, Eliya and Wolf, Lior},
  year = {2019},
  month = apr,
  abstract = {We present a deep learning method for singing voice conversion. The proposed network is not conditioned on the text or on the notes, and it directly converts the audio of one singer to the voice of another. Training is performed without any form of supervision: no lyrics or any kind of phonetic features, no notes, and no matching samples between singers. The proposed network employs a single CNN encoder for all singers, a single WaveNet decoder, and a classifier that enforces the latent representation to be singer-agnostic. Each singer is represented by one embedding vector, which the decoder is conditioned on. In order to deal with relatively small datasets, we propose a new data augmentation scheme, as well as new training losses and protocols that are based on backtranslation. Our evaluation presents evidence that the conversion produces natural signing voices that are highly recognizable as the target singer.},
  archivePrefix = {arXiv},
  eprint = {1904.06590},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/W7DXBJK9/Nachmani und Wolf - 2019 - Unsupervised Singing Voice Conversion.pdf;/home/daniel/Zotero/storage/LRHNP7HC/1904.html},
  journal = {arXiv:1904.06590 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{nairRectifiedLinear2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Nair, V. and Hinton, G. E.},
  year = {2010},
  pages = {807--814}
}

@article{nakamuraTimeDomainAudio2020,
  title = {Time-{{Domain Audio Source Separation Based}} on {{Wave}}-{{U}}-{{Net Combined}} with {{Discrete Wavelet Transform}}},
  author = {Nakamura, Tomohiko and Saruwatari, Hiroshi},
  year = {2020},
  month = jan,
  abstract = {We propose a time-domain audio source separation method using down-sampling (DS) and up-sampling (US) layers based on a discrete wavelet transform (DWT). The proposed method is based on one of the state-of-the-art deep neural networks, Wave-U-Net, which successively down-samples and up-samples feature maps. We find that this architecture resembles that of multiresolution analysis, and reveal that the DS layers of Wave-U-Net cause aliasing and may discard information useful for the separation. Although the effects of these problems may be reduced by training, to achieve a more reliable source separation method, we should design DS layers capable of overcoming the problems. With this belief, focusing on the fact that the DWT has an anti-aliasing filter and the perfect reconstruction property, we design the proposed layers. Experiments on music source separation show the efficacy of the proposed method and the importance of simultaneously considering the anti-aliasing filters and the perfect reconstruction property.},
  archivePrefix = {arXiv},
  eprint = {2001.10190},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/9NUK8QNR/Nakamura und Saruwatari - 2020 - Time-Domain Audio Source Separation Based on Wave-.pdf;/home/daniel/Zotero/storage/TTXFZ4WE/2001.html},
  journal = {arXiv:2001.10190 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{nakanoAutomaticSinging2006,
  title = {An Automatic Singing Skill Evaluation Method for Unknown Melodies Using Pitch Interval Accuracy and Vibrato Features},
  author = {Nakano, Tomoyasu and Goto, Masataka and Hiraga, Yuzuru},
  year = {2006},
  volume = {12},
  pages = {1},
  journal = {Rn}
}

@article{nakanoSubjectiveEvaluation2006,
  title = {Subjective Evaluation of Common Singing Skills Using the Rank Ordering Method},
  author = {Nakano, Tomoyasu and Goto, Masataka and Hiraga, Yuzuru},
  year = {2006},
  pages = {1507--1512},
  journal = {Proc. of ICMPC}
}

@inproceedings{nakanoVocaListener2Singing2011,
  title = {{{VocaListener2}}: {{A}} Singing Synthesis System Able to Mimic a User's Singing in Terms of Voice Timbre Changes as Well as Pitch and Dynamics},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Nakano, Tomoyasu and Goto, Masataka},
  year = {2011},
  pages = {453--456},
  publisher = {{IEEE}}
}

@inproceedings{namLearningSparse2012,
  title = {Learning {{Sparse Feature Representations}} for {{Music Annotation}} and {{Retrieval}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Nam, Juhan and Herrera, Jorge and Slaney, Malcolm and Smith, Julius O},
  year = {2012},
  pages = {565--570}
}

@article{narayanaswamyAudioSource2019,
  title = {Audio {{Source Separation}} via {{Multi}}-{{Scale Learning}} with {{Dilated Dense U}}-{{Nets}}},
  author = {Narayanaswamy, Vivek Sivaraman and Katoch, Sameeksha and Thiagarajan, Jayaraman J. and Song, Huan and Spanias, Andreas},
  year = {2019},
  month = apr,
  abstract = {Modern audio source separation techniques rely on optimizing sequence model architectures such as, 1D-CNNs, on mixture recordings to generalize well to unseen mixtures. Specifically, recent focus is on time-domain based architectures such as Wave-U-Net which exploit temporal context by extracting multi-scale features. However, the optimality of the feature extraction process in these architectures has not been well investigated. In this paper, we examine and recommend critical architectural changes that forge an optimal multi-scale feature extraction process. To this end, we replace regular \$1-\$D convolutions with adaptive dilated convolutions that have innate capability of capturing increased context by using large temporal receptive fields. We also investigate the impact of dense connections on the extraction process that encourage feature reuse and better gradient flow. The dense connections between the downsampling and upsampling paths of a U-Net architecture capture multi-resolution information leading to improved temporal modelling. We evaluate the proposed approaches on the MUSDB test dataset. In addition to providing an improved performance over the state-of-the-art, we also provide insights on the impact of different architectural choices on complex data-driven solutions for source separation.},
  archivePrefix = {arXiv},
  eprint = {1904.04161},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/JI7W7HKT/Narayanaswamy et al. - 2019 - Audio Source Separation via Multi-Scale Learning w.pdf;/home/daniel/Zotero/storage/2KC9R3CU/1904.html},
  journal = {arXiv:1904.04161 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{neilPhasedLSTM2016,
  title = {Phased {{LSTM}}: {{Accelerating Recurrent Network Training}} for {{Long}} or {{Event}}-Based {{Sequences}}},
  shorttitle = {Phased {{LSTM}}},
  author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  year = {2016},
  month = oct,
  volume = {abs/1610.09513},
  abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.},
  archivePrefix = {arXiv},
  eprint = {1610.09513},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/66SC5GT7/Neil et al. - 2016 - Phased LSTM Accelerating Recurrent Network Traini.pdf;/home/daniel/Zotero/storage/ECBK7AS7/1610.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{nesterovMethodSolving1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}} (1/K2)},
  booktitle = {Soviet {{Mathematics Doklady}}},
  author = {Nesterov, Y.},
  year = {1983},
  volume = {27},
  pages = {372--376}
}

@incollection{nguyenDualDiscriminator2017,
  title = {Dual {{Discriminator Generative Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Nguyen, Tu and Le, Trung and Vu, Hung and Phung, Dinh},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {2670--2680},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{nguyenImprovingLearning1990,
  title = {Improving the Learning Speed of 2-Layer Neural Networks by Choosing Initial Values of the Adaptive Weights},
  booktitle = {{{IJCNN International Joint Conference}} on {{Neural Networks}}},
  author = {Nguyen, Derrick and Widrow, Bernard},
  year = {1990},
  month = jun,
  pages = {21-26 vol.3},
  doi = {10.1109/IJCNN.1990.137819},
  abstract = {The authors describe how a two-layer neural network can approximate any nonlinear function by forming a union of piecewise linear segments. A method is given for picking initial weights for the network to decrease training time. The authors have used the method to initialize adaptive weights over a large number of different training problems and have achieved major improvements in learning speed in every case. The improvement is best when a large number of hidden units is used with a complicated desired response. The authors have used the method to train the truck-backer-upper and were able to decrease the training time from about two days to four hours},
  keywords = {2-layer neural networks,adaptive systems,adaptive weights,complicated desired response,hidden units,initial weights,learning speed,learning systems,neural nets,nonlinear function,piecewise linear segments,training problems,training time,truck-backer-upper,two-layer neural network}
}

@article{nicholFirstOrderMetaLearning2018,
  title = {On {{First}}-{{Order Meta}}-{{Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  year = {2018},
  month = oct,
  volume = {abs/1803.02999},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archivePrefix = {arXiv},
  eprint = {1803.02999},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/8QDDUSG6/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf;/home/daniel/Zotero/storage/GA36EPXS/1803.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{nolascoAudiobasedIdentification2019,
  title = {Audio-Based Identification of Beehive States},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Nolasco, I. and Terenzi, A. and Cecchi, S. and Orcioni, S. and Bear, H. L. and Benetos, E.},
  year = {2019},
  pages = {8256--8260}
}

@article{nowozinFGANTraining2016,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.00709}
}

@phdthesis{nugrahaMultichannelAudio2015,
  title = {Multichannel Audio Source Separation with Deep Neural Networks},
  author = {Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel},
  year = {2015},
  school = {Inria},
  type = {{{PhD Thesis}}}
}

@article{odenaDeconvolutionCheckerboard2016,
  title = {Deconvolution and {{Checkerboard Artifacts}}},
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  year = {2016},
  doi = {10.23915/distill.00003},
  file = {/home/daniel/Zotero/storage/7YGJ7LHR/Odena et al. - 2016 - Deconvolution and Checkerboard Artifacts.pdf},
  journal = {Distill}
}

@article{odenaSemiSupervisedLearning2016,
  title = {Semi-{{Supervised Learning}} with {{Generative Adversarial Networks}}},
  author = {Odena, Augustus},
  year = {2016},
  month = jun,
  abstract = {We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.},
  archivePrefix = {arXiv},
  eprint = {1606.01583},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/TZGUM5NG/Odena - 2016 - Semi-Supervised Learning with Generative Adversari.pdf;/home/daniel/Zotero/storage/M78M3DXW/1606.html},
  journal = {arXiv:1606.01583 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{oldenIlluminatingBlack2002,
  title = {Illuminating the ``black Box: A Randomization Approach for Understanding Variable Contributions in Artificial Neural Networks},
  author = {Olden, Julian D and Jackson, Donald A},
  year = {2002},
  volume = {154},
  pages = {135--150},
  journal = {Ecological modelling},
  number = {1}
}

@article{olsonMeasurementLoudness1972,
  title = {The Measurement of Loudness},
  author = {Olson, H. F.},
  year = {1972},
  volume = {56 No. 2},
  pages = {18--22},
  journal = {Audio Magazine}
}

@inproceedings{ono2015Signal2015,
  title = {The 2015 Signal Separation Evaluation Campaign},
  booktitle = {International {{Conference}} on {{Latent Variable Analysis}} and {{Signal Separation}}},
  author = {Ono, Nobutaka and Rafii, Zafar and Kitamura, Daichi and Ito, Nobutaka and Liutkus, Antoine},
  year = {2015},
  pages = {387--395},
  publisher = {{Springer}}
}

@article{oordParallelWaveNet2017,
  title = {Parallel {{WaveNet}}: {{Fast High}}-{{Fidelity Speech Synthesis}}},
  shorttitle = {Parallel {{WaveNet}}},
  author = {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  year = {2017},
  month = nov,
  volume = {abs/1711.10433},
  abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
  archivePrefix = {arXiv},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/SVD9WALX/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf;/home/daniel/Zotero/storage/IG5F9CE2/1711.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning}
}

@article{oordRepresentationLearning2019,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  volume = {abs/1807.03748},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/P39ZFGE4/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf;/home/daniel/Zotero/storage/XQECERNC/1807.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ozerovAdaptationBayesian2007,
  title = {Adaptation of {{Bayesian}} Models for Single-Channel Source Separation and Its Application to Voice/Music Separation in Popular Songs},
  author = {Ozerov, Alexey and Philippe, Pierrick and Bimbot, Frdric and Gribonval, Rmi},
  year = {2007},
  volume = {15},
  pages = {1564--1578},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {5}
}

@article{ozerovGeneralFlexible2012,
  title = {A General Flexible Framework for the Handling of Prior Information in Audio Source Separation},
  author = {Ozerov, Alexey and Vincent, Emmanuel and Bimbot, Fr{\'e}d{\'e}ric},
  year = {2012},
  volume = {20},
  pages = {1118--1133},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {4}
}

@article{paigeInferenceNetworks2016,
  title = {Inference {{Networks}} for {{Sequential Monte Carlo}} in {{Graphical Models}}},
  author = {Paige, Brooks and Wood, Frank},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.06701}
}

@article{paineAnalysisUnsupervised2014,
  title = {An {{Analysis}} of {{Unsupervised Pre}}-Training in {{Light}} of {{Recent Advances}}},
  author = {Paine, Tom Le and Khorrami, Pooya and Han, Wei and Huang, Thomas S.},
  year = {2014},
  volume = {abs/1412.6597},
  file = {/home/daniel/Zotero/storage/5GI8UKG9/Paine et al. - 2014 - An Analysis of Unsupervised Pre-training in Light .pdf},
  journal = {CoRR}
}

@article{paliwalAssessingContribution2011,
  title = {Assessing the Contribution of Variables in Feed Forward Neural Network},
  author = {Paliwal, Mukta and Kumar, Usha A},
  year = {2011},
  volume = {11},
  pages = {3690--3696},
  journal = {Applied Soft Computing},
  number = {4}
}

@article{panSurveyTransfer2009,
  title = {A Survey on Transfer Learning},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2009},
  volume = {22},
  pages = {1345--1359},
  publisher = {{IEEE}},
  journal = {IEEE Transactions on knowledge and data engineering},
  number = {10}
}

@book{papadimitriouCombinatorialOptimization1998,
  title = {Combinatorial Optimization: Algorithms and Complexity},
  author = {Papadimitriou, C. H. and Steiglitz, K.},
  year = {1998},
  publisher = {{Courier Corporation}}
}

@article{papernotDistillationDefense2015,
  title = {Distillation as a {{Defense}} to {{Adversarial Perturbations}} against {{Deep Neural Networks}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.04508}
}

@article{papernotLimitationsDeep2015,
  title = {The {{Limitations}} of {{Deep Learning}} in {{Adversarial Settings}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.07528}
}

@inproceedings{parkerCreatingAudio2004,
  title = {Creating Audio Textures by Example: Tiling and Stitching},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Parker, J. R. and Behm, B.},
  year = {2004},
  pages = {317--320},
  publisher = {{IEEE}}
}

@article{parkMusicSource2018,
  title = {Music {{Source Separation Using Stacked Hourglass Networks}}},
  author = {Park, Sungheon and Kim, Taehoon and Lee, Kyogu and Kwak, Nojun},
  year = {2018},
  month = jun,
  volume = {abs/1805.08559},
  abstract = {In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.},
  archivePrefix = {arXiv},
  eprint = {1805.08559},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/D8CMU698/Park et al. - 2018 - Music Source Separation Using Stacked Hourglass Ne.pdf;/home/daniel/Zotero/storage/5BZ7HL9H/1805.html},
  journal = {CoRR},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{pascualLearningProblemagnostic2019,
  title = {Learning {{Problem}}-Agnostic {{Speech Representations}} from {{Multiple Self}}-Supervised {{Tasks}}},
  author = {Pascual, Santiago and Ravanelli, Mirco and Serr{\`a}, Joan and Bonafonte, Antonio and Bengio, Yoshua},
  year = {2019},
  month = apr,
  abstract = {Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.},
  archivePrefix = {arXiv},
  eprint = {1904.03416},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/BJYP36XS/Pascual et al. - 2019 - Learning Problem-agnostic Speech Representations f.pdf;/home/daniel/Zotero/storage/XVNXXBAP/1904.html},
  journal = {arXiv:1904.03416 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{pascualSEGANSpeech2017,
  title = {{{SEGAN}}: {{Speech}} Enhancement Generative Adversarial Network},
  author = {Pascual, Santiago and Bonafonte, Antonio and Serra, Joan},
  year = {2017},
  journal = {arXiv preprint arXiv:1703.09452}
}

@inproceedings{pathakContextEncoders2016,
  title = {Context Encoders: {{Feature}} Learning by Inpainting},
  booktitle = {Proc. of the {{IEEE}} Conference on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  year = {2016},
  pages = {2536--2544}
}

@inproceedings{pattersonEfficientAuditory1987,
  title = {An Efficient Auditory Filterbank Based on the Gammatone Function},
  booktitle = {A Meeting of the {{IOC Speech Group}} on {{Auditory Modelling}} at {{RSRE}}},
  author = {Patterson, RD and {Nimmo-Smith}, Ian and Holdsworth, John and Rice, Peter},
  year = {1987},
  volume = {2}
}

@inproceedings{paulusMusicStructure2006,
  title = {Music Structure Analysis by Finding Repeated Parts},
  booktitle = {Proceedings of the 1st {{ACM}} Workshop on {{Audio}} and Music Computing Multimedia},
  author = {Paulus, Jouni and Klapuri, Anssi},
  year = {2006},
  pages = {59--68},
  publisher = {{ACM}}
}

@article{pengFeatureSelection2005,
  title = {Feature Selection Based on Mutual Information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy},
  author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
  year = {2005},
  volume = {27},
  pages = {1226--1238},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}

@article{perez-lapilloImprovingSinging2019,
  title = {Improving Singing Voice Separation with the {{Wave}}-{{U}}-{{Net}} Using {{Minimum Hyperspherical Energy}}},
  author = {{Perez-Lapillo}, Joaquin and Galkin, Oleksandr and Weyde, Tillman},
  year = {2019},
  month = oct,
  abstract = {In recent years, deep learning has surpassed traditional approaches to the problem of singing voice separation. The Wave-U-Net is a recent deep network architecture that operates directly on the time domain. The standard Wave-U-Net is trained with data augmentation and early stopping to prevent overfitting. Minimum hyperspherical energy (MHE) regularization has recently proven to increase generalization in image classification problems by encouraging a diversified filter configuration. In this work, we apply MHE regularization to the 1D filters of the Wave-U-Net. We evaluated this approach for separating the vocal part from mixed music audio recordings on the MUSDB18 dataset. We found that adding MHE regularization to the loss function consistently improves singing voice separation, as measured in the Signal to Distortion Ratio on test recordings, leading to the current best time-domain system for singing voice extraction.},
  archivePrefix = {arXiv},
  eprint = {1910.10071},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/KTJHUS23/Perez-Lapillo et al. - 2019 - Improving singing voice separation with the Wave-U.pdf;/home/daniel/Zotero/storage/DCEXRHSF/1910.html},
  journal = {arXiv:1910.10071 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{petersDeepContextualized2018,
  title = {Deep Contextualized Word Representations},
  booktitle = {Proc. of the {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{NAACL}})},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = jun,
  volume = {1},
  pages = {2227--2237},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1202},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}

@article{pezeshkiDeconstructingLadder2015,
  title = {Deconstructing the Ladder Network Architecture},
  author = {Pezeshki, Mohammad and Fan, Linxi and Brakel, Philemon and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06430}
}

@article{pingClariNetParallel2018,
  title = {{{ClariNet}}: {{Parallel Wave Generation}} in {{End}}-to-{{End Text}}-to-{{Speech}}},
  shorttitle = {{{ClariNet}}},
  author = {Ping, Wei and Peng, Kainan and Chen, Jitong},
  year = {2018},
  month = jul,
  abstract = {In this work, we propose a new solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (van den Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we introduce the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model.},
  archivePrefix = {arXiv},
  eprint = {1807.07281},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/472YAVIR/Ping et al. - 2018 - ClariNet Parallel Wave Generation in End-to-End T.pdf;/home/daniel/Zotero/storage/G237JZ96/1807.html},
  journal = {arXiv:1807.07281 [cs, eess]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{ponsTimbreAnalysis2017,
  title = {Timbre {{Analysis}} of {{Music Audio Signals}} with {{Convolutional Neural Networks}}},
  author = {Pons, Jordi and Slizovskaia, Olga and Gong, Rong and G{\'o}mez, Emilia and Serra, Xavier},
  year = {2017},
  journal = {arXiv preprint arXiv:1703.06697}
}

@article{povelPerceptionTemporal1985,
  title = {Perception of Temporal Patterns},
  author = {Povel, Dirk-Jan and Essens, Peter},
  year = {1985},
  volume = {2},
  pages = {411--440},
  journal = {Music Perception: An Interdisciplinary Journal},
  number = {4}
}

@inproceedings{pretetSingingVoice2019,
  title = {Singing {{Voice Separation}}: {{A Study}} on {{Training Data}}},
  shorttitle = {Singing {{Voice Separation}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Pretet, Laure and Hennequin, Romain and {Royo-Letelier}, Jimena and Vaglio, Andrea},
  year = {2019},
  month = may,
  pages = {506--510},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683555},
  isbn = {978-1-4799-8131-1}
}

@inproceedings{proutskovaBreathyResonant2012,
  title = {Breathy or {{Resonant}} - {{A Controlled}} and {{Curated Dataset}} for {{Phonation Mode Detection}} in {{Singing}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Proutskova, Polina and Rhodes, Christophe and Wiggins, Geraint A. and Crawford, Tim},
  year = {2012},
  pages = {589--594},
  file = {/home/daniel/Zotero/storage/4VU2S2C9/Proutskova et al. - 2012 - Breathy or Resonant - A Controlled and Curated Dat.pdf}
}

@article{proutskovaBreathyResonant2013,
  title = {Breathy, {{Resonant}}, {{Pressed}} \textendash{} {{Automatic Detection}} of {{Phonation Mode}} from {{Audio Recordings}} of {{Singing}}},
  author = {Proutskova, Polina and Rhodes, Christophe and Crawford, Tim and {Geraint Wiggins}},
  year = {2013},
  volume = {42},
  pages = {171--186},
  doi = {10.1080/09298215.2013.821496},
  abstract = {Abstract In this paper we present an experiment on automatic detection of phonation modes from recordings of sustained sung vowels. We created an open dataset specifically for this experiment, containing recordings of nine vowels from multiple languages, sung by a female singer on all pitches in her vocal range in phonation modes breathy, neutral, flow (resonant) and pressed. The dataset is available under a Creative Commons license at http://www.proutskova.de/phonation-modes. First, glottal flow waveform is estimated via inverse filtering (IAIF) from audio recordings. Then six parameters of the glottal flow waveform are calculated. A 4-class Support Vector Machine classifier is constructed to separate these features into phonation mode classes. We automated the IAIF approach by computing the values of the input arguments \textendash{} lip radiation and formant count \textendash{} leading to the best-performing SVM classifiers (average classification accuracy over 60\%), yielding a physical model for the articulation of the vowels. We examine the steps needed to generalize and extend the experimental work presented in this paper in order to apply this method in ethnomusicological investigations.},
  file = {/home/daniel/Zotero/storage/8TVNDAZX/Proutskova et al. - 2013 - Breathy, Resonant, Pressed – Automatic Detection o.pdf},
  journal = {Journal of New Music Research},
  number = {2}
}

@inproceedings{puJointGANMultiDomain2018,
  title = {{{JointGAN}}: {{Multi}}-{{Domain Joint Distribution Learning}} with {{Generative Adversarial Nets}}},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Pu, Yunchen and Dai, Shuyang and Gan, Zhe and Wang, Weiyao and Wang, Guoyin and Zhang, Yizhe and Henao, Ricardo and Duke, Lawrence Carin},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {4151--4160},
  publisher = {{PMLR}},
  address = {{Stockholmsm{\"a}ssan, Stockholm Sweden}},
  abstract = {A new generative adversarial network is developed for joint distribution matching.Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain.The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning.From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{qiLosssensitiveGenerative2017,
  title = {Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities},
  author = {Qi, Guo-Jun},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.06264}
}

@article{rabinerTutorialHidden1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, Lawrence R},
  year = {1989},
  volume = {77},
  pages = {257--286},
  journal = {Proceedings of the IEEE},
  number = {2}
}

@article{radfordUnsupervisedRepresentation2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2015},
  volume = {abs/1511.06434},
  file = {/home/daniel/Zotero/storage/XACHYA72/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf},
  journal = {CoRR}
}

@inproceedings{raffelMirEval2014,
  title = {Mir\_eval: {{A}} Transparent Implementation of Common {{MIR}} Metrics},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Raffel, Colin and Mcfee, Brian and Humphrey, Eric and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel},
  year = {2014},
  month = oct
}

@inproceedings{rafiiCombiningModeling2013,
  title = {Combining {{Modeling Of Singing Voice And Background Music For Automatic Separation Of Musical Mixtures}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Rafii, Zafar and Germain, Fran{\c c}ois and Sun, Dennis L and Mysore, Gautham J},
  year = {2013},
  volume = {10},
  pages = {645--680}
}

@misc{rafiiMusdb18Corpus2017,
  title = {The {{Musdb18 Corpus For Music Separation}}},
  author = {Rafii, Zafar and Liutkus, Antoine and St{\"o}ter, Fabian-Robert and Mimilakis, Stylianos Ioannis and Bittner, Rachel},
  year = {2017},
  month = dec,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.1117372},
  abstract = {The sigsep musdb18 data set consists of a total of 150 full-track songs of different styles and includes both the stereo mixtures and the original sources, divided between a training subset and a test subset.

Its purpose is to serve as a reference database for the design and the evaluation of source separation algorithms. The objective of such signal processing methods is to estimate one or more sources from a set of mixtures, e.g. for karaoke applications. It has been used as the official dataset in the professionally-produced music recordings task for SiSEC 2018, which is the international campaign for the evaluation of source separation algorithms.

{$<$}em{$>$}musdb18{$<$}/em{$>$} contains two folders, a folder with a training set: ``train'', composed of 100 songs, and a folder with a test set: ``test'', composed of 50 songs. Supervised approaches should be trained on the training set and tested on both sets.

All files from the {$<$}em{$>$}musdb18{$<$}/em{$>$} dataset are encoded in the Native Instruments stems format (.mp4). It is a multitrack format composed of 5 stereo streams, each one encoded in AAC @256kbps. These signals correspond to:

	0 - The mixture,
	1 - The drums,
	2 - The bass,
	3 - The rest of the accompaniment,
	4 - The vocals.

For each file, the mixture correspond to the sum of all the signals. All signals are stereophonic and encoded at 44.1kHz.

As the {$<$}em{$>$}MUSDB18{$<$}/em{$>$} is encoded as STEMS, it relies on ffmpeg to read the multi-stream files. We provide a python wrapper called stempeg that allows to easily parse the dataset and decode the stem tracks on-the-fly.}
}

@article{rafiiRepeatingPattern2013,
  title = {Repeating Pattern Extraction Technique ({{REPET}}): {{A}} Simple Method for Music/Voice Separation},
  author = {Rafii, Zafar and Pardo, Bryan},
  year = {2013},
  volume = {21},
  pages = {73--84},
  journal = {IEEE transactions on audio, speech, and language processing},
  number = {1}
}

@inproceedings{raikoIterativeNeural2014,
  title = {Iterative Neural Autoregressive Distribution Estimator Nade-k},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Raiko, Tapani and Li, Yao and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  pages = {325--333}
}

@inproceedings{ramonaVocalDetection2008,
  title = {Vocal Detection in Music with {{Support Vector Machines}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ramona, Mathieu and Richard, Ga{\"e}l and David, Bertrand},
  year = {2008},
  pages = {1885--1888}
}

@incollection{raoContextawareFeatures2011,
  title = {Context-Aware Features for Singing Voice Detection in Polyphonic Music},
  booktitle = {Adaptive {{Multimedia Retrieval}}. {{Large}}-{{Scale Multimedia Retrieval}} and {{Evaluation}}},
  author = {Rao, Vishweshwara and Gupta, Chitralekha and Rao, Preeti},
  year = {2011},
  pages = {43--57},
  publisher = {{Springer}}
}

@inproceedings{rasmusSemisupervisedLearning2015,
  title = {Semi-Supervised Learning with Ladder Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  year = {2015},
  pages = {3546--3554}
}

@book{rcoreteamLanguageEnvironment2017,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2017},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}}
}

@inproceedings{regnierSingingVoice2009,
  title = {Singing Voice Detection in Music Tracks Using Direct Voice Vibrato Detection},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Regnier, Lise and Peeters, Geoffroy},
  year = {2009},
  pages = {1685--1688},
  publisher = {{IEEE}}
}

@article{rethageWavenetSpeech2017,
  title = {A {{Wavenet}} for {{Speech Denoising}}},
  author = {Rethage, Dario and Pons, Jordi and Serra, Xavier},
  year = {2017},
  volume = {abs/1706.07162},
  file = {/home/daniel/Zotero/storage/B4QWXG6U/Rethage et al. - 2017 - A Wavenet for Speech Denoising.pdf},
  journal = {CoRR}
}

@article{rezendeStochasticBackpropagation2014,
  title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  journal = {arXiv preprint arXiv:1401.4082}
}

@article{ribeiroModelAgnosticInterpretability2016,
  title = {Model-{{Agnostic Interpretability}} of {{Machine Learning}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.05386}
}

@article{richmondUseArticulatory2015,
  title = {The Use of Articulatory Movement Data in Speech Synthesis Applications: {{An}} Overview\textemdash{{Application}} of Articulatory Movements Using Machine Learning Algorithms\textemdash{}},
  author = {Richmond, Korin and Ling, Zhenhua and Yamagishi, Junichi},
  year = {2015},
  volume = {36},
  pages = {467--477},
  journal = {Acoustical Science and Technology},
  number = {6}
}

@inproceedings{rifaiContractiveAutoencoders2011,
  title = {Contractive Auto-Encoders: {{Explicit}} Invariance during Feature Extraction},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  year = {2011},
  pages = {833--840}
}

@book{robinsonReplayGain2001,
  title = {Replay {{Gain}} - {{A}} Proposed Standard},
  author = {Robinson, David},
  year = {2001}
}

@book{roliltd.JUCEFramework2004,
  title = {{{JUCE Framework}}},
  author = {{ROLI Ltd.}},
  year = {2004}
}

@inproceedings{ronnebergerUnetConvolutional2015,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  booktitle = {Proc. of the {{International Conference}} on {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  author = {Ronneberger, O. and Fischer, P. and Brox, T.},
  year = {2015},
  pages = {234--241},
  publisher = {{Springer}}
}

@inproceedings{rouasAutomaticClassification2016,
  title = {Automatic Classification of Phonation Modes in Singing Voice: Towards Singing Style Characterisation and Application to Ethnomusicological Recordings},
  booktitle = {Interspeech},
  author = {Rouas, Jean-Luc and Ioannidis, Leonidas},
  year = {2016},
  volume = {2016},
  pages = {150--154}
}

@article{rousseeuwSilhouettesGraphical1987,
  title = {Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  author = {Rousseeuw, P. J.},
  year = {1987},
  volume = {20},
  pages = {53--65},
  journal = {Journal of computational and applied mathematics}
}

@book{rowlandDRowAudioJUCE2010,
  title = {{{dRowAudio}} - {{A JUCE}} Module for High Level Audio Application Development},
  author = {Rowland, D.},
  year = {2010}
}

@inproceedings{rubinsteinComparativeStudy2010,
  title = {A Comparative Study of Image Retargeting},
  booktitle = {{{ACM}} Transactions on Graphics ({{TOG}})},
  author = {Rubinstein, Michael and Gutierrez, Diego and Sorkine, Olga and Shamir, Ariel},
  year = {2010},
  volume = {29},
  pages = {160},
  publisher = {{ACM}}
}

@article{saatchiBayesianGAN2017,
  title = {Bayesian {{GAN}}},
  author = {Saatchi, Yunus and Wilson, Andrew Gordon},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.09558}
}

@article{sahaiSpectrogramFeature2019,
  title = {Spectrogram {{Feature Losses}} for {{Music Source Separation}}},
  author = {Sahai, Abhimanyu and Weber, Romann and McWilliams, Brian},
  year = {2019},
  month = jan,
  abstract = {In this paper we study deep learning-based music source separation, and explore using an alternative loss to the standard spectrogram pixel-level L2 loss for model training. Our main contribution is in demonstrating that adding a high-level feature loss term, extracted from the spectrograms using a VGG net, can improve separation quality vis-a-vis a pure pixel-level loss. We show this improvement in the context of the MMDenseNet, a State-of-the-Art deep learning model for this task, for the extraction of drums and vocal sounds from songs in the musdb18 database, covering a broad range of western music genres. We believe that this finding can be generalized and applied to broader machine learning-based systems in the audio domain.},
  archivePrefix = {arXiv},
  eprint = {1901.05061},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/GBK62U8Y/Sahai et al. - 2019 - Spectrogram Feature Losses for Music Source Separa.pdf;/home/daniel/Zotero/storage/LIVD2QXM/1901.html},
  journal = {arXiv:1901.05061 [cs, eess, stat]},
  keywords = {62; 68,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,H.5.5,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{sakLearningAcoustic2015,
  title = {Learning Acoustic Frame Labeling for Speech Recognition with Recurrent Neural Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sak, Ha{\c s}im and Senior, Andrew and Rao, Kanishka and Irsoy, Ozan and Graves, Alex and Beaufays, Fran{\c c}oise and Schalkwyk, Johan},
  year = {2015},
  pages = {4280--4284},
  file = {/home/daniel/Zotero/storage/ZY6ZGR5L/Sak, Senior, Rao, Irsoy, Graves, Beaufays, Schalkwyk - Learning acoustic frame labeling for speech recognition with recurrent neural networks.pdf}
}

@article{salimansImprovedTechniques2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year = {2016},
  volume = {abs/1606.03498},
  journal = {CoRR}
}

@inproceedings{salimansMarkovChain2015,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: {{Bridging}} the Gap},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Salimans, Tim and Kingma, Diederik P and Welling, Max and others},
  year = {2015},
  pages = {1218--1226}
}

@article{salimansPixelCNNImproving2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with Discretized Logistic Mixture Likelihood and Other Modifications},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.05517}
}

@article{santoroOneshotLearning2016,
  title = {One-Shot {{Learning}} with {{Memory}}-{{Augmented Neural Networks}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.06065}
}

@inproceedings{satoSoundtrackGeneration2016,
  title = {A Soundtrack Generation System to Synchronize the Climax of a Video Clip with Music},
  booktitle = {{{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Sato, Haruki and Hirai, Tatsunori and Nakano, Tomoyasu and Goto, Masataka and Morishima, Shigeo},
  year = {2016},
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/home/daniel/Zotero/storage/YLTWRUAH/Sato, Hirai, Nakano, Goto, Morishima - A soundtrack generation system to synchronise the climax of a video clip with music.pdf}
}

@article{scellierBiologicallyPlausible2016,
  title = {Towards a Biologically Plausible Backprop},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.05179}
}

@inproceedings{schluterExploringData2015,
  title = {Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Schl{\"u}ter, J. and Grill, T.},
  year = {2015}
}

@inproceedings{schluterImprovedMusical2014,
  title = {Improved Musical Onset Detection with {{Convolutional Neural Networks}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Schl{\"u}ter, J. and B{\"o}ck, S.},
  year = {2014},
  month = may,
  pages = {6979--6983},
  doi = {10.1109/ICASSP.2014.6854953},
  keywords = {audio signal processing,CNN,Computer architecture,computer vision problem,Convolution,convolutional neural networks,Detectors,hand-designed methods,harmonic onsets,improved musical onset detection,information retrieval,knowledge engineering,learning (artificial intelligence),machine learning,Multi-layer neural network,music,music analysis,Music information retrieval,neural nets,Neural networks,percussive onsets,polyphonic music signals,signal processing tasks,Spectrogram,spectrograms,Training}
}

@inproceedings{schluterLearningPinpoint2016,
  title = {Learning to Pinpoint Singing Voice from Weakly Labeled Examples},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Schl{\"u}ter, J.},
  year = {2016},
  pages = {44--50}
}

@book{schluterSaliencyMaps2015,
  title = {Saliency {{Maps}} and {{Guided Backpropagation}}},
  author = {Schl{\"u}ter, J.},
  year = {2015}
}

@article{schmidhuberDeepLearning2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  volume = {61},
  pages = {85--117},
  journal = {Neural Networks}
}

@article{schmidhuberDiscoveringNeural1997,
  title = {Discovering Neural Nets with Low {{Kolmogorov}} Complexity and High Generalization Capability},
  author = {Schmidhuber, J{\"u}rgen},
  year = {1997},
  volume = {10},
  pages = {857--873},
  journal = {Neural Networks},
  number = {5}
}

@article{schmidhuberSelfdelimitingNeural2012,
  title = {Self-Delimiting Neural Networks},
  author = {Schmidhuber, Juergen},
  year = {2012},
  journal = {arXiv preprint arXiv:1210.0118}
}

@article{schneiderWav2vecUnsupervised2019,
  title = {Wav2vec: {{Unsupervised Pre}}-Training for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  year = {2019},
  month = apr,
  volume = {CoRR},
  abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36\% when only a few hours of transcribed data is available. Our approach achieves 2.43\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using three orders of magnitude less labeled training data.},
  archivePrefix = {arXiv},
  eprint = {1904.05862},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/MG5PAWMN/Schneider et al. - 2019 - wav2vec Unsupervised Pre-training for Speech Reco.pdf;/home/daniel/Zotero/storage/6BX4TFM8/1904.html},
  journal = {abs/1904.05862},
  keywords = {Computer Science - Computation and Language}
}

@article{schoefflerNextGeneration2015,
  title = {Towards the {{Next Generation}} of {{Web}}-Based {{Experiments}}: {{A Case Study Assessing Basic Audio Quality Following}} the {{ITU}}-{{R Recommendation BS}}.1534 ({{MUSHRA}})},
  author = {Schoeffler, Michael and St{\"o}ter, Fabian-robert and Edler, Bernd and Herre, J{\"u}rgen},
  year = {2015},
  journal = {1st Web Audio Conference}
}

@inproceedings{schorkhuberMatlabToolbox2014,
  title = {A {{Matlab}} Toolbox for Efficient Perfect Reconstruction Time-Frequency Transforms with Log-Frequency Resolution},
  booktitle = {Audio {{Engineering Society Conference}}: 53rd {{International Conference}}: {{Semantic Audio}}},
  author = {Sch{\"o}rkhuber, Christian and Klapuri, Anssi and Holighaus, Nicki and D{\"o}rfler, Monika},
  year = {2014},
  publisher = {{Audio Engineering Society}}
}

@inproceedings{schreiberSinglestepApproach2018,
  title = {A Single-Step Approach to Musical Tempo Estimation Using a Convolutional Neural Network.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Schreiber, Hendrik},
  year = {2018},
  pages = {95--105}
}

@inproceedings{schulmanGradientEstimation2015,
  title = {Gradient Estimation Using Stochastic Computation Graphs},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  year = {2015},
  pages = {3528--3536}
}

@inproceedings{schwarzPrinciplesApplications2008,
  title = {Principles and Applications of Interactive Corpus-Based Concatenative Synthesis},
  booktitle = {Journ{\'e}es d'{{Informatique Musicale}} ({{JIM}})},
  author = {Schwarz, Diemo and Cahen, Roland and Britton, Sam},
  year = {2008},
  file = {/home/daniel/Zotero/storage/LHJB79ZU/Schwarz, Cahen, Britton - Principles and applications of interactive corpus-based concatenative synthesis.pdf}
}

@book{sengpielHumanPerception2018,
  title = {The Human Perception of Loudness},
  author = {Sengpiel, E.},
  year = {2018}
}

@article{serraUniversalNeural2018,
  title = {Towards a Universal Neural Network Encoder for Time Series},
  author = {Serr{\`a}, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
  year = {2018},
  month = may,
  abstract = {We study the use of a time series encoder to learn representations that are useful on data set types with which it has not been trained on. The encoder is formed of a convolutional neural network whose temporal output is summarized by a convolutional attention mechanism. This way, we obtain a compact, fixed-length representation from longer, variable-length time series. We evaluate the performance of the proposed approach on a well-known time series classification benchmark, considering full adaptation, partial adaptation, and no adaptation of the encoder to the new data type. Results show that such strategies are competitive with the state-of-the-art, often outperforming conceptually-matching approaches. Besides accuracy scores, the facility of adaptation and the efficiency of pre-trained encoders make them an appealing option for the processing of scarcely- or non-labeled time series.},
  archivePrefix = {arXiv},
  eprint = {1805.03908},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/Z4UNEECK/Serrà et al. - 2018 - Towards a universal neural network encoder for tim.pdf;/home/daniel/Zotero/storage/HZ4UXJ7S/1805.html},
  journal = {arXiv:1805.03908 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{sharmaAutomaticLyricstoaudio2019,
  title = {Automatic {{Lyrics}}-to-Audio {{Alignment}} on {{Polyphonic Music Using Singing}}-Adapted {{Acoustic Models}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sharma, Bidisha and Gupta, Chitralekha and Li, Haizhou and Wang, Ye},
  year = {2019},
  month = may,
  pages = {396--400},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682582},
  isbn = {978-1-4799-8131-1}
}

@article{shelhamerClockworkConvnets2016,
  title = {Clockwork {{Convnets}} for {{Video Semantic Segmentation}}},
  author = {Shelhamer, Evan and Rakelly, Kate and Hoffman, Judy and Darrell, Trevor},
  year = {2016},
  volume = {abs/1608.03609},
  archivePrefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/ShelhamerRHD16},
  eprint = {1608.03609},
  eprinttype = {arxiv},
  journal = {CoRR}
}

@article{shepardStructuralRepresentations1982,
  title = {Structural Representations of Musical Pitch},
  author = {Shepard, Roger N},
  year = {1982},
  pages = {343--390},
  journal = {The psychology of music}
}

@article{shiNormalizedCuts2000,
  title = {Normalized Cuts and Image Segmentation},
  author = {Shi, Jianbo and Malik, Jitendra},
  year = {2000},
  volume = {22},
  pages = {888--905},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}

@book{siegelNonparametricStatistics1988,
  title = {Nonparametric Statistics for the Behavioral Sciences},
  author = {Siegel, S. and Castellan, N.J.},
  year = {1988},
  edition = {Second edition},
  publisher = {{McGraw\textendash{}Hill, Inc.}}
}

@inproceedings{sigtiaAudioChord2015,
  title = {Audio {{Chord Recognition}} with a {{Hybrid Recurrent Neural Network}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Sigtia, Siddharth and {Boulanger-Lewandowski}, Nicolas and Dixon, Simon},
  year = {2015}
}

@article{sigtiaEndtoEndNeural2015,
  title = {An {{End}}-to-{{End Neural Network}} for {{Polyphonic Music Transcription}}},
  author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
  year = {2015},
  journal = {arXiv preprint arXiv:1508.01774}
}

@inproceedings{sigtiaImprovedMusic2014,
  title = {Improved Music Feature Learning with Deep Neural Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sigtia, Siddharth and Dixon, Sam},
  year = {2014},
  pages = {6959--6963},
  publisher = {{IEEE}}
}

@article{simonyanDeepConvolutional2013,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6034}
}

@inproceedings{simpsonDeepKaraoke2015,
  title = {Deep Karaoke: {{Extracting}} Vocals from Musical Mixtures Using a Convolutional Deep Neural Network},
  booktitle = {International {{Conference}} on {{Latent Variable Analysis}} and {{Signal Separation}}},
  author = {Simpson, Andrew JR and Roma, Gerard and Plumbley, Mark D},
  year = {2015},
  pages = {429--436},
  publisher = {{Springer}}
}

@inproceedings{skovenborgEvaluationDifferent2004,
  title = {Evaluation of Different Loudness Models with Music and Speech Material},
  booktitle = {Proc. of the {{AES}} 117th {{Convention}}, {{San Francisco}}},
  author = {Skovenborg, Esben and Nielsen, S{\o}ren H},
  year = {2004}
}

@phdthesis{smithCorrelationAnalyses2013,
  title = {Correlation {{Analyses}} of {{Encoded Music Performance}}},
  author = {Smith, Jeffrey C.},
  year = {2013},
  address = {{Stanford, CA, USA}},
  school = {Stanford University},
  type = {{{PhD Thesis}}}
}

@inproceedings{smithDesignCreation2011,
  title = {Design and Creation of a Large-Scale Database of Structural Annotations.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Smith, Jordan Bennett Louis and Burgoyne, John Ashley and Fujinaga, Ichiro and De Roure, David and Downie, J Stephen},
  year = {2011}
}

@article{soltauNeuralSpeech2016,
  title = {Neural {{Speech Recognizer}}: {{Acoustic}}-to-{{Word LSTM Model}} for {{Large Vocabulary Speech Recognition}}},
  author = {Soltau, Hagen and Liao, Hank and Sak, Hasim},
  year = {2016},
  journal = {arXiv preprint arXiv:1610.09975}
}

@inproceedings{sonderbyAmortisedMap2017,
  title = {Amortised Map Inference for Image Super-Resolution},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {S{\o}nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Husz{\'a}r, Ferenc},
  year = {2017},
  file = {/home/daniel/Zotero/storage/WG3GKBM8/Sonderby, Caballero, Theis, Shi, Huszar - Amortised MAP inference for image super-resolution.pdf}
}

@article{sonderbyHowTrain2016,
  title = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks},
  author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.02282}
}

@article{sonderbyLadderVariational2016,
  title = {Ladder {{Variational Autoencoders}}},
  author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  journal = {arXiv preprint arXiv:1602.02282}
}

@article{speedVoiceSynthesis2012,
  title = {Voice Synthesis Using the Three-Dimensional Digital Waveguide Mesh},
  author = {Speed, Matthew DA},
  year = {2012}
}

@article{spratleyUnifiedNeural2019,
  title = {A {{Unified Neural Architecture}} for {{Instrumental Audio Tasks}}},
  author = {Spratley, Steven and Beck, Daniel and Cohn, Trevor},
  year = {2019},
  month = feb,
  abstract = {Within Music Information Retrieval (MIR), prominent tasks -- including pitch-tracking, source-separation, super-resolution, and synthesis -- typically call for specialised methods, despite their similarities. Conditional Generative Adversarial Networks (cGANs) have been shown to be highly versatile in learning general image-to-image translations, but have not yet been adapted across MIR. In this work, we present an end-to-end supervisable architecture to perform all aforementioned audio tasks, consisting of a WaveNet synthesiser conditioned on the output of a jointly-trained cGAN spectrogram translator. In doing so, we demonstrate the potential of such flexible techniques to unify MIR tasks, promote efficient transfer learning, and converge research to the improvement of powerful, general methods. Finally, to the best of our knowledge, we present the first application of GANs to guided instrument synthesis.},
  archivePrefix = {arXiv},
  eprint = {1903.00142},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/747EZ3EP/Spratley et al. - 2019 - A Unified Neural Architecture for Instrumental Aud.pdf;/home/daniel/Zotero/storage/35YT2R52/1903.html},
  journal = {arXiv:1903.00142 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@inproceedings{springenbergStrivingSimplicity2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}}) {{Workshop Track}}},
  author = {Springenberg, J. T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015}
}

@article{springenbergUnsupervisedSemisupervised2015,
  title = {Unsupervised and {{Semi}}-Supervised {{Learning}} with {{Categorical Generative Adversarial Networks}}},
  author = {Springenberg, Jost Tobias},
  year = {2015},
  month = nov,
  abstract = {In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
  archivePrefix = {arXiv},
  eprint = {1511.06390},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/VEX6HXSL/Springenberg - 2015 - Unsupervised and Semi-supervised Learning with Cat.pdf;/home/daniel/Zotero/storage/H3MRFZ4A/1511.html},
  journal = {arXiv:1511.06390 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{stollerAdversarialSemiSupervised2018,
  title = {Adversarial {{Semi}}-{{Supervised Audio Source Separation}} Applied to {{Singing Voice Extraction}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
  year = {2018},
  pages = {2391--2395},
  publisher = {{IEEE}},
  address = {{Calgary, Canada}},
  abstract = {The state of the art in music source separation employs neural networks trained in a supervised fashion on multi-track databases to estimate the sources from a given mixture. With only few datasets available, often extensive data augmentation is used to combat overfitting. Mixing random tracks, however, can even reduce separation performance as instruments in real music are strongly correlated. The key concept in our approach is that source estimates of an optimal separator should be indistinguishable from real source signals. Based on this idea, we drive the separator towards outputs deemed as realistic by discriminator networks that are trained to tell apart real from separator samples. This way, we can also use unpaired source and mixture recordings without the drawbacks of creating unrealistic music mixtures. Our framework is widely applicable as it does not assume a specific network architecture or number of sources. To our knowledge, this is the first adoption of adversarial training for music source separation. In a prototype experiment for singing voice separation, separation performance increases with our approach compared to purely supervised training.}
}

@inproceedings{stollerAnalysisClassification2016,
  title = {Analysis and Classification of Phonation Modes in Singing},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Stoller, Daniel and Dixon, Simon},
  year = {2016},
  volume = {17},
  pages = {80--86},
  abstract = {Phonation mode is an expressive aspect of the singing voice and can be described using the four categories neutral, breathy, pressed and flow. Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufficiently investigated the characteristic features of the different phonation modes which enable successful classification. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal flatness and average energy that correctly categorises 78\% of the recordings.}
}

@phdthesis{stollerConstrainedbasedRearrangement2015,
  title = {Constrained-Based Rearrangement of Music},
  author = {Stoller, Daniel},
  year = {2015},
  address = {{Germany}},
  school = {Technical University Dortmund},
  type = {Master's {{Thesis}}}
}

@inproceedings{stollerDetectionCutPoints2018,
  title = {Detection of {{Cut}}-{{Points}} for {{Automatic Music Rearrangement}}},
  booktitle = {2018 {{IEEE}} 28th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Stoller, D. and Akkermans, V. and Dixon, S.},
  year = {2018},
  pages = {1--6},
  doi = {10.1109/MLSP.2018.8516706},
  abstract = {Existing music recordings are often rearranged, for example to fit their duration and structure to video content. Often an expert is needed to find suitable cut points allowing for imperceptible transitions between different sections. In previous work, the search for these cuts is restricted to the beginnings of beats or measures and only timbre and loudness are taken into account, while melodic expectations and instrument continuity are neglected. We instead aim to learn these features by training neural networks on a dataset of over 300 popular Western songs to classify which note onsets are suitable entry or exit points for a cut. We investigate existing and novel architectures and different feature representations, and find that best performance is achieved using neural networks with two-dimensional convolutions applied to spectrogram input covering several seconds of audio with a high temporal resolution of 23 or 46 ms. Finally, we analyse our best model using saliency maps and find it attends to rhythmical structures and the presence of sounds at the onset position, suggesting instrument activity to be important for predicting cut quality.},
  file = {/home/daniel/Zotero/storage/SEIS2D3B/Stoller, Akkermans, Dixon - Detection of cut-points for automatic music rearrangement.pdf},
  keywords = {Adaptation models,Feature extraction,Instruments,Music,Neural networks,Task analysis,Training}
}

@inproceedings{stollerEndtoendLyrics2019,
  title = {End-to-End {{Lyrics Alignment}} for {{Polyphonic Music Using An Audio}}-to-{{Character Recognition Model}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Stoller, Daniel and Durand, Simon and Ewert, Sebastian},
  year = {2019},
  publisher = {{IEEE}},
  address = {{Brighton, UK}},
  abstract = {Time-aligned lyrics can enrich the music listening experience by enabling karaoke, text-based song retrieval and intra-song navigation, and other applications. Compared to text-to-speech alignment, lyrics alignment remains highly challenging, despite many attempts to combine numerous sub-modules including vocal separation and detection in an effort to break down the problem. Furthermore, training required fine-grained annotations to be available in some form. Here, we present a novel system based on a modified Wave-U-Net architecture, which predicts character probabilities directly from raw audio using learnt multi-scale representations of the various signal components. There are no sub-modules whose interdependencies need to be optimized. Our training procedure is designed to work with weak, line-level annotations available in the real world. With a mean alignment error of 0.35s on a standard dataset our system outperforms the state-of-the-art by an order of magnitude.},
  code = {https://github.com/f90/jamendolyrics},
  file = {/home/daniel/Zotero/storage/B5XJUFWQ/Stoller, Durand, Ewert - End-to-end lyrics alignment for polyphonic music using an audio-to-character recognition model.pdf}
}

@inproceedings{stollerImpactFrame2015,
  title = {Impact of {{Frame Size}} and {{Instrumentation}} on {{Chroma}}-{{Based Automatic Chord Recognition}}},
  booktitle = {Data {{Science}}, {{Learning}} by {{Latent Structures}}, and {{Knowledge Discovery}}},
  author = {Stoller, Daniel and Mauch, Matthias and Vatolkin, Igor and Weihs, Claus},
  editor = {Lausen, Berthold and {Krolak-Schwerdt}, Sabine and B{\"o}hmer, Matthias},
  year = {2015},
  pages = {411--421},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {This paper presents a comparative study of classification performance in automatic audio chord recognition based on three chroma feature implementations, with the aim of distinguishing effects of frame size, instrumentation, and choice of chroma feature. Until recently, research in automatic chord recognition has focused on the development of complete systems. While results have remarkably improved, the understanding of the error sources remains lacking. In order to isolate sources of chord recognition error, we create a corpus of artificial instrument mixtures and investigate (a) the influence of different chroma frame sizes and (b) the impact of instrumentation and pitch height. We show that recognition performance is significantly affected not only by the method used, but also by the nature of the audio input. We compare these results to those obtained from a corpus of more than 200 real-world pop songs from The Beatles and other artists for the case in which chord boundaries are known in advance.},
  isbn = {978-3-662-44983-7}
}

@article{stollerIntuitiveEfficient2018,
  title = {Intuitive and Efficient Computer-Aided Music Rearrangement with Optimised Processing of Audio Transitions},
  author = {Stoller, Daniel and Vatolkin, Igor and M{\"u}ller, Heinrich},
  year = {2018},
  volume = {47},
  pages = {416--437},
  doi = {10.1080/09298215.2018.1473448},
  abstract = {A promising approach to create new versions of existing music pieces automatically is to cut out and rearrange sections so that transitions are minimally perceptible and constraints regarding duration or structure are fulfilled. We evaluate previous work and improve on its limitations, particularly the disregard for loudness changes at cuts and the unintuitive control over the musical structure of the output. Our software provides a user-friendly interface, which we make more responsive by greatly accelerating the search for an optimal output track using the A* algorithm. Listening experiments demonstrate an improvement in perceived audio quality.},
  journal = {Journal of New Music Research},
  number = {5}
}

@inproceedings{stollerJointlyDetecting2018,
  title = {Jointly {{Detecting}} and {{Separating Singing Voice}}: {{A Multi}}-{{Task Approach}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
  editor = {Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic},
  year = {2018},
  pages = {329--339}
}

@inproceedings{stollerSeqUNetOneDimensional2020,
  title = {Seq-{{U}}-{{Net}}: {{A One}}-{{Dimensional Causal U}}-{{Net}} for {{Efficient Sequence Modelling}}},
  shorttitle = {Seq-{{U}}-{{Net}}},
  booktitle = {Proc. of the {{International Joint Conference}} on {{Artificial Intelligence}} - {{Pacific Rim International Conference}} on {{Artificial Intelligence}} ({{IJCAI}}-{{PRICAI}})},
  author = {Stoller, Daniel and Tian, Mi and Ewert, Sebastian and Dixon, Simon},
  year = {2020},
  abstract = {Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. However, efficiently modelling long-term dependencies in these sequences is still challenging. Although the receptive field of these models grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, prohibiting the use of longer receptive fields in practice. To increase efficiency, we make use of the "slow feature" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model ("Seq-U-Net") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance in all tasks.},
  code = {https://github.com/f90/Seq-U-Net},
  file = {/home/daniel/Zotero/storage/BBHPSMSI/Stoller et al. - 2019 - Seq-U-Net A One-Dimensional Causal U-Net for Effi.pdf;/home/daniel/Zotero/storage/YJJDVXD9/1911.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{stollerTrainingGenerative2020,
  title = {Training {{Generative Adversarial Networks}} from {{Incomplete Observations}} Using {{Factorised Discriminators}}},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
  year = {2020},
  code = {https://github.com/f90/FactorGAN}
}

@inproceedings{stollerWaveUNetMultiScale2018,
  title = {Wave-{{U}}-{{Net}}: {{A Multi}}-{{Scale Neural Network}} for {{End}}-to-{{End Source Separation}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
  year = {2018},
  volume = {19},
  pages = {334--340},
  abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
  file = {/home/daniel/Zotero/storage/RB7VFI4C/Stoller, Ewert, Dixon - Wave-u-net - A multi-scale neural network for end-to-end audio source separation.pdf}
}

@article{stoter2018Signal2018,
  title = {The 2018 {{Signal Separation Evaluation Campaign}}},
  author = {St{\"o}ter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka},
  year = {2018},
  month = jul,
  volume = {CoRR},
  abstract = {This paper reports the organization and results for the 2018 community-based Signal Separation Evaluation Campaign (SiSEC 2018). This year's edition was focused on audio and pursued the effort towards scaling up and making it easier to prototype audio separation software in an era of machine-learning based systems. For this purpose, we prepared a new music separation database: MUSDB18, featuring close to 10h of audio. Additionally, open-source software was released to automatically load, process and report performance on MUSDB18. Furthermore, a new official Python version for the BSSEval toolbox was released, along with reference implementations for three oracle separation methods: ideal binary mask, ideal ratio mask, and multichannel Wiener filter. We finally report the results obtained by the participants.},
  archivePrefix = {arXiv},
  eprint = {1804.06267},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/KKKDI4Z5/Stöter et al. - 2018 - The 2018 Signal Separation Evaluation Campaign.pdf;/home/daniel/Zotero/storage/5RF67AEB/1804.html},
  journal = {abs/1804.06267},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{stoterOpenUnmixReference2019,
  title = {Open-{{Unmix}} - {{A Reference Implementation}} for {{Music Source Separation}}},
  author = {St{\"o}ter, Fabian-Robert and Uhlich, Stefan and Liutkus, Antoine and Mitsufuji, Yuki},
  year = {2019},
  month = sep,
  volume = {4},
  pages = {1667},
  issn = {2475-9066},
  doi = {10.21105/joss.01667},
  journal = {Journal of Open Source Software},
  number = {41}
}

@inproceedings{stowellBirdDetection2016,
  title = {Bird Detection in Audio: {{A}} Survey and a Challenge},
  booktitle = {2016 {{IEEE}} 26th International Workshop on Machine Learning for Signal Processing ({{MLSP}})},
  author = {Stowell, D. and Wood, M. and Stylianou, Y. and Glotin, H.},
  year = {2016},
  pages = {1--6}
}

@book{stowellMakingMusic2010,
  title = {Making Music through Real-Time Voice Timbre Analysis: Machine Learning and Timbral Control},
  author = {Stowell, Dan},
  year = {2010},
  publisher = {{Queen Mary, University of London}}
}

@article{sturmClassificationAccuracy2013,
  title = {Classification Accuracy Is Not Enough},
  author = {Sturm, Bob L},
  year = {2013},
  volume = {41},
  pages = {371--406},
  journal = {Journal of Intelligent Information Systems},
  number = {3}
}

@inproceedings{sturmSurveyEvaluation2012,
  title = {A Survey of Evaluation in Music Genre Recognition},
  booktitle = {International {{Workshop}} on {{Adaptive Multimedia Retrieval}}},
  author = {Sturm, Bob L},
  year = {2012},
  pages = {29--66},
  publisher = {{Springer}}
}

@inproceedings{stylianouVoiceTransformation2009,
  title = {Voice Transformation: A Survey},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Stylianou, Yannis},
  year = {2009},
  pages = {3585--3588},
  publisher = {{IEEE}}
}

@article{subakanGenerativeAdversarial2017,
  title = {Generative {{Adversarial Source Separation}}},
  author = {S{\"u}bakan, Y. Cem and Smaragdis, Paris},
  year = {2017},
  volume = {abs/1710.10779},
  file = {/home/daniel/Zotero/storage/8BCC4DZQ/Sübakan und Smaragdis - 2017 - Generative Adversarial Source Separation.pdf;/home/daniel/Zotero/storage/JQJGX2BR/Subakan und Smaragdis - 2017 - Generative Adversarial Source Separation.pdf;/home/daniel/Zotero/storage/ML2B549L/1710.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound,Statistics - Machine Learning}
}

@inproceedings{sulyokCorpustaughtEvolutionary2015,
  title = {Corpus-Taught {{Evolutionary Music Composition}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Artificial Life}} 2015 ({{ECAL}} 2015)},
  author = {Sulyok, Csaba and McPherson, Andrew and Harte, Christopher},
  year = {2015},
  volume = {13},
  pages = {587--594}
}

@article{sundbergEstimatingPerceived2004,
  title = {Estimating Perceived Phonatory Pressedness in Singing from Flow Glottograms},
  author = {Sundberg, Johan and Thal{\'e}n, Margareta and Alku, Paavo and Vilkman, Erkki},
  year = {2004},
  volume = {18},
  pages = {56--62},
  journal = {Journal of Voice},
  number = {1}
}

@book{sundbergScienceSinging1987,
  title = {The Science of the Singing Voice},
  author = {Sundberg, Johan},
  year = {1987},
  publisher = {{Illinois University Press}}
}

@article{sundbergSubstylesBelting2012,
  title = {Substyles of {{Belting}}: {{Phonatory}} and {{Resonatory Characteristics}}},
  author = {Sundberg, Johan and Thal{\'e}n, Margareta and Popeil, Lisa},
  year = {2012},
  volume = {26},
  pages = {44--50},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/j.jvoice.2010.10.007},
  abstract = {Summary Belting has been described as speechlike, yell-like, or shouting voice production commonly used in contemporary commercial music genres and substantially differing from the esthetic of the Western classical voice tradition. This investigation attempts to describe phonation and resonance characteristics of different substyles of belting (heavy, brassy, ringy, nasal, and speechlike) and the classical style. A professional singer and voice teacher, skilled in these genres, served as the single subject. The recorded material was found representative according to a classification test performed by an expert panel. Subglottal pressure was measured as the oral pressure during the occlusion for the consonant /p/. The voice source and formant frequencies were analyzed by inverse filtering the audio signal. The subglottal pressure and measured flow glottogram parameters differed clearly between the styles heavy and classical assuming opposite extremes in most parameters. The formant frequencies, by contrast, showed fewer less systematic differences between the substyles but were clearly separated from the classical style with regard to the first formant. Thus, the differences between the belting substyles mainly concerned the voice source.},
  journal = {Journal of Voice},
  keywords = {Formant frequencies},
  number = {1}
}

@article{sundbergWhatSpecial1990,
  title = {What's so Special about Singers?},
  author = {Sundberg, Johan},
  year = {1990},
  volume = {4},
  pages = {107--119},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/S0892-1997(05)80135-3},
  abstract = {Summary Research on singers' breathing, phonation and articulation patterns during singing is reviewed and comparisons are made with typical speech patterns. It is found that singers' and nonsingers' use of the voice differ in several respects. The reasons for these differences are discussed and explanations are proposed referring to the special demands raised on singers with respect to economization of vocal effort and flexibility of phonation.},
  journal = {Journal of Voice},
  number = {2}
}

@article{sunderhaufLimitsPotentials2018,
  title = {The Limits and Potentials of Deep Learning for Robotics},
  author = {S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  year = {2018},
  volume = {37},
  pages = {405--420},
  publisher = {{SAGE Publications Sage UK: London, England}},
  journal = {The International Journal of Robotics Research},
  number = {4-5}
}

@inproceedings{sunUniversalSpeech2013,
  title = {Universal Speech Models for Speaker Independent Single Channel Source Separation},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Sun, Dennis L and Mysore, Gautham J},
  year = {2013},
  pages = {141--145},
  publisher = {{IEEE}}
}

@article{sutskeverPrincipledUnsupervised2015,
  title = {Towards {{Principled Unsupervised Learning}}},
  author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06440}
}

@article{szegedyIntriguingProperties2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6199}
}

@article{tachibanaSingingVoice2014,
  title = {Singing Voice Enhancement in Monaural Music Signals Based on Two-Stage Harmonic/Percussive Sound Separation on Multiple Resolution Spectrograms},
  author = {Tachibana, Hideyuki and Ono, Nobutaka and Sagayama, Shigeki},
  year = {2014},
  volume = {22},
  pages = {228--237},
  journal = {IEEE/ACM transactions on audio, speech, and language processing},
  number = {1}
}

@inproceedings{tauscherAudioResynthesis2013,
  title = {Audio {{Resynthesis}} on the {{Dancefloor}}: {{A Music Structural Approach}}},
  booktitle = {Proceedings of the {{Vision}}, {{Modeling}}, and {{Visualization Workshop}}, {{Lugano}}, {{Switzerland}}},
  author = {Tauscher, J. and Wenger, S. and Magnor, M.},
  year = {2013},
  pages = {41--48}
}

@inproceedings{taylorFactoredConditional2009,
  title = {Factored Conditional Restricted {{Boltzmann}} Machines for Modeling Motion Style},
  booktitle = {Proc. of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Taylor, Graham W and Hinton, Geoffrey E},
  year = {2009},
  pages = {1025--1032},
  publisher = {{ACM}}
}

@inproceedings{taylorModelingHuman2006,
  title = {Modeling Human Motion Using Binary Latent Variables},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Taylor, Graham W and Hinton, Geoffrey E and Roweis, Sam T},
  year = {2006},
  pages = {1345--1352}
}

@article{terasawaPerceptualDistance2005,
  title = {Perceptual Distance in Timbre Space},
  author = {Terasawa, Hiroko and Slaney, Malcolm and Berger, Jonathan},
  year = {2005}
}

@article{theanodevelopmentteamTheanoPython2016,
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  author = {{Theano Development Team}},
  year = {2016},
  month = may,
  volume = {abs/1605.02688},
  file = {/home/daniel/Zotero/storage/I38IT7L2/Theano Development Team - 2016 - Theano A Python framework for fast computation of.pdf},
  journal = {arXiv e-prints},
  keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation}
}

@article{theisNoteEvaluation2015,
  title = {A Note on the Evaluation of Generative Models},
  author = {Theis, Lucas and van den Oord, A{\"a}ron and Bethge, Matthias},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.01844}
}

@article{tranDeepHierarchical2017,
  title = {Deep and {{Hierarchical Implicit Models}}},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.08896}
}

@article{trehubInfantsAdults1999,
  title = {Infants' and Adults' Perception of Scale Structure.},
  author = {Trehub, Sandra E and Schellenberg, E Glenn and Kamenetsky, Stuart B},
  year = {1999},
  volume = {25},
  pages = {965},
  journal = {Journal of experimental psychology: Human perception and performance},
  number = {4}
}

@inproceedings{triantafillouMetalearningSemisupervised2018,
  title = {Meta-Learning for Semi-Supervised Few-Shot Classification},
  booktitle = {Proc. of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Triantafillou, Eleni and Larochelle, Hugo and Snell, Jake and Tenenbaum, Josh and Swersky, Kevin Jordan and Ren, Mengye and Zemel, Richard and Ravi, Sachin},
  year = {2018}
}

@article{trinhLearningLongerterm2018,
  title = {Learning {{Longer}}-Term {{Dependencies}} in {{RNNs}} with {{Auxiliary Losses}}},
  author = {Trinh, Trieu H. and Dai, Andrew M. and Luong, Minh-Thang and Le, Quoc V.},
  year = {2018},
  month = feb,
  volume = {abs/1803.00144},
  abstract = {Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\textbackslash,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.},
  archivePrefix = {arXiv},
  eprint = {1803.00144},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/DHRTKJKY/Trinh et al. - 2018 - Learning Longer-term Dependencies in RNNs with Aux.pdf;/home/daniel/Zotero/storage/ZRJ7I2DC/1803.html},
  journal = {CoRR},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{trinhSelfieSelfsupervised2019,
  title = {Selfie: {{Self}}-Supervised {{Pretraining}} for {{Image Embedding}}},
  shorttitle = {Selfie},
  author = {Trinh, Trieu H. and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = jun,
  abstract = {We introduce a pretraining technique called Selfie, which stands for SELF-supervised Image Embedding. Selfie generalizes the concept of masked language modeling to continuous data, such as images. Given masked-out patches in an input image, our method learns to select the correct patch, among other "distractor" patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate our method on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying amounts of labeled data, from 5\% to 100\% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 x 224 with 60 examples per class (5\%), our method improves the mean accuracy of ResNet-50 from 35.6\% to 46.7\%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across datasets.},
  archivePrefix = {arXiv},
  eprint = {1906.02940},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/QKNBZ8IN/Trinh et al. - 2019 - Selfie Self-supervised Pretraining for Image Embe.pdf;/home/daniel/Zotero/storage/JDUBSLHD/1906.html},
  journal = {arXiv:1906.02940},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning}
}

@article{tripathyLearningImagetoimage2018,
  title = {Learning Image-to-Image Translation Using Paired and Unpaired Training Samples},
  author = {Tripathy, Soumya and Kannala, Juho and Rahtu, Esa},
  year = {2018},
  month = may,
  volume = {abs/805.03189},
  abstract = {Image-to-image translation is a general name for a task where an image from one domain is converted to a corresponding image in another domain, given sufficient training data. Traditionally different approaches have been proposed depending on whether aligned image pairs or two sets of (unaligned) examples from both domains are available for training. While paired training samples might be difficult to obtain, the unpaired setup leads to a highly under-constrained problem and inferior results. In this paper, we propose a new general purpose image-to-image translation model that is able to utilize both paired and unpaired training data simultaneously. We compare our method with two strong baselines and obtain both qualitatively and quantitatively improved results. Our model outperforms the baselines also in the case of purely paired and unpaired training data. To our knowledge, this is the first work to consider such hybrid setup in image-to-image translation.},
  archivePrefix = {arXiv},
  eprint = {1805.03189},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/BGNVYTWW/Tripathy et al. - 2018 - Learning image-to-image translation using paired a.pdf;/home/daniel/Zotero/storage/AXNBVL7E/1805.html},
  journal = {CoRR},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{tsaiTranscribingLyrics2018,
  title = {Transcribing {{Lyrics}} from {{Commercial Song Audio}}: The {{First Step Towards Singing Content Processing}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tsai, Che-Ping and Tuan, Yi-Lin and Lee, Lin-shan},
  year = {2018},
  pages = {5749--5753},
  doi = {10.1109/ICASSP.2018.8462247},
  file = {/home/daniel/Zotero/storage/ZWN77JC3/Tsai, Tuan, Lee - Transcribing lyrics from commercial song audio - the first step towards singing content processing.pdf},
  keywords = {Acoustic Model Adaptation,Acoustics,Adaptation models,Data models,Genre,Hidden Markov models,Human voice,Lyrics,Prolonged Vowels,Song Audio,Testing,Training}
}

@article{turneriiiQualitativeInterview2010,
  title = {Qualitative Interview Design: {{A}} Practical Guide for Novice Investigators},
  author = {Turner III, Daniel W},
  year = {2010},
  volume = {15},
  pages = {754},
  journal = {The qualitative report},
  number = {3}
}

@article{tzanetakisMusicalGenre2002,
  title = {Musical Genre Classification of Audio Signals},
  author = {Tzanetakis, George and Cook, Perry},
  year = {2002},
  volume = {10},
  pages = {293--302},
  publisher = {{IEEE}},
  journal = {IEEE Transactions on Speech and Audio Processing},
  number = {5}
}

@inproceedings{tzengSimultaneousDeep2015,
  title = {Simultaneous Deep Transfer across Domains and Tasks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate},
  year = {2015},
  pages = {4068--4076}
}

@inproceedings{uhlichDeepNeural2015,
  title = {Deep Neural Network Based Instrument Extraction from Music},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Uhlich, Stefan and Giron, Franck and Mitsufuji, Yuki},
  year = {2015},
  pages = {2135--2139},
  publisher = {{IEEE}}
}

@inproceedings{uhlichImprovingMusic2017,
  title = {Improving Music Source Separation Based on Deep Neural Networks through Data Augmentation and Network Blending},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Uhlich, S. and Porcu, M. and Giron, F. and Enenkl, M. and Kemp, T. and Takahashi, N. and Mitsufuji, Y.},
  year = {2017},
  month = mar,
  pages = {261--265},
  doi = {10.1109/ICASSP.2017.7952158},
  keywords = {Blending,blending scheme yields,Context,data augmentation,Deep neural network (DNN),deep neural network architectures,Indexes,Instruments,Long-short term memory (LSTM),mixture models,multichannel Wiener filter post-processing,music,Music source separation (MSS),music source separation improvement,recurrent network,recurrent neural nets,Recurrent neural networks,SiSEC DSD100 dataset,Source separation,speech processing,Training,Wiener filters}
}

@inproceedings{ullrichBoundaryDetection2014,
  title = {Boundary {{Detection}} in {{Music Structure Analysis}} Using {{Convolutional Neural Networks}}},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Ullrich, Karen and Schl{\"u}ter, Jan and Grill, Thomas},
  year = {2014},
  address = {{Taipei, Taiwan}}
}

@article{vandenbroekeWhatAutoencoders2016,
  title = {What Auto-Encoders Could Learn from Brains-{{Generation}} as Feedback in Deep Unsupervised Learning and Inference},
  author = {Van Den Broeke, Gerben and others},
  year = {2016}
}

@inproceedings{vandenoordConditionalImage2016,
  title = {Conditional Image Generation with Pixelcnn Decoders},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{van den Oord}, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},
  year = {2016},
  pages = {4790--4798}
}

@inproceedings{vandenoordTransferLearning2014,
  title = {Transfer Learning by Supervised Pre-Training for Audio-Based Music Classification},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {{van den Oord}, A{\"a}ron and Dieleman, Sander and Schrauwen, Benjamin},
  year = {2014}
}

@article{vandenoordWaveNetGenerative2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  author = {{van den Oord}, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  journal = {arXiv:1609.03499}
}

@article{vasquezMelNetGenerative2019,
  title = {{{MelNet}}: {{A Generative Model}} for {{Audio}} in the {{Frequency Domain}}},
  shorttitle = {{{MelNet}}},
  author = {Vasquez, Sean and Lewis, Mike},
  year = {2019},
  month = jun,
  volume = {abs/1906.01083},
  abstract = {Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.},
  archivePrefix = {arXiv},
  eprint = {1906.01083},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/QRY6AZ2B/Vasquez und Lewis - 2019 - MelNet A Generative Model for Audio in the Freque.pdf;/home/daniel/Zotero/storage/DPABTKV8/1906.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{vaswaniAttentionAll2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  volume = {abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/FD6Y88ER/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/daniel/Zotero/storage/BJUKEKJW/1706.html},
  journal = {CoRR},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{vatolkinEvolutionaryMultiobjective2019,
  title = {Evolutionary Multi-Objective Training Set Selection of Data Instances and Augmentations for Vocal Detection},
  booktitle = {8th {{International Conference}} on {{Computational Intelligence}} in {{Music}}, {{Sound}}, {{Art}} and {{Design}} ({{EvoMUSART}})},
  author = {Vatolkin, Igor and Stoller, Daniel},
  year = {2019},
  pages = {201--216},
  doi = {10.1007/978-3-030-16667-0_14}
}

@article{venkataramaniEndtoendSource2017,
  title = {End-to-End {{Source Separation}} with {{Adaptive Front}}-{{Ends}}},
  author = {Venkataramani, Shrikant and Smaragdis, Paris},
  year = {2017},
  volume = {abs/1705.02514},
  file = {/home/daniel/Zotero/storage/EHVSK9VK/Venkataramani und Smaragdis - 2017 - End-to-end Source Separation with Adaptive Front-E.pdf},
  journal = {CoRR}
}

@article{venkataramaniPerformanceBased2018,
  title = {Performance {{Based Cost Functions}} for {{End}}-to-{{End Speech Separation}}},
  author = {Venkataramani, Shrikant and Higa, Ryley and Smaragdis, Paris},
  year = {2018},
  month = jun,
  abstract = {Recent neural network strategies for source separation attempt to model audio signals by processing their waveforms directly. Mean squared error (MSE) that measures the Euclidean distance between waveforms of denoised speech and the ground-truth speech, has been a natural cost-function for these approaches. However, MSE is not a perceptually motivated measure and may result in large perceptual discrepancies. In this paper, we propose and experiment with new loss functions for end-to-end source separation. These loss functions are motivated by BSS\textbackslash\_Eval and perceptual metrics like source to distortion ratio (SDR), source to interference ratio (SIR), source to artifact ratio (SAR) and short-time objective intelligibility ratio (STOI). This enables the flexibility to mix and match these loss functions depending upon the requirements of the task. Subjective listening tests reveal that combinations of the proposed cost functions help achieve superior separation performance as compared to stand-alone MSE and SDR costs.},
  archivePrefix = {arXiv},
  eprint = {1806.00511},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/X9ZMZRNZ/Venkataramani et al. - 2018 - Performance Based Cost Functions for End-to-End Sp.pdf;/home/daniel/Zotero/storage/67YUFWQN/1806.html},
  journal = {arXiv:1806.00511 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  primaryClass = {cs, eess}
}

@article{verdoliniLaryngealAdduction1998,
  title = {Laryngeal Adduction in Resonant Voice},
  author = {Verdolini, Katherine and Druker, David G. and Palmer, Phyllis M. and Samawi, Hani},
  year = {1998},
  volume = {12},
  pages = {315--327},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/S0892-1997(98)80021-0},
  abstract = {Summary The primary question in this study was whether subjects with nodules and subjects with healthy larynges would produce ``resonant voice\dbend{}? with a similar laryngeal configuration. A second question regarded whether the electroglottographic closed quotient (EGG CQ) could be used to noninvasively distinguish resonant from other voice types. Twelve adult singers and actors served as subjects, including 6 persons with healthy larynges and 6 persons with nodules. Performers were used as an attempt to maximize token validity and stability. Subjects produced repeated tokens of resonant, pressed, normal, and breathy voice during sustained vowels. Laryngeal adduction was directly estimated using blinded, ordinal, visual-perceptual ratings based on videoscopic views of the larynx. \{EGG\} \{CQs\} were further calculated based on separate trials. The perceptual ratings indicated that subjects in both groups produced resonant voice with a barely adducted or barely abducted laryngeal configuration that was distinct from configurations for pressed and breathy (but not normal) voice. Previous literature suggests that this configuration may be relevant in many cases of voice therapy (I). Average \{CQs\} distinguished resonant from pressed voice, but inconsistently distinguished resonant from breathy voice. Further \{CQs\} were reliably different across healthy subjects and subjects with nodules. Thus, the utility of this measure to noninvasively estimate resonant voice may be limited, particularly without ongoing subject-specific calibration procedures.},
  journal = {Journal of Voice},
  keywords = {Resonant voice},
  number = {3}
}

@inproceedings{verfaillePerceptualEvaluation2005,
  title = {Perceptual Evaluation of Vibrato Models},
  booktitle = {Conference on {{Interdisciplinary Musicology}}},
  author = {Verfaille, Vincent and Guastavino, Catherine},
  year = {2005},
  publisher = {{Citeseer}}
}

@inproceedings{verhelstOverlapaddTechnique1993,
  title = {An Overlap-Add Technique Based on Waveform Similarity ({{WSOLA}}) for High Quality Time-Scale Modification of Speech},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Verhelst, W. and Roelands, M.},
  year = {1993},
  pages = {554--557},
  keywords = {fast Fourier transforms,online operation,online processing,short-time Fourier transform,speech analysis and processing,synchronized overlap-add,time-scale modification of speech,time-varying systems,waveform analysis,waveform similarity}
}

@article{vermaNeuralogramDeep2019,
  title = {Neuralogram: {{A Deep Neural Network Based Representation}} for {{Audio Signals}}},
  shorttitle = {Neuralogram},
  author = {Verma, Prateek and Chafe, Chris and Berger, Jonathan},
  year = {2019},
  month = apr,
  abstract = {We propose the Neuralogram -- a deep neural network based representation for understanding audio signals which, as the name suggests, transforms an audio signal to a dense, compact representation based upon embeddings learned via a neural architecture. Through a series of probing signals, we show how our representation can encapsulate pitch, timbre and rhythm-based information, and other attributes. This representation suggests a method for revealing meaningful relationships in arbitrarily long audio signals that are not readily represented by existing algorithms. This has the potential for numerous applications in audio understanding, music recommendation, meta-data extraction to name a few.},
  archivePrefix = {arXiv},
  eprint = {1904.05073},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/D2HJ27ER/Verma et al. - 2019 - Neuralogram A Deep Neural Network Based Represent.pdf;/home/daniel/Zotero/storage/MNE8AL6C/1904.html},
  journal = {arXiv:1904.05073 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{vincentImprovedPerceptual2012,
  title = {Improved {{Perceptual Metrics}} for the {{Evaluation}} of {{Audio Source Separation}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Vincent, Emmanuel},
  year = {2012},
  pages = {430--437},
  publisher = {{Springer}},
  abstract = {We aim to predict the perceived quality of estimated source signals in the context of audio source separation. Recently, we proposed a set of metrics called PEASS that consist of three computation steps: decomposition of the estimation error into three components, measurement of the salience of each component via the PEMO-Q auditory-motivated measure, and combination of these saliences via a nonlinear mapping trained on subjective opinion scores. The parameters of the decomposition were shown to have little influence on the prediction performance. In this paper, we evaluate the impact of the parameters of PEMO-Q and the nonlinear mapping on the prediction performance. By selecting the optimal parameters, we improve the average correlation with mean opinion scores (MOS) from 0.738 to 0.909 in a cross-validation setting. The resulting improved metrics are used in the context of the 2011 Signal Separation Evaluation Campaign (SiSEC).},
  file = {/home/daniel/Zotero/storage/UW5PXWYL/Vincent - Improved perceptual metrics for the evaluation of audio source separation.pdf},
  isbn = {978-3-642-28551-6}
}

@article{vincentPerformanceMeasurement2006,
  title = {Performance Measurement in Blind Audio Source Separation},
  author = {Vincent, E. and Gribonval, R. and Fevotte, C.},
  year = {2006},
  volume = {14},
  pages = {1462--1469},
  issn = {1558-7916},
  doi = {10.1109/TSA.2005.858005},
  file = {/home/daniel/Zotero/storage/J8YYRV8S/Vincent et al. - 2006 - Performance measurement in blind audio source sepa.pdf},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {additive noise,Additive noise,algorithmic artifacts,audio signal processing,Audio source separation,blind audio source separation,blind source separation,Data mining,distortion,Distortion measurement,distortions,Energy measurement,evaluation,Filters,Image analysis,Independent component analysis,interference,Interference,measure,Microphones,performance,quality,source estimation,Source separation,time-invariant gains,time-varying filters},
  number = {4}
}

@article{vincentStackedDenoising2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  volume = {11},
  pages = {3371--3408},
  journal = {Journal of Machine Learning Research},
  number = {Dec}
}

@inproceedings{vinyalsMatchingNetworks2016,
  title = {Matching Networks for One Shot Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim and Wierstra, Daan and others},
  year = {2016},
  pages = {3630--3638}
}

@article{vinyalsOrderMatters2015,
  title = {Order Matters: {{Sequence}} to Sequence for Sets},
  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06391}
}

@inproceedings{voglDrumTranscription2017,
  title = {Drum Transcription via Joint Beat and Drum Modeling Using Convolutional Recurrent Neural Networks},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Vogl, Richard and Dorfer, Matthias and Widmer, Gerhard and Knees, Peter},
  year = {2017},
  pages = {150--157}
}

@inproceedings{wagerIntonationDataset2019,
  title = {Intonation: {{A Dataset}} of {{Quality Vocal Performances Refined}} by {{Spectral Clustering}} on {{Pitch Congruence}}},
  shorttitle = {Intonation},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wager, Sanna and Tzanetakis, George and Sullivan, Stefan and Wang, Cheng-i and Shimmin, John and Kim, Minje and Cook, Perry},
  year = {2019},
  month = may,
  pages = {476--480},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683554},
  isbn = {978-1-4799-8131-1}
}

@inproceedings{wangEncodingTime2015,
  title = {Encoding {{Time Series}} as {{Images}} for {{Visual Inspection}} and {{Classification Using Tiled Convolutional Neural Networks}}},
  booktitle = {Workshops at the {{Twenty}}-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Zhiguang and Oates, Tim},
  year = {2015}
}

@inproceedings{wangLyricAllyAutomatic2004,
  title = {{{LyricAlly}}: Automatic Synchronization of Acoustic Musical Signals and Textual Lyrics},
  booktitle = {Proceedings of the {{Annual ACM International Conference}} on {{Multimedia}} ({{ACMMM}})},
  author = {Wang, Ye and Kan, Min-Yen and Nwe, Tin Lay and Shenoy, Arun and Yin, Jun},
  year = {2004},
  pages = {212--219}
}

@article{wangSuperGLUEStickier2019,
  title = {{{SuperGLUE}}: {{A}} Stickier Benchmark for General-Purpose Language Understanding Systems},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2019},
  volume = {abs/1905.00537},
  journal = {CoRR}
}

@article{wangSupervisedSpeech2018,
  title = {Supervised Speech Separation Based on Deep Learning: {{An}} Overview},
  author = {Wang, DeLiang and Chen, Jitong},
  year = {2018},
  volume = {26},
  pages = {1702--1726},
  publisher = {{IEEE}},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  number = {10}
}

@incollection{waschkaiiComposingGenetic2007,
  title = {Composing with {{Genetic Algorithms}}: {{GenDash}}},
  booktitle = {Evolutionary {{Computer Music}}},
  author = {Waschka II, Rodney},
  year = {2007},
  pages = {117--136},
  publisher = {{Springer}}
}

@article{wattsSingingPower2006,
  title = {The {{Singing Power Ratio}} as an {{Objective Measure}} of {{Singing Voice Quality}} in {{Untrained Talented}} and {{Nontalented Singers}}},
  author = {Watts, Christopher and {Barnes-Burroughs}, Kathryn and Estis, Julie and Blanton, Debra},
  year = {2006},
  volume = {20},
  pages = {82--88},
  issn = {0892-1997},
  doi = {http://dx.doi.org/10.1016/j.jvoice.2004.12.003},
  abstract = {Summary A growing body of contemporary research has investigated differences between trained and untrained singing voices. However, few studies have separated untrained singers into those who do and do not express abilities related to singing talent, including accurate pitch control and production of a pleasant timbre (voice quality). This investigation studied measures of the singing power ratio (SPR), which is a quantitative measure of the resonant quality of the singing voice. \{SPR\} reflects the amplification or suppression in the vocal tract of the harmonics produced by the sound source. This measure was acquired from the voices of untrained talented and nontalented singers as a means to objectively investigate voice quality differences. Measures of \{SPR\} were acquired from vocal samples with fast Fourier transform (FFT) power spectra to analyze the amplitude level of the partials in the acoustic spectrum. Long-term average spectra (LTAS) were also analyzed. Results indicated significant differences in \{SPR\} between groups, which suggest that vocal tract resonance, and its effect on perceived vocal timbre or quality, may be an important variable related to the perception of singing talent. \{LTAS\} confirmed group differences in the tuning of vocal tract harmonics.},
  journal = {Journal of Voice},
  keywords = {Singing talent},
  number = {1}
}

@article{websterBriefComparison2014,
  title = {A {{Brief Comparison}} of {{Loudness Evaluation Methods}}},
  author = {Webster, P. and Jiuicek, O.},
  year = {2014},
  volume = {20},
  pages = {8--11},
  journal = {Acoustic Sheets, Czech Technical University in Prague},
  number = {2}
}

@inproceedings{wengerConstrainedExampleBased2011,
  title = {Constrained {{Example}}-{{Based Audio Synthesis}}},
  booktitle = {International {{Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Wenger, S. and Magnor, M.},
  year = {2011},
  pages = {1--6}
}

@inproceedings{wengerGeneticAlgorithm2012,
  title = {A {{Genetic Algorithm}} for {{Audio Retargeting}}},
  booktitle = {{{ACM Multimedia}} ({{ACMMM}})},
  author = {Wenger, S. and Magnor, M.},
  year = {2012},
  pages = {705--708}
}

@article{weningerAutomaticAssessment2011,
  title = {Automatic Assessment of Singer Traits in Popular Music: {{Gender}}, Age, Height and Race},
  author = {Weninger, Felix and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  year = {2011},
  volume = {48},
  pages = {510},
  journal = {young (y)}
}

@phdthesis{wennerMusicRetargeting2012,
  title = {Music {{Retargeting}} and {{Synthesis}}},
  author = {Wenner, S.},
  year = {2012},
  school = {Swiss Federal Institute of Technology Zurich},
  type = {Master's {{Thesis}}}
}

@article{wennerScalableMusic2013,
  title = {Scalable {{Music}}: {{Automatic Music Retargeting}} and {{Synthesis}}},
  author = {Wenner, Simon and Bazin, Jean-Charles and {Sorkine-Hornung}, Alexander and Kim, Changil and Gross, Markus},
  year = {2013},
  volume = {32},
  pages = {345--354},
  journal = {Computer Graphics Forum (Proceedings of Eurographics 2013)},
  number = {2}
}

@book{wfmuFreeMusic2009,
  title = {Free {{Music Archive}}},
  author = {{WFMU}},
  year = {2009}
}

@article{wiskottSlowFeature2002,
  title = {Slow {{Feature Analysis}}: {{Unsupervised Learning}} of {{Invariances}}},
  shorttitle = {Slow {{Feature Analysis}}},
  author = {Wiskott, Laurenz and Sejnowski, Terrence J.},
  year = {2002},
  month = apr,
  volume = {14},
  pages = {715--770},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602317318938},
  file = {/home/daniel/Zotero/storage/9HJ26GVW/Wiskott und Sejnowski - 2002 - Slow Feature Analysis Unsupervised Learning of In.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{wuUnsupervisedFeature2018,
  title = {Unsupervised {{Feature Learning}} via {{Non}}-{{Parametric Instance}}-Level {{Discrimination}}},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
  year = {2018},
  month = may,
  abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
  archivePrefix = {arXiv},
  eprint = {1805.01978},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/FQ6TCTF8/Wu et al. - 2018 - Unsupervised Feature Learning via Non-Parametric I.pdf;/home/daniel/Zotero/storage/379QUGTQ/1805.html},
  journal = {arXiv:1805.01978 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{yangLowRankRepresentation2013,
  title = {Low-{{Rank Representation}} of {{Both Singing Voice}} and {{Music Accompaniment Via Learned Dictionaries}}.},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Yang, Yi-Hsuan},
  year = {2013},
  pages = {427--432}
}

@inproceedings{ycartComparativeStudy2019,
  title = {A Comparative Study of Neural Models for Polyphonic Music Sequence Transduction},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Ycart, Adrien and Stoller, Daniel and Benetos, Emmanouil},
  year = {2019},
  pages = {470--477}
}

@inproceedings{ycartPolyphonicMusic2018,
  title = {Polyphonic {{Music Sequence Transduction}} with {{Meter}}-{{Constrained LSTM Networks}}},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ycart, Adrien and Benetos, Emmanouil},
  year = {2018},
  month = apr,
  pages = {386--390},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2018.8462128},
  file = {/home/daniel/Zotero/storage/2J5VPMTJ/Ycart und Benetos - 2018 - Polyphonic Music Sequence Transduction with Meter-.pdf},
  isbn = {978-1-5386-4658-8}
}

@inproceedings{ycartStudyLSTM2017,
  title = {A Study on {{LSTM}} Networks for Polyphonic Music Sequence Modelling},
  booktitle = {Proc. of the {{International Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}})},
  author = {Ycart, Adrien and Benetos, Emmanouil},
  year = {2017},
  pages = {421--427},
  organization = {{ISMIR}}
}

@incollection{yoonFeaturebasedSynchronization2006,
  title = {Feature-Based Synchronization of Video and Background Music},
  booktitle = {Advances in {{Machine Vision}}, {{Image Processing}}, and {{Pattern Analysis}}},
  author = {Yoon, Jong-Chul and Lee, In-Kwon and Lee, Hyun-Chul},
  year = {2006},
  pages = {205--214},
  publisher = {{Springer}}
}

@article{yoonGAINMissing2018,
  title = {{{GAIN}}: {{Missing Data Imputation}} Using {{Generative Adversarial Nets}}},
  shorttitle = {{{GAIN}}},
  author = {Yoon, Jinsung and Jordon, James and {van der Schaar}, Mihaela},
  year = {2018},
  month = jun,
  volume = {abs/1806.02920},
  abstract = {We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.},
  archivePrefix = {arXiv},
  eprint = {1806.02920},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/LJA4XG56/Yoon et al. - 2018 - GAIN Missing Data Imputation using Generative Adv.pdf;/home/daniel/Zotero/storage/Y9UVY7LC/1806.html},
  journal = {CoRR},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{yosinskiUnderstandingNeural2015,
  title = {Understanding Neural Networks through Deep Visualization},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  year = {2015},
  journal = {arXiv preprint arXiv:1506.06579}
}

@inproceedings{yuanMultisourceLearning2012,
  title = {Multi-Source Learning for Joint Analysis of Incomplete Multi-Modality Neuroimaging Data},
  booktitle = {18th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Yuan, Lei and Wang, Yalin and Thompson, Paul M and Narayan, Vaibhav A and Ye, Jieping},
  year = {2012},
  pages = {1149--1157},
  publisher = {{ACM}}
}

@article{yuMultiscaleContext2015,
  title = {Multi-Scale Context Aggregation by Dilated Convolutions},
  author = {Yu, Fisher and Koltun, Vladlen},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.07122}
}

@inproceedings{yuSeqGANSequence2017,
  title = {{{SeqGAN}}: {{Sequence Generative Adversarial Nets}} with {{Policy Gradient}}},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  year = {2017},
  pages = {2852--2858}
}

@article{zacharakisInterlanguageUnification2015,
  title = {An {{Interlanguage Unification}} of {{Musical Timbre}}},
  author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D},
  year = {2015},
  volume = {32},
  pages = {394--412},
  journal = {Music Perception: An Interdisciplinary Journal},
  number = {4}
}

@article{zarSignificanceTesting1972,
  title = {Significance Testing of the {{Spearman}} Rank Correlation Coefficient},
  author = {Zar, J. H},
  year = {1972},
  volume = {67},
  pages = {578--580},
  journal = {Journal of the American Statistical Association},
  number = {339}
}

@inproceedings{zeilerDeconvolutionalNetworks2010,
  title = {Deconvolutional Networks},
  booktitle = {Proc. of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
  year = {2010},
  pages = {2528--2535},
  publisher = {{IEEE}}
}

@incollection{zeilerVisualizingUnderstanding2014,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer Vision\textendash{{ECCV}} 2014},
  author = {Zeiler, Matthew D and Fergus, Rob},
  year = {2014},
  pages = {818--833},
  publisher = {{Springer}}
}

@article{zhangLearningAudio2017,
  title = {Learning {{Audio Sequence Representations}} for {{Acoustic Event Classification}}},
  author = {Zhang, Zixing and Liu, Ding and Han, Jing and Schuller, Bj{\"o}rn},
  year = {2017},
  month = jul,
  abstract = {Acoustic Event Classification (AEC) has become a significant task for machines to perceive the surrounding auditory scene. However, extracting effective representations that capture the underlying characteristics of the acoustic events is still challenging. Previous methods mainly focused on designing the audio features in a 'hand-crafted' manner. Interestingly, data-learnt features have been recently reported to show better performance. Up to now, these were only considered on the frame-level. In this paper, we propose an unsupervised learning framework to learn a vector representation of an audio sequence for AEC. This framework consists of a Recurrent Neural Network (RNN) encoder and a RNN decoder, which respectively transforms the variable-length audio sequence into a fixed-length vector and reconstructs the input sequence on the generated vector. After training the encoder-decoder, we feed the audio sequences to the encoder and then take the learnt vectors as the audio sequence representations. Compared with previous methods, the proposed method can not only deal with the problem of arbitrary-lengths of audio streams, but also learn the salient information of the sequence. Extensive evaluation on a large-size acoustic event database is performed, and the empirical results demonstrate that the learnt audio sequence representation yields a significant performance improvement by a large margin compared with other state-of-the-art hand-crafted sequence features for AEC.},
  archivePrefix = {arXiv},
  eprint = {1707.08729},
  eprinttype = {arxiv},
  file = {/home/daniel/Zotero/storage/JMXGXVRK/Zhang et al. - 2017 - Learning Audio Sequence Representations for Acoust.pdf;/home/daniel/Zotero/storage/NBFHEX6Z/1707.html},
  journal = {arXiv:1707.08729 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@article{zhangUnsupervisedAudio2017,
  title = {Unsupervised {{Audio Source Separation}} via {{Spectrum Energy Preserved Wasserstein Learning}}},
  author = {Zhang, Ning and Yan, Junchi and Zhou, Yu Chen},
  year = {2017},
  volume = {abs/1711.04121},
  file = {/home/daniel/Zotero/storage/A9BPI5LK/Zhang et al. - 2017 - Unsupervised Audio Source Separation via Spectrum .pdf},
  journal = {CoRR}
}

@article{zhaoDeeperUnderstanding2017,
  title = {Towards {{Deeper Understanding}} of {{Variational Autoencoding Models}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.08658}
}

@article{zhaoEnergybasedGenerative2016,
  title = {Energy-Based Generative Adversarial Network},
  author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.03126}
}

@article{zhengComparisonDifferent2001,
  title = {Comparison of Different Implementations of {{MFCC}}},
  author = {Zheng, F. and Zhang, Gu. and Song, Z.},
  year = {2001},
  volume = {16},
  pages = {582--589},
  issn = {1000-9000},
  doi = {10.1007/BF02943243},
  journal = {Journal of Computer Science and Technology},
  keywords = {auto-regressive analysis,frequency band energy,generalized initial/final,MFCC},
  language = {English},
  number = {6}
}

@inproceedings{zhuUnpairedImagetoimage2017,
  title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
  booktitle = {Proc. of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  year = {2017},
  pages = {2223--2232},
  file = {/home/daniel/Zotero/storage/2QHCYDK4/Zhu et al. - 2017 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;/home/daniel/Zotero/storage/V5TF6NEI/1703.html}
}

@article{zwickerProgramCalculating1991,
  title = {Program for Calculating Loudness According to {{DIN}} 45631 ({{ISO 532B}}).},
  author = {Zwicker, E. and Fastl, H. and Widmann, U. and Kurakata, K. and Kuwano, S. and Namba, S.},
  year = {1991},
  volume = {12},
  pages = {39--42},
  journal = {Journal of the Acoustical Society of Japan (E)},
  number = {1}
}


