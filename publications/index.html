<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>List of publications - Dans World</title>
<meta name="description" content="Machine learning, music information retrieval, and other things">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_GB">
<meta property="og:site_name" content="Dans World">
<meta property="og:title" content="List of publications">
<meta property="og:url" content="https://dans.world/publications/">












  

  


<link rel="canonical" href="https://dans.world/publications/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Daniel Stoller",
      "url": "https://dans.world",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Dans World Feed">

<!-- 

 -->

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="/assets/css/academicons.css">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Dans World</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/research/" >Research</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/publications/" >Publications</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/posts/" >Blog</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Daniel Stoller</h3>
    
    
      <p class="author__bio" itemprop="description">
        Researcher in Machine Learning and Music Information Retrieval.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">London, UK</span>
        </li>
      

      

      
        <li>
          <a href="mailto:business@dstoller.net">
            <meta itemprop="email" content="business@dstoller.net" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/daniel-stoller" itemprop="sameAs">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/f90" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

        <li>
    <a href="https://orcid.org/0000-0002-8615-4144" itemprop="sameAs">
      <i class="ai ai-orcid-square ai-fw"></i> ORCID
    </a>
  </li>

  <li>
    <a href="https://scholar.google.co.uk/citations?user=Ozxm6UsAAAAJ" itemprop="sameAs">
      <i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar
    </a>
</li>

  <li>
    <a href="https://qmul.academia.edu/DanielStoller" itemprop="sameAs">
      <i class="ai ai-academia-square ai-fw"></i> Academia.edu
    </a>
</li>
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="List of publications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">List of publications
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="journal-articles">Journal articles</h2>

<ol class="bibliography"><li><div>
<a name="stollerIntuitiveEfficient2018" />
<b>Intuitive and Efficient Computer-Aided Music Rearrangement with Optimised Processing of Audio Transitions</b> <span style="font-size:15px"> (2018) </span>
<br />


  Journal of New Music Research
  <br />

<i>Stoller, Daniel and Vatolkin, Igor and Müller, Heinrich</i>
</div>


    






<a download="stollerIntuitiveEfficient2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@article%7BstollerIntuitiveEfficient2018,%0A%20%20title%20=%20%7BIntuitive%20and%20Efficient%20Computer-Aided%20Music%20Rearrangement%20with%20Optimised%20Processing%20of%20Audio%20Transitions%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Vatolkin,%20Igor%20and%20M%7B%5C%22u%7Dller,%20Heinrich%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20volume%20=%20%7B47%7D,%0A%20%20pages%20=%20%7B416--437%7D,%0A%20%20doi%20=%20%7B10.1080/09298215.2018.1473448%7D,%0A%20%20abstract%20=%20%7BA%20promising%20approach%20to%20create%20new%20versions%20of%20existing%20music%20pieces%20automatically%20is%20to%20cut%20out%20and%20rearrange%20sections%20so%20that%20transitions%20are%20minimally%20perceptible%20and%20constraints%20regarding%20duration%20or%20structure%20are%20fulfilled.%20We%20evaluate%20previous%20work%20and%20improve%20on%20its%20limitations,%20particularly%20the%20disregard%20for%20loudness%20changes%20at%20cuts%20and%20the%20unintuitive%20control%20over%20the%20musical%20structure%20of%20the%20output.%20Our%20software%20provides%20a%20user-friendly%20interface,%20which%20we%20make%20more%20responsive%20by%20greatly%20accelerating%20the%20search%20for%20an%20optimal%20output%20track%20using%20the%20A*%20algorithm.%20Listening%20experiments%20demonstrate%20an%20improvement%20in%20perceived%20audio%20quality.%7D,%0A%20%20journal%20=%20%7BJournal%20of%20New%20Music%20Research%7D,%0A%20%20number%20=%20%7B5%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerIntuitiveEfficient2018/">Details</a></li></ol>

<h2 id="conference-papers">Conference papers</h2>

<ol class="bibliography"><li><div>
<a name="stollerSeqUNetOneDimensional2020" />
<b>Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling</b> <span style="font-size:15px"> (2020) </span>
<br />

  Proc. of the International Joint Conference on Artificial Intelligence - Pacific Rim International Conference on Artificial Intelligence (IJCAI-PRICAI)
  <br />


<i>Stoller, Daniel and Tian, Mi and Ewert, Sebastian and Dixon, Simon</i>
</div>


    
        <a target="_blank" rel="noopener noreferrer" href="/repository/stollerSeqUNetOneDimensional2020.preprint.pdf"><input class="button0" type="button" value="Preprint PDF" /></a>
    



    <a target="_blank" rel="noopener noreferrer" href="https://github.com/f90/Seq-U-Net"><input class="button0" type="button" value="Code" /></a>




<a download="stollerSeqUNetOneDimensional2020.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerSeqUNetOneDimensional2020,%0A%20%20title%20=%20%7BSeq-%7B%7BU%7D%7D-%7B%7BNet%7D%7D:%20%7B%7BA%20One%7D%7D-%7B%7BDimensional%20Causal%20U%7D%7D-%7B%7BNet%7D%7D%20for%20%7B%7BEfficient%20Sequence%20Modelling%7D%7D%7D,%0A%20%20shorttitle%20=%20%7BSeq-%7B%7BU%7D%7D-%7B%7BNet%7D%7D%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BInternational%20Joint%20Conference%7D%7D%20on%20%7B%7BArtificial%20Intelligence%7D%7D%20-%20%7B%7BPacific%20Rim%20International%20Conference%7D%7D%20on%20%7B%7BArtificial%20Intelligence%7D%7D%20(%7B%7BIJCAI%7D%7D-%7B%7BPRICAI%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Tian,%20Mi%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2020%7D,%0A%20%20abstract%20=%20%7BConvolutional%20neural%20networks%20(CNNs)%20with%20dilated%20filters%20such%20as%20the%20Wavenet%20or%20the%20Temporal%20Convolutional%20Network%20(TCN)%20have%20shown%20good%20results%20in%20a%20variety%20of%20sequence%20modelling%20tasks.%20However,%20efficiently%20modelling%20long-term%20dependencies%20in%20these%20sequences%20is%20still%20challenging.%20Although%20the%20receptive%20field%20of%20these%20models%20grows%20exponentially%20with%20the%20number%20of%20layers,%20computing%20the%20convolutions%20over%20very%20long%20sequences%20of%20features%20in%20each%20layer%20is%20time%20and%20memory-intensive,%20prohibiting%20the%20use%20of%20longer%20receptive%20fields%20in%20practice.%20To%20increase%20efficiency,%20we%20make%20use%20of%20the%20%22slow%20feature%22%20hypothesis%20stating%20that%20many%20features%20of%20interest%20are%20slowly%20varying%20over%20time.%20For%20this,%20we%20use%20a%20U-Net%20architecture%20that%20computes%20features%20at%20multiple%20time-scales%20and%20adapt%20it%20to%20our%20auto-regressive%20scenario%20by%20making%20convolutions%20causal.%20We%20apply%20our%20model%20(%22Seq-U-Net%22)%20to%20a%20variety%20of%20tasks%20including%20language%20and%20audio%20generation.%20In%20comparison%20to%20TCN%20and%20Wavenet,%20our%20network%20consistently%20saves%20memory%20and%20computation%20time,%20with%20speed-ups%20for%20training%20and%20inference%20of%20over%204x%20in%20the%20audio%20generation%20experiment%20in%20particular,%20while%20achieving%20a%20comparable%20performance%20in%20all%20tasks.%7D,%0A%20%20code%20=%20%7Bhttps://github.com/f90/Seq-U-Net%7D,%0A%20%20keywords%20=%20%7BComputer%20Science%20-%20Machine%20Learning,Computer%20Science%20-%20Sound,Electrical%20Engineering%20and%20Systems%20Science%20-%20Audio%20and%20Speech%20Processing,Statistics%20-%20Machine%20Learning%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerSeqUNetOneDimensional2020/">Details</a></li>
<li><div>
<a name="stollerTrainingGenerative2020" />
<b>Training Generative Adversarial Networks from Incomplete Observations Using Factorised Discriminators</b> <span style="font-size:15px"> (2020) </span>
<br />

  Proc. of the International Conference on Learning Representations (ICLR)
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerTrainingGenerative2020.published.pdf"><input class="button0" type="button" value="PDF" /></a>



    <a target="_blank" rel="noopener noreferrer" href="https://github.com/f90/FactorGAN"><input class="button0" type="button" value="Code" /></a>




<a download="stollerTrainingGenerative2020.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerTrainingGenerative2020,%0A%20%20title%20=%20%7BTraining%20%7B%7BGenerative%20Adversarial%20Networks%7D%7D%20from%20%7B%7BIncomplete%20Observations%7D%7D%20Using%20%7B%7BFactorised%20Discriminators%7D%7D%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BInternational%20Conference%7D%7D%20on%20%7B%7BLearning%20Representations%7D%7D%20(%7B%7BICLR%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2020%7D,%0A%20%20code%20=%20%7Bhttps://github.com/f90/FactorGAN%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerTrainingGenerative2020/">Details</a></li>
<li><div>
<a name="chettriEnsembleModels2019" />
<b>Ensemble Models for Spoofing Detection in Automatic Speaker Verification</b> <span style="font-size:15px"> (2019) </span>
<br />

  Proceedings of INTERSPEECH
  <br />


<i>Chettri, Bhusan and Stoller, Daniel and Morfi, Veronica and Ramírez, Marco A. Martínez and Benetos, Emmanouil and Sturm, Bob L.</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/chettriEnsembleModels2019.published.pdf"><input class="button0" type="button" value="PDF" /></a>



    <a target="_blank" rel="noopener noreferrer" href="https://github.com/BhusanChettri/ASVspoof2019/"><input class="button0" type="button" value="Code" /></a>




<a download="chettriEnsembleModels2019.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BchettriEnsembleModels2019,%0A%20%20title%20=%20%7BEnsemble%20Models%20for%20Spoofing%20Detection%20in%20Automatic%20Speaker%20Verification%7D,%0A%20%20booktitle%20=%20%7BProceedings%20of%20%7B%7BINTERSPEECH%7D%7D%7D,%0A%20%20author%20=%20%7BChettri,%20Bhusan%20and%20Stoller,%20Daniel%20and%20Morfi,%20Veronica%20and%20Ram%7B%5C'i%7Drez,%20Marco%20A.%20Mart%7B%5C'i%7Dnez%20and%20Benetos,%20Emmanouil%20and%20Sturm,%20Bob%20L.%7D,%0A%20%20year%20=%20%7B2019%7D,%0A%20%20pages%20=%20%7B1018--1022%7D,%0A%20%20code%20=%20%7Bhttps://github.com/BhusanChettri/ASVspoof2019/%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/chettriEnsembleModels2019/">Details</a></li>
<li><div>
<a name="mishraGANbasedGeneration2019" />
<b>GAN-Based Generation and Automatic Selection of Explanations for Neural Networks</b> <span style="font-size:15px"> (2019) </span>
<br />

  Safe Machine Learning Workshop at the International Conference on Learning Representations (ICLR)
  <br />


<i>Mishra, Saumitra and Stoller, Daniel and Benetos, Emmanouil and Sturm, Bob L. and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/mishraGANbasedGeneration2019.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="mishraGANbasedGeneration2019.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BmishraGANbasedGeneration2019,%0A%20%20title%20=%20%7B%7B%7BGAN%7D%7D-Based%20Generation%20and%20Automatic%20Selection%20of%20Explanations%20for%20Neural%20Networks%7D,%0A%20%20booktitle%20=%20%7BSafe%20%7B%7BMachine%20Learning%20Workshop%7D%7D%20at%20the%20%7B%7BInternational%20Conference%7D%7D%20on%20%7B%7BLearning%20Representations%7D%7D%20(%7B%7BICLR%7D%7D)%7D,%0A%20%20author%20=%20%7BMishra,%20Saumitra%20and%20Stoller,%20Daniel%20and%20Benetos,%20Emmanouil%20and%20Sturm,%20Bob%20L.%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2019%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/mishraGANbasedGeneration2019/">Details</a></li>
<li><div>
<a name="stollerEndtoendLyrics2019" />
<b>End-to-End Lyrics Alignment for Polyphonic Music Using An Audio-to-Character Recognition Model</b> <span style="font-size:15px"> (2019) </span>
<br />

  Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  <br />


<i>Stoller, Daniel and Durand, Simon and Ewert, Sebastian</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerEndtoendLyrics2019.published.pdf"><input class="button0" type="button" value="PDF" /></a>



    <a target="_blank" rel="noopener noreferrer" href="https://github.com/f90/jamendolyrics"><input class="button0" type="button" value="Code" /></a>




<a download="stollerEndtoendLyrics2019.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerEndtoendLyrics2019,%0A%20%20title%20=%20%7BEnd-to-End%20%7B%7BLyrics%20Alignment%7D%7D%20for%20%7B%7BPolyphonic%20Music%20Using%20An%20Audio%7D%7D-to-%7B%7BCharacter%20Recognition%20Model%7D%7D%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BIEEE%20International%20Conference%7D%7D%20on%20%7B%7BAcoustics%7D%7D,%20%7B%7BSpeech%7D%7D%20and%20%7B%7BSignal%20Processing%7D%7D%20(%7B%7BICASSP%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Durand,%20Simon%20and%20Ewert,%20Sebastian%7D,%0A%20%20year%20=%20%7B2019%7D,%0A%20%20publisher%20=%20%7B%7BIEEE%7D%7D,%0A%20%20address%20=%20%7B%7BBrighton,%20UK%7D%7D,%0A%20%20abstract%20=%20%7BTime-aligned%20lyrics%20can%20enrich%20the%20music%20listening%20experience%20by%20enabling%20karaoke,%20text-based%20song%20retrieval%20and%20intra-song%20navigation,%20and%20other%20applications.%20Compared%20to%20text-to-speech%20alignment,%20lyrics%20alignment%20remains%20highly%20challenging,%20despite%20many%20attempts%20to%20combine%20numerous%20sub-modules%20including%20vocal%20separation%20and%20detection%20in%20an%20effort%20to%20break%20down%20the%20problem.%20Furthermore,%20training%20required%20fine-grained%20annotations%20to%20be%20available%20in%20some%20form.%20Here,%20we%20present%20a%20novel%20system%20based%20on%20a%20modified%20Wave-U-Net%20architecture,%20which%20predicts%20character%20probabilities%20directly%20from%20raw%20audio%20using%20learnt%20multi-scale%20representations%20of%20the%20various%20signal%20components.%20There%20are%20no%20sub-modules%20whose%20interdependencies%20need%20to%20be%20optimized.%20Our%20training%20procedure%20is%20designed%20to%20work%20with%20weak,%20line-level%20annotations%20available%20in%20the%20real%20world.%20With%20a%20mean%20alignment%20error%20of%200.35s%20on%20a%20standard%20dataset%20our%20system%20outperforms%20the%20state-of-the-art%20by%20an%20order%20of%20magnitude.%7D,%0A%20%20code%20=%20%7Bhttps://github.com/f90/jamendolyrics%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerEndtoendLyrics2019/">Details</a></li>
<li><div>
<a name="vatolkinEvolutionaryMultiobjective2019" />
<b>Evolutionary Multi-Objective Training Set Selection of Data Instances and Augmentations for Vocal Detection</b> <span style="font-size:15px"> (2019) </span>
<br />

  8th International Conference on Computational Intelligence in Music, Sound, Art and Design (EvoMUSART)
  <br />


<i>Vatolkin, Igor and Stoller, Daniel</i>
</div>


    






<a download="vatolkinEvolutionaryMultiobjective2019.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BvatolkinEvolutionaryMultiobjective2019,%0A%20%20title%20=%20%7BEvolutionary%20Multi-Objective%20Training%20Set%20Selection%20of%20Data%20Instances%20and%20Augmentations%20for%20Vocal%20Detection%7D,%0A%20%20booktitle%20=%20%7B8th%20%7B%7BInternational%20Conference%7D%7D%20on%20%7B%7BComputational%20Intelligence%7D%7D%20in%20%7B%7BMusic%7D%7D,%20%7B%7BSound%7D%7D,%20%7B%7BArt%7D%7D%20and%20%7B%7BDesign%7D%7D%20(%7B%7BEvoMUSART%7D%7D)%7D,%0A%20%20author%20=%20%7BVatolkin,%20Igor%20and%20Stoller,%20Daniel%7D,%0A%20%20year%20=%20%7B2019%7D,%0A%20%20pages%20=%20%7B201--216%7D,%0A%20%20doi%20=%20%7B10.1007/978-3-030-16667-0_14%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/vatolkinEvolutionaryMultiobjective2019/">Details</a></li>
<li><div>
<a name="ycartComparativeStudy2019" />
<b>A Comparative Study of Neural Models for Polyphonic Music Sequence Transduction</b> <span style="font-size:15px"> (2019) </span>
<br />

  Proc. of the International Society for Music Information Retrieval Conference (ISMIR)
  <br />


<i>Ycart, Adrien and Stoller, Daniel and Benetos, Emmanouil</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/ycartComparativeStudy2019.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="ycartComparativeStudy2019.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BycartComparativeStudy2019,%0A%20%20title%20=%20%7BA%20Comparative%20Study%20of%20Neural%20Models%20for%20Polyphonic%20Music%20Sequence%20Transduction%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BInternational%20Society%7D%7D%20for%20%7B%7BMusic%20Information%20Retrieval%20Conference%7D%7D%20(%7B%7BISMIR%7D%7D)%7D,%0A%20%20author%20=%20%7BYcart,%20Adrien%20and%20Stoller,%20Daniel%20and%20Benetos,%20Emmanouil%7D,%0A%20%20year%20=%20%7B2019%7D,%0A%20%20pages%20=%20%7B470--477%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/ycartComparativeStudy2019/">Details</a></li>
<li><div>
<a name="stollerAdversarialSemiSupervised2018" />
<b>Adversarial Semi-Supervised Audio Source Separation Applied to Singing Voice Extraction</b> <span style="font-size:15px"> (2018) </span>
<br />

  Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerAdversarialSemiSupervised2018.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="stollerAdversarialSemiSupervised2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerAdversarialSemiSupervised2018,%0A%20%20title%20=%20%7BAdversarial%20%7B%7BSemi%7D%7D-%7B%7BSupervised%20Audio%20Source%20Separation%7D%7D%20Applied%20to%20%7B%7BSinging%20Voice%20Extraction%7D%7D%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BIEEE%20International%20Conference%7D%7D%20on%20%7B%7BAcoustics%7D%7D,%20%7B%7BSpeech%7D%7D%20and%20%7B%7BSignal%20Processing%7D%7D%20(%7B%7BICASSP%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20pages%20=%20%7B2391--2395%7D,%0A%20%20publisher%20=%20%7B%7BIEEE%7D%7D,%0A%20%20address%20=%20%7B%7BCalgary,%20Canada%7D%7D,%0A%20%20abstract%20=%20%7BThe%20state%20of%20the%20art%20in%20music%20source%20separation%20employs%20neural%20networks%20trained%20in%20a%20supervised%20fashion%20on%20multi-track%20databases%20to%20estimate%20the%20sources%20from%20a%20given%20mixture.%20With%20only%20few%20datasets%20available,%20often%20extensive%20data%20augmentation%20is%20used%20to%20combat%20overfitting.%20Mixing%20random%20tracks,%20however,%20can%20even%20reduce%20separation%20performance%20as%20instruments%20in%20real%20music%20are%20strongly%20correlated.%20The%20key%20concept%20in%20our%20approach%20is%20that%20source%20estimates%20of%20an%20optimal%20separator%20should%20be%20indistinguishable%20from%20real%20source%20signals.%20Based%20on%20this%20idea,%20we%20drive%20the%20separator%20towards%20outputs%20deemed%20as%20realistic%20by%20discriminator%20networks%20that%20are%20trained%20to%20tell%20apart%20real%20from%20separator%20samples.%20This%20way,%20we%20can%20also%20use%20unpaired%20source%20and%20mixture%20recordings%20without%20the%20drawbacks%20of%20creating%20unrealistic%20music%20mixtures.%20Our%20framework%20is%20widely%20applicable%20as%20it%20does%20not%20assume%20a%20specific%20network%20architecture%20or%20number%20of%20sources.%20To%20our%20knowledge,%20this%20is%20the%20first%20adoption%20of%20adversarial%20training%20for%20music%20source%20separation.%20In%20a%20prototype%20experiment%20for%20singing%20voice%20separation,%20separation%20performance%20increases%20with%20our%20approach%20compared%20to%20purely%20supervised%20training.%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerAdversarialSemiSupervised2018/">Details</a></li>
<li><div>
<a name="stollerDetectionCutPoints2018" />
<b>Detection of Cut-Points for Automatic Music Rearrangement</b> <span style="font-size:15px"> (2018) </span>
<br />

  2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)
  <br />


<i>Stoller, D. and Akkermans, V. and Dixon, S.</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerDetectionCutPoints2018.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="stollerDetectionCutPoints2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerDetectionCutPoints2018,%0A%20%20title%20=%20%7BDetection%20of%20%7B%7BCut%7D%7D-%7B%7BPoints%7D%7D%20for%20%7B%7BAutomatic%20Music%20Rearrangement%7D%7D%7D,%0A%20%20booktitle%20=%20%7B2018%20%7B%7BIEEE%7D%7D%2028th%20%7B%7BInternational%20Workshop%7D%7D%20on%20%7B%7BMachine%20Learning%7D%7D%20for%20%7B%7BSignal%20Processing%7D%7D%20(%7B%7BMLSP%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20D.%20and%20Akkermans,%20V.%20and%20Dixon,%20S.%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20pages%20=%20%7B1--6%7D,%0A%20%20doi%20=%20%7B10.1109/MLSP.2018.8516706%7D,%0A%20%20abstract%20=%20%7BExisting%20music%20recordings%20are%20often%20rearranged,%20for%20example%20to%20fit%20their%20duration%20and%20structure%20to%20video%20content.%20Often%20an%20expert%20is%20needed%20to%20find%20suitable%20cut%20points%20allowing%20for%20imperceptible%20transitions%20between%20different%20sections.%20In%20previous%20work,%20the%20search%20for%20these%20cuts%20is%20restricted%20to%20the%20beginnings%20of%20beats%20or%20measures%20and%20only%20timbre%20and%20loudness%20are%20taken%20into%20account,%20while%20melodic%20expectations%20and%20instrument%20continuity%20are%20neglected.%20We%20instead%20aim%20to%20learn%20these%20features%20by%20training%20neural%20networks%20on%20a%20dataset%20of%20over%20300%20popular%20Western%20songs%20to%20classify%20which%20note%20onsets%20are%20suitable%20entry%20or%20exit%20points%20for%20a%20cut.%20We%20investigate%20existing%20and%20novel%20architectures%20and%20different%20feature%20representations,%20and%20find%20that%20best%20performance%20is%20achieved%20using%20neural%20networks%20with%20two-dimensional%20convolutions%20applied%20to%20spectrogram%20input%20covering%20several%20seconds%20of%20audio%20with%20a%20high%20temporal%20resolution%20of%2023%20or%2046%20ms.%20Finally,%20we%20analyse%20our%20best%20model%20using%20saliency%20maps%20and%20find%20it%20attends%20to%20rhythmical%20structures%20and%20the%20presence%20of%20sounds%20at%20the%20onset%20position,%20suggesting%20instrument%20activity%20to%20be%20important%20for%20predicting%20cut%20quality.%7D,%0A%20%20keywords%20=%20%7BAdaptation%20models,Feature%20extraction,Instruments,Music,Neural%20networks,Task%20analysis,Training%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerDetectionCutPoints2018/">Details</a></li>
<li><div>
<a name="stollerJointlyDetecting2018" />
<b>Jointly Detecting and Separating Singing Voice: A Multi-Task Approach</b> <span style="font-size:15px"> (2018) </span>
<br />

  Latent Variable Analysis and Signal Separation
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerJointlyDetecting2018.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="stollerJointlyDetecting2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerJointlyDetecting2018,%0A%20%20title%20=%20%7BJointly%20%7B%7BDetecting%7D%7D%20and%20%7B%7BSeparating%20Singing%20Voice%7D%7D:%20%7B%7BA%20Multi%7D%7D-%7B%7BTask%20Approach%7D%7D%7D,%0A%20%20booktitle%20=%20%7BLatent%20%7B%7BVariable%20Analysis%7D%7D%20and%20%7B%7BSignal%20Separation%7D%7D%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20editor%20=%20%7BDeville,%20Yannick%20and%20Gannot,%20Sharon%20and%20Mason,%20Russell%20and%20Plumbley,%20Mark%20D.%20and%20Ward,%20Dominic%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20pages%20=%20%7B329--339%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerJointlyDetecting2018/">Details</a></li>
<li><div>
<a name="stollerWaveUNetMultiScale2018" />
<b>Wave-U-Net: A Multi-Scale Neural Network for End-to-End Source Separation</b> <span style="font-size:15px"> (2018) </span>
<br />

  Proc. of the International Society for Music Information Retrieval Conference (ISMIR)
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerWaveUNetMultiScale2018.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="stollerWaveUNetMultiScale2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerWaveUNetMultiScale2018,%0A%20%20title%20=%20%7BWave-%7B%7BU%7D%7D-%7B%7BNet%7D%7D:%20%7B%7BA%20Multi%7D%7D-%7B%7BScale%20Neural%20Network%7D%7D%20for%20%7B%7BEnd%7D%7D-to-%7B%7BEnd%20Source%20Separation%7D%7D%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BInternational%20Society%7D%7D%20for%20%7B%7BMusic%20Information%20Retrieval%20Conference%7D%7D%20(%7B%7BISMIR%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20volume%20=%20%7B19%7D,%0A%20%20pages%20=%20%7B334--340%7D,%0A%20%20abstract%20=%20%7BModels%20for%20audio%20source%20separation%20usually%20operate%20on%20the%20magnitude%20spectrum,%20which%20ignores%20phase%20information%20and%20makes%20separation%20performance%20dependant%20on%20hyper-parameters%20for%20the%20spectral%20front-end.%20Therefore,%20we%20investigate%20end-to-end%20source%20separation%20in%20the%20time-domain,%20which%20allows%20modelling%20phase%20information%20and%20avoids%20fixed%20spectral%20transformations.%20Due%20to%20high%20sampling%20rates%20for%20audio,%20employing%20a%20long%20temporal%20input%20context%20on%20the%20sample%20level%20is%20difficult,%20but%20required%20for%20high%20quality%20separation%20results%20because%20of%20long-range%20temporal%20correlations.%20In%20this%20context,%20we%20propose%20the%20Wave-U-Net,%20an%20adaptation%20of%20the%20U-Net%20to%20the%20one-dimensional%20time%20domain,%20which%20repeatedly%20resamples%20feature%20maps%20to%20compute%20and%20combine%20features%20at%20different%20time%20scales.%20We%20introduce%20further%20architectural%20improvements,%20including%20an%20output%20layer%20that%20enforces%20source%20additivity,%20an%20upsampling%20technique%20and%20a%20context-aware%20prediction%20framework%20to%20reduce%20output%20artifacts.%20Experiments%20for%20singing%20voice%20separation%20indicate%20that%20our%20architecture%20yields%20a%20performance%20comparable%20to%20a%20state-of-the-art%20spectrogram-based%20U-Net%20architecture,%20given%20the%20same%20data.%20Finally,%20we%20reveal%20a%20problem%20with%20outliers%20in%20the%20currently%20used%20SDR%20evaluation%20metrics%20and%20suggest%20reporting%20rank-based%20statistics%20to%20alleviate%20this%20problem.%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerWaveUNetMultiScale2018/">Details</a></li>
<li><div>
<a name="stollerAnalysisClassification2016" />
<b>Analysis and Classification of Phonation Modes in Singing</b> <span style="font-size:15px"> (2016) </span>
<br />

  Proc. of the International Society for Music Information Retrieval Conference (ISMIR)
  <br />


<i>Stoller, Daniel and Dixon, Simon</i>
</div>


    <a target="_blank" rel="noopener noreferrer" href="/repository/stollerAnalysisClassification2016.published.pdf"><input class="button0" type="button" value="PDF" /></a>






<a download="stollerAnalysisClassification2016.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerAnalysisClassification2016,%0A%20%20title%20=%20%7BAnalysis%20and%20Classification%20of%20Phonation%20Modes%20in%20Singing%7D,%0A%20%20booktitle%20=%20%7BProc.%20of%20the%20%7B%7BInternational%20Society%7D%7D%20for%20%7B%7BMusic%20Information%20Retrieval%20Conference%7D%7D%20(%7B%7BISMIR%7D%7D)%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Dixon,%20Simon%7D,%0A%20%20year%20=%20%7B2016%7D,%0A%20%20volume%20=%20%7B17%7D,%0A%20%20pages%20=%20%7B80--86%7D,%0A%20%20abstract%20=%20%7BPhonation%20mode%20is%20an%20expressive%20aspect%20of%20the%20singing%20voice%20and%20can%20be%20described%20using%20the%20four%20categories%20neutral,%20breathy,%20pressed%20and%20flow.%20Previous%20attempts%20at%20automatically%20classifying%20the%20phonation%20mode%20on%20a%20dataset%20containing%20vowels%20sung%20by%20a%20female%20professional%20have%20been%20lacking%20in%20accuracy%20or%20have%20not%20sufficiently%20investigated%20the%20characteristic%20features%20of%20the%20different%20phonation%20modes%20which%20enable%20successful%20classification.%20In%20this%20paper,%20we%20extract%20a%20large%20range%20of%20features%20from%20this%20dataset,%20including%20specialised%20descriptors%20of%20pressedness%20and%20breathiness,%20to%20analyse%20their%20explanatory%20power%20and%20robustness%20against%20changes%20of%20pitch%20and%20vowel.%20We%20train%20and%20optimise%20a%20feed-forward%20neural%20network%20(NN)%20with%20one%20hidden%20layer%20on%20all%20features%20using%20cross%20validation%20to%20achieve%20a%20mean%20F-measure%20above%200.85%20and%20an%20improved%20performance%20compared%20to%20previous%20work.%20Applying%20feature%20selection%20based%20on%20mutual%20information%20and%20retaining%20the%20nine%20highest%20ranked%20features%20as%20input%20to%20a%20NN%20results%20in%20a%20mean%20F-measure%20of%200.78,%20demonstrating%20the%20suitability%20of%20these%20features%20to%20discriminate%20between%20phonation%20modes.%20Training%20and%20pruning%20a%20decision%20tree%20yields%20a%20simple%20rule%20set%20based%20only%20on%20cepstral%20peak%20prominence%20(CPP),%20temporal%20flatness%20and%20average%20energy%20that%20correctly%20categorises%2078%5C%25%20of%20the%20recordings.%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerAnalysisClassification2016/">Details</a></li>
<li><div>
<a name="stollerImpactFrame2015" />
<b>Impact of Frame Size and Instrumentation on Chroma-Based Automatic Chord Recognition</b> <span style="font-size:15px"> (2015) </span>
<br />

  Data Science, Learning by Latent Structures, and Knowledge Discovery
  <br />


<i>Stoller, Daniel and Mauch, Matthias and Vatolkin, Igor and Weihs, Claus</i>
</div>


    






<a download="stollerImpactFrame2015.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BstollerImpactFrame2015,%0A%20%20title%20=%20%7BImpact%20of%20%7B%7BFrame%20Size%7D%7D%20and%20%7B%7BInstrumentation%7D%7D%20on%20%7B%7BChroma%7D%7D-%7B%7BBased%20Automatic%20Chord%20Recognition%7D%7D%7D,%0A%20%20booktitle%20=%20%7BData%20%7B%7BScience%7D%7D,%20%7B%7BLearning%7D%7D%20by%20%7B%7BLatent%20Structures%7D%7D,%20and%20%7B%7BKnowledge%20Discovery%7D%7D%7D,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Mauch,%20Matthias%20and%20Vatolkin,%20Igor%20and%20Weihs,%20Claus%7D,%0A%20%20editor%20=%20%7BLausen,%20Berthold%20and%20%7BKrolak-Schwerdt%7D,%20Sabine%20and%20B%7B%5C%22o%7Dhmer,%20Matthias%7D,%0A%20%20year%20=%20%7B2015%7D,%0A%20%20pages%20=%20%7B411--421%7D,%0A%20%20publisher%20=%20%7B%7BSpringer%20Berlin%20Heidelberg%7D%7D,%0A%20%20address%20=%20%7B%7BBerlin,%20Heidelberg%7D%7D,%0A%20%20abstract%20=%20%7BThis%20paper%20presents%20a%20comparative%20study%20of%20classification%20performance%20in%20automatic%20audio%20chord%20recognition%20based%20on%20three%20chroma%20feature%20implementations,%20with%20the%20aim%20of%20distinguishing%20effects%20of%20frame%20size,%20instrumentation,%20and%20choice%20of%20chroma%20feature.%20Until%20recently,%20research%20in%20automatic%20chord%20recognition%20has%20focused%20on%20the%20development%20of%20complete%20systems.%20While%20results%20have%20remarkably%20improved,%20the%20understanding%20of%20the%20error%20sources%20remains%20lacking.%20In%20order%20to%20isolate%20sources%20of%20chord%20recognition%20error,%20we%20create%20a%20corpus%20of%20artificial%20instrument%20mixtures%20and%20investigate%20(a)%20the%20influence%20of%20different%20chroma%20frame%20sizes%20and%20(b)%20the%20impact%20of%20instrumentation%20and%20pitch%20height.%20We%20show%20that%20recognition%20performance%20is%20significantly%20affected%20not%20only%20by%20the%20method%20used,%20but%20also%20by%20the%20nature%20of%20the%20audio%20input.%20We%20compare%20these%20results%20to%20those%20obtained%20from%20a%20corpus%20of%20more%20than%20200%20real-world%20pop%20songs%20from%20The%20Beatles%20and%20other%20artists%20for%20the%20case%20in%20which%20chord%20boundaries%20are%20known%20in%20advance.%7D,%0A%20%20isbn%20=%20%7B978-3-662-44983-7%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a>
<a class="details" href="/repository/stollerImpactFrame2015/">Details</a></li></ol>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
    
    
    
      <li><a href="https://github.com/f90"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Daniel Stoller. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.1.0/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>





  </body>
</html>