<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>List of publications - Dans World</title>
<meta name="description" content="Machine learning, music information retrieval, and other things">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_GB">
<meta property="og:site_name" content="Dans World">
<meta property="og:title" content="List of publications">
<meta property="og:url" content="https://dans.world/publications/">












  

  


<link rel="canonical" href="https://dans.world/publications/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Daniel Stoller",
      "url": "https://dans.world",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Dans World Feed">

<!-- 
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

 -->

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="/assets/css/academicons.css">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Dans World</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/research/" >Research</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/publications/" >Publications</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dans.world/" >Blog</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Daniel Stoller</h3>
    
    
      <p class="author__bio" itemprop="description">
        Researcher in Machine Learning and Music Information Retrieval.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">London, UK</span>
        </li>
      

      

      
        <li>
          <a href="mailto:business@dstoller.net">
            <meta itemprop="email" content="business@dstoller.net" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/daniel-stoller" itemprop="sameAs">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/f90" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

        <li>
    <a href="https://orcid.org/0000-0002-8615-4144" itemprop="sameAs">
      <i class="ai ai-orcid-square ai-fw"></i> ORCID
    </a>
  </li>

  <li>
    <a href="https://scholar.google.co.uk/citations?user=Ozxm6UsAAAAJ" itemprop="sameAs">
      <i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar
    </a>
</li>

  <li>
    <a href="https://qmul.academia.edu/DanielStoller" itemprop="sameAs">
      <i class="ai ai-academia-square ai-fw"></i> Academia.edu
    </a>
</li>
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="List of publications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">List of publications
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="journal-articles">Journal articles</h2>

<ol class="bibliography"><li><div>
<b>Intuitive and efficient computer-aided music rearrangement with optimised
	processing of audio transitions</b> <span style="font-size:15px"> (2018) </span>
<br />


Journal of New Music Research

<i>Stoller, Daniel and Vatolkin, Igor and MÃ¼ller, Heinrich</i>
</div>

<!--

    <a href="http://doi.org/10.1080/09298215.2018.1473448"><input class="button1" type="button" value="doi" /></a>

-->





<!--

    <a href=" https://doi.org/10.1080/09298215.2018.1473448 
"><input class="button" type="button" value="link" /></a>

-->




    <a href=" https://doi.org/10.1080/09298215.2018.1473448 
"><input class="button0" type="button" value="Link" /></a>


<a download="Stoller2018b.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@article%7BStoller2018b,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Vatolkin,%20Igor%20and%20M%C3%BCller,%20Heinrich%7D,%0A%20%20title%20=%20%7BIntuitive%20and%20efficient%20computer-aided%20music%20rearrangement%20with%20optimised%0A%20%20%09processing%20of%20audio%20transitions%7D,%0A%20%20journal%20=%20%7BJournal%20of%20New%20Music%20Research%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20volume%20=%20%7B0%7D,%0A%20%20pages%20=%20%7B1-22%7D,%0A%20%20number%20=%20%7B0%7D,%0A%20%20abstract%20=%20%7BA%20promising%20approach%20to%20create%20new%20versions%20of%20existing%20music%20pieces%0A%20%20%09automatically%20is%20to%20cut%20out%20and%20rearrange%20sections%20so%20that%20transitions%0A%20%20%09are%20minimally%20perceptible%20and%20constraints%20regarding%20duration%20or%20structure%0A%20%20%09are%20fulfilled.%20We%20evaluate%20previous%20work%20and%20improve%20on%20its%20limitations,%0A%20%20%09particularly%20the%20disregard%20for%20loudness%20changes%20at%20cuts%20and%20the%20unintuitive%0A%20%20%09control%20over%20the%20musical%20structure%20of%20the%20output.%20Our%20software%20provides%0A%20%20%09a%20user-friendly%20interface,%20which%20we%20make%20more%20responsive%20by%20greatly%0A%20%20%09accelerating%20the%20search%20for%20an%20optimal%20output%20track%20using%20the%20A*%0A%20%20%09algorithm.%20Listening%20experiments%20demonstrate%20an%20improvement%20in%20perceived%0A%20%20%09audio%20quality.%7D,%0A%20%20doi%20=%20%7B10.1080/09298215.2018.1473448%7D,%0A%20%20eprint%20=%20%7B%20https://doi.org/10.1080/09298215.2018.1473448%20%7D,%0A%20%20publisher%20=%20%7BRoutledge%7D,%0A%20%20url%20=%20%7B%20https://doi.org/10.1080/09298215.2018.1473448%20%0A%20%20%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2018b/">Details</a></li></ol>

<h2 id="conference-papers">Conference papers</h2>

<ol class="bibliography"><li><div>
<b>Jointly Detecting and Separating Singing Voice: A Multi-Task Approach</b> <span style="font-size:15px"> (2018) </span>
<br />

  Latent Variable Analysis and Signal Separation
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>

<!--

-->





<!--

    <a href="https://link.springer.com/chapter/10.1007/978-3-319-93764-9_31"><input class="button" type="button" value="link" /></a>

-->




    <a href="https://link.springer.com/chapter/10.1007/978-3-319-93764-9_31"><input class="button0" type="button" value="Link" /></a>


<a download="Stoller2018c.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BStoller2018c,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20title%20=%20%7BJointly%20Detecting%20and%20Separating%20Singing%20Voice:%20A%20Multi-Task%20Approach%7D,%0A%20%20booktitle%20=%20%7BLatent%20Variable%20Analysis%20and%20Signal%20Separation%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20editor%20=%20%7BDeville,%20Yannick%20and%20Gannot,%20Sharon%20and%20Mason,%20Russell%20and%20Plumbley,%20Mark%20D.%20and%20Ward,%20Dominic%7D,%0A%20%20pages%20=%20%7B329--339%7D,%0A%20%20address%20=%20%7BCham%7D,%0A%20%20publisher%20=%20%7BSpringer%20International%20Publishing%7D,%0A%20%20abstract%20=%20%7BA%20main%20challenge%20in%20applying%20deep%20learning%20to%20music%20processing%20is%0A%20%20%09the%20availability%20of%20training%20data.%20One%20potential%20solution%20is%20Multi-task%0A%20%20%09Learning,%20in%20which%20the%20model%20also%20learns%20to%20solve%20related%20auxiliary%0A%20%20%09tasks%20on%20additional%20datasets%20to%20exploit%20their%20correlation.%20While%0A%20%20%09intuitive%20in%20principle,%20it%20can%20be%20challenging%20to%20identify%20related%0A%20%20%09tasks%20and%20construct%20the%20model%20to%20optimally%20share%20information%20between%0A%20%20%09tasks.%20In%20this%20paper,%20we%20explore%20vocal%20activity%20detection%20as%20an%20additional%0A%20%20%09task%20to%20stabilise%20and%20improve%20the%20performance%20of%20vocal%20separation.%0A%20%20%09Further,%20we%20identify%20problematic%20biases%20specific%20to%20each%20dataset%0A%20%20%09that%20could%20limit%20the%20generalisation%20capability%20of%20separation%20and%0A%20%20%09detection%20models,%20to%20which%20our%20proposed%20approach%20is%20robust.%20Experiments%0A%20%20%09show%20improved%20performance%20in%20separation%20as%20well%20as%20vocal%20detection%0A%20%20%09compared%20to%20single-task%20baselines.%20However,%20we%20find%20that%20the%20commonly%0A%20%20%09used%20Signal-to-Distortion%20Ratio%20(SDR)%20metrics%20did%20not%20capture%20the%0A%20%20%09improvement%20on%20non-vocal%20sections,%20indicating%20the%20need%20for%20improved%0A%20%20%09evaluation%20methodologies.%7D,%0A%20%20isbn%20=%20%7B978-3-319-93764-9%7D,%0A%20%20url%20=%20%7Bhttps://link.springer.com/chapter/10.1007/978-3-319-93764-9_31%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2018c/">Details</a></li>
<li><div>
<b>Adversarial Semi-Supervised Audio Source Separation applied to Singing
	Voice Extraction</b> <span style="font-size:15px"> (2018) </span>
<br />

  Proceedings of the IEEE International Conference on Acoustics, Speech,
	and Signal Processing (ICASSP)
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>

<!--

-->


    <a href="/repository/Stoller2018.published.pdf"><input class="button0" type="button" value="PDF" /></a>



    <a href="https://arxiv.org/abs/1711.00048"><input class="button0" type="button" value="Preprint" /></a>


<!--

-->





<a download="Stoller2018.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BStoller2018,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20title%20=%20%7BAdversarial%20Semi-Supervised%20Audio%20Source%20Separation%20applied%20to%20Singing%0A%20%20%09Voice%20Extraction%7D,%0A%20%20booktitle%20=%20%7BProceedings%20of%20the%20IEEE%20International%20Conference%20on%20Acoustics,%20Speech,%0A%20%20%09and%20Signal%20Processing%20(%7BICASSP%7D)%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20pages%20=%20%7B2391--2395%7D,%0A%20%20address%20=%20%7BCalgary,%20Canada%7D,%0A%20%20publisher%20=%20%7BIEEE%7D,%0A%20%20abstract%20=%20%7BThe%20state%20of%20the%20art%20in%20music%20source%20separation%20employs%20neural%20networks%0A%20%20%09trained%20in%20a%20supervised%20fashion%20on%20multi-track%20databases%20to%20estimate%0A%20%20%09the%20sources%20from%20a%20given%20mixture.%20With%20only%20few%20datasets%20available,%0A%20%20%09often%20extensive%20data%20augmentation%20is%20used%20to%20combat%20overfitting.%0A%20%20%09Mixing%20random%20tracks,%20however,%20can%20even%20reduce%20separation%20performance%0A%20%20%09as%20instruments%20in%20real%20music%20are%20strongly%20correlated.%20The%20key%20concept%0A%20%20%09in%20our%20approach%20is%20that%20source%20estimates%20of%20an%20optimal%20separator%0A%20%20%09should%20be%20indistinguishable%20from%20real%20source%20signals.%20Based%20on%20this%0A%20%20%09idea,%20we%20drive%20the%20separator%20towards%20outputs%20deemed%20as%20realistic%0A%20%20%09by%20discriminator%20networks%20that%20are%20trained%20to%20tell%20apart%20real%20from%0A%20%20%09separator%20samples.%20This%20way,%20we%20can%20also%20use%20unpaired%20source%20and%0A%20%20%09mixture%20recordings%20without%20the%20drawbacks%20of%20creating%20unrealistic%0A%20%20%09music%20mixtures.%20Our%20framework%20is%20widely%20applicable%20as%20it%20does%20not%0A%20%20%09assume%20a%20specific%20network%20architecture%20or%20number%20of%20sources.%20To%20our%0A%20%20%09knowledge,%20this%20is%20the%20first%20adoption%20of%20adversarial%20training%20for%0A%20%20%09music%20source%20separation.%20In%20a%20prototype%20experiment%20for%20singing%20voice%0A%20%20%09separation,%20separation%20performance%20increases%20with%20our%20approach%20compared%0A%20%20%09to%20purely%20supervised%20training.%7D,%0A%20%20preprint%20=%20%7Bhttps://arxiv.org/abs/1711.00048%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2018/">Details</a></li>
<li><div>
<b>Wave-U-Net: A Multi-Scale Neural Network for End-to-End Source
	Separation</b> <span style="font-size:15px"> (2018) </span>
<br />

  Proceedings of the International Society for Music Information Retrieval
	Conference (ISMIR)
  <br />


<i>Stoller, Daniel and Ewert, Sebastian and Dixon, Simon</i>
</div>

<!--

-->


    <a href="/repository/Stoller2018a.published.pdf"><input class="button0" type="button" value="PDF" /></a>




<!--

-->





<a download="Stoller2018a.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BStoller2018a,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Ewert,%20Sebastian%20and%20Dixon,%20Simon%7D,%0A%20%20title%20=%20%7B%7BWave-U-Net%7D:%20A%20Multi-Scale%20Neural%20Network%20for%20End-to-End%20Source%0A%20%20%09Separation%7D,%0A%20%20booktitle%20=%20%7BProceedings%20of%20the%20International%20Society%20for%20Music%20Information%20Retrieval%0A%20%20%09Conference%20(%7BISMIR%7D)%7D,%0A%20%20year%20=%20%7B2018%7D,%0A%20%20volume%20=%20%7B19%7D,%0A%20%20pages%20=%20%7B334--340%7D,%0A%20%20abstract%20=%20%7BModels%20for%20audio%20source%20separation%20usually%20operate%20on%20the%20magnitude%0A%20%20%09spectrum,%20which%20ignores%20phase%20information%20and%20makes%20separation%20performance%0A%20%20%09dependant%20on%20hyper-parameters%20for%20the%20spectral%20front-end.%20Therefore,%0A%20%20%09we%20investigate%20end-to-end%20source%20separation%20in%20the%20time-domain,%20which%0A%20%20%09allows%20modelling%20phase%20information%20and%20avoids%20fixed%20spectral%20transformations.%0A%20%20%09Due%20to%20high%20sampling%20rates%20for%20audio,%20employing%20a%20long%20temporal%20input%0A%20%20%09context%20on%20the%20sample%20level%20is%20difficult,%20but%20required%20for%20high%20quality%0A%20%20%09separation%20results%20because%20of%20long-range%20temporal%20correlations.%20In%0A%20%20%09this%20context,%20we%20propose%20the%20Wave-U-Net,%20an%20adaptation%20of%20the%20U-Net%0A%20%20%09to%20the%20one-dimensional%20time%20domain,%20which%20repeatedly%20resamples%20feature%0A%20%20%09maps%20to%20compute%20and%20combine%20features%20at%20different%20time%20scales.%20We%0A%20%20%09introduce%20further%20architectural%20improvements,%20including%20an%20output%0A%20%20%09layer%20that%20enforces%20source%20additivity,%20an%20upsampling%20technique%20and%0A%20%20%09a%20context-aware%20prediction%20framework%20to%20reduce%20output%20artifacts.%0A%20%20%09Experiments%20for%20singing%20voice%20separation%20indicate%20that%20our%20architecture%0A%20%20%09yields%20a%20performance%20comparable%20to%20a%20state-of-the-art%20spectrogram-based%0A%20%20%09U-Net%20architecture,%20given%20the%20same%20data.%20Finally,%20we%20reveal%20a%20problem%0A%20%20%09with%20outliers%20in%20the%20currently%20used%20SDR%20evaluation%20metrics%20and%20suggest%0A%20%20%09reporting%20rank-based%20statistics%20to%20alleviate%20this%20problem.%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2018a/">Details</a></li>
<li><div>
<b>Analysis and classification of phonation modes in singing</b> <span style="font-size:15px"> (2016) </span>
<br />

  Proceedings of the International Society for Music Information Retrieval
	Conference (ISMIR)
  <br />


<i>Stoller, Daniel and Dixon, Simon</i>
</div>

<!--

-->


    <a href="/repository/Stoller2016.published.pdf"><input class="button0" type="button" value="PDF" /></a>




<!--

-->





<a download="Stoller2016.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BStoller2016,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Dixon,%20Simon%7D,%0A%20%20title%20=%20%7BAnalysis%20and%20classification%20of%20phonation%20modes%20in%20singing%7D,%0A%20%20booktitle%20=%20%7BProceedings%20of%20the%20International%20Society%20for%20Music%20Information%20Retrieval%0A%20%20%09Conference%20(%7BISMIR%7D)%7D,%0A%20%20year%20=%20%7B2016%7D,%0A%20%20volume%20=%20%7B17%7D,%0A%20%20pages%20=%20%7B80--86%7D,%0A%20%20abstract%20=%20%7BPhonation%20mode%20is%20an%20expressive%20aspect%20of%20the%20singing%20voice%20and%20can%0A%20%20%09be%20described%20using%20the%20four%20categories%20neutral,%20breathy,%20pressed%0A%20%20%09and%20flow.%20Previous%20attempts%20at%20automatically%20classifying%20the%20phonation%0A%20%20%09mode%20on%20a%20dataset%20containing%20vowels%20sung%20by%20a%20female%20professional%0A%20%20%09have%20been%20lacking%20in%20accuracy%20or%20have%20not%20sufficiently%20investigated%0A%20%20%09the%20characteristic%20features%20of%20the%20different%20phonation%20modes%20which%0A%20%20%09enable%20successful%20classification.%20In%20this%20paper,%20we%20extract%20a%20large%0A%20%20%09range%20of%20features%20from%20this%20dataset,%20including%20specialised%20descriptors%0A%20%20%09of%20pressedness%20and%20breathiness,%20to%20analyse%20their%20explanatory%20power%0A%20%20%09and%20robustness%20against%20changes%20of%20pitch%20and%20vowel.%20We%20train%20and%20optimise%0A%20%20%09a%20feed-forward%20neural%20network%20(NN)%20with%20one%20hidden%20layer%20on%20all%20features%0A%20%20%09using%20cross%20validation%20to%20achieve%20a%20mean%20F-measure%20above%200.85%20and%0A%20%20%09an%20improved%20performance%20compared%20to%20previous%20work.%0A%20%20%09%0A%20%20%09Applying%20feature%20selection%20based%20on%20mutual%20information%20and%20retaining%0A%20%20%09the%20nine%20highest%20ranked%20features%20as%20input%20to%20a%20NN%20results%20in%20a%20mean%0A%20%20%09F-measure%20of%200.78,%20demonstrating%20the%20suitability%20of%20these%20features%0A%20%20%09to%20discriminate%20between%20phonation%20modes.%20Training%20and%20pruning%20a%20decision%0A%20%20%09tree%20yields%20a%20simple%20rule%20set%20based%20only%20on%20cepstral%20peak%20prominence%0A%20%20%09(CPP),%20temporal%20flatness%20and%20average%20energy%20that%20correctly%20categorises%0A%20%20%0978%25%20of%20the%20recordings.%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2016/">Details</a></li>
<li><div>
<b>Impact of Frame Size and Instrumentation on Chroma-Based Automatic
	Chord Recognition</b> <span style="font-size:15px"> (2015) </span>
<br />

  Data Science, Learning by Latent Structures, and Knowledge Discovery
  <br />


<i>Stoller, Daniel and Mauch, Matthias and Vatolkin, Igor and Weihs, Claus</i>
</div>

<!--

-->





<!--

    <a href="https://doi.org/10.1007/978-3-662-44983-7_36"><input class="button" type="button" value="link" /></a>

-->




    <a href="https://doi.org/10.1007/978-3-662-44983-7_36"><input class="button0" type="button" value="Link" /></a>


<a download="Stoller2015a.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@inproceedings%7BStoller2015a,%0A%20%20author%20=%20%7BStoller,%20Daniel%20and%20Mauch,%20Matthias%20and%20Vatolkin,%20Igor%20and%20Weihs,%20Claus%7D,%0A%20%20title%20=%20%7BImpact%20of%20Frame%20Size%20and%20Instrumentation%20on%20Chroma-Based%20Automatic%0A%20%20%09Chord%20Recognition%7D,%0A%20%20booktitle%20=%20%7BData%20Science,%20Learning%20by%20Latent%20Structures,%20and%20Knowledge%20Discovery%7D,%0A%20%20year%20=%20%7B2015%7D,%0A%20%20editor%20=%20%7BLausen,%20Berthold%20and%20Krolak-Schwerdt,%20Sabine%20and%20B%7B%5C%22o%7Dhmer,%20Matthias%7D,%0A%20%20pages%20=%20%7B411--421%7D,%0A%20%20address%20=%20%7BBerlin,%20Heidelberg%7D,%0A%20%20publisher%20=%20%7BSpringer%20Berlin%20Heidelberg%7D,%0A%20%20abstract%20=%20%7BThis%20paper%20presents%20a%20comparative%20study%20of%20classification%20performance%0A%20%20%09in%20automatic%20audio%20chord%20recognition%20based%20on%20three%20chroma%20feature%0A%20%20%09implementations,%20with%20the%20aim%20of%20distinguishing%20effects%20of%20frame%0A%20%20%09size,%20instrumentation,%20and%20choice%20of%20chroma%20feature.%20Until%20recently,%0A%20%20%09research%20in%20automatic%20chord%20recognition%20has%20focused%20on%20the%20development%0A%20%20%09of%20complete%20systems.%20While%20results%20have%20remarkably%20improved,%20the%0A%20%20%09understanding%20of%20the%20error%20sources%20remains%20lacking.%20In%20order%20to%20isolate%0A%20%20%09sources%20of%20chord%20recognition%20error,%20we%20create%20a%20corpus%20of%20artificial%0A%20%20%09instrument%20mixtures%20and%20investigate%20(a)%20the%20influence%20of%20different%0A%20%20%09chroma%20frame%20sizes%20and%20(b)%20the%20impact%20of%20instrumentation%20and%20pitch%0A%20%20%09height.%20We%20show%20that%20recognition%20performance%20is%20significantly%20affected%0A%20%20%09not%20only%20by%20the%20method%20used,%20but%20also%20by%20the%20nature%20of%20the%20audio%0A%20%20%09input.%20We%20compare%20these%20results%20to%20those%20obtained%20from%20a%20corpus%20of%0A%20%20%09more%20than%20200%20real-world%20pop%20songs%20from%20The%20Beatles%20and%20other%20artists%0A%20%20%09for%20the%20case%20in%20which%20chord%20boundaries%20are%20known%20in%20advance.%7D,%0A%20%20isbn%20=%20%7B978-3-662-44983-7%7D,%0A%20%20url%20=%20%7Bhttps://doi.org/10.1007/978-3-662-44983-7_36%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2015a/">Details</a></li></ol>

<h2 id="thesis">Thesis</h2>

<ol class="bibliography"><li><div>
<b>Constrained-based rearrangement of music</b> <span style="font-size:15px"> (2015) </span>
<br />


<i>Stoller, Daniel</i>
</div>

<!--

-->


    <a href="/repository/Stoller2015.published.pdf"><input class="button0" type="button" value="PDF" /></a>




<!--

-->





<a download="Stoller2015.bib" href="data:application/x-bibtex,%7B%25raw%25%7D@mastersthesis%7BStoller2015,%0A%20%20author%20=%20%7BStoller,%20Daniel%7D,%0A%20%20title%20=%20%7B%7BConstrained-based%20rearrangement%20of%20music%7D%7D,%0A%20%20school%20=%20%7BTechnical%20University%20Dortmund%7D,%0A%20%20year%20=%20%7B2015%7D,%0A%20%20address%20=%20%7BGermany%7D,%0A%20%20institution%20=%20%7BTechnical%20University%20Dortmund%7D,%0A%20%20owner%20=%20%7BDaniel%7D,%0A%20%20timestamp%20=%20%7B2016.08.17%7D%0A%7D%0A%7B%25endraw%25%7D"><input class="button0" type="button" value="Bibtex" /></a><a class="details" href="/repository/Stoller2015/">Details</a></li></ol>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/f90"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Daniel Stoller. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.1.0/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>





  </body>
</html>